[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"index.html","id":"back-to-in-person-teaching","chapter":"Preface","heading":"Back to in-person teaching","text":"\ncorona virus pandemic still impacts lives changes way\nteach learn. course held person, hope \nback nearly normal. learnt lot online-teaching phase\ntools proved useful. Thus, try move \n-person phase.\ncourse, use following tools:ILIAS: moodle platform UoC. registered already.Campuswire: chat platform decrease number emails allow natural exchange participants lecturer. received invitation email, , email , please.","code":""},{"path":"index.html","id":"intended-learning-outcomes-ilos","chapter":"Preface","heading":"Intended learning outcomes (ILOs)","text":"end course able \nImport/read data R.\n\nPrepare data analysis.\n\nVisualize data.\n\nExplain apply statistical methods learnt course.\n\nCombine code report reproducible way.\n\nApply selected methods learnt course new data set \nwrite reproducible report.\n","code":""},{"path":"index.html","id":"literature","chapter":"Preface","heading":"Literature","text":"using book ModernDive: Statistical Inference via Data Science (Ismay Kim 2021) mainly. Additionally, recommend time time R Data Science (Wickham Grolemund 2021) OpenIntro Statistics (Diez, √áetinkaya-Rundel, Barr 2019). report, additional literature search depending topic.","code":""},{"path":"index.html","id":"why-these-lecture-notes","chapter":"Preface","heading":"Why these lecture notes","text":"document working live document updated course. comprehensive, help navigate introduction R statistics smoothly.use different colour boxes\nInfos tips\n\nLearning outcomes\n\nimportant\n\ndefinition\n\nexercise inside chapter.\n","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Preface","heading":"Acknowledgements","text":"document draws free material provided byModernDive: Ismay Kim (2021) free Problem Sets authored Jenny Smetzer, William Hopper, Albert Y. Kim, Chester Ismay (https://moderndive.github.io/moderndive_labs/index.html)ModernDive: Ismay Kim (2021) free Problem Sets authored Jenny Smetzer, William Hopper, Albert Y. Kim, Chester Ismay (https://moderndive.github.io/moderndive_labs/index.html)R Data Science (r4ds): Wickham Grolemund (2021)R Data Science (r4ds): Wickham Grolemund (2021)Data Science Box (https://datasciencebox.org/) free book Diez, √áetinkaya-Rundel, Barr (2019)Data Science Box (https://datasciencebox.org/) free book Diez, √áetinkaya-Rundel, Barr (2019)One thank people enough contribution  community !Credit: https://xkcd.com/2400/","code":""},{"path":"index.html","id":"reproducibility","chapter":"Preface","heading":"Reproducibility","text":"book written RStudio using Bookdown compiled R version 4.2.3 (2023-03-15). need following packages reproduce examples work exercises:complete information last session build book:work licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":"## R version 4.2.3 (2023-03-15)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 22.04.2 LTS\n## \n## Matrix products: default\n## BLAS:   /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3\n## LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3\n## \n## locale:\n##  [1] LC_CTYPE=de_DE.UTF-8       LC_NUMERIC=C              \n##  [3] LC_TIME=de_DE.UTF-8        LC_COLLATE=de_DE.UTF-8    \n##  [5] LC_MONETARY=de_DE.UTF-8    LC_MESSAGES=de_DE.UTF-8   \n##  [7] LC_PAPER=de_DE.UTF-8       LC_NAME=C                 \n##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C       \n## \n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base     \n## \n## other attached packages:\n##  [1] DT_0.22           forcats_0.5.1     stringr_1.4.0     dplyr_1.0.9      \n##  [5] purrr_0.3.4       readr_2.1.2       tidyr_1.2.0       tibble_3.1.7     \n##  [9] ggplot2_3.3.6     tidyverse_1.3.1   kableExtra_1.3.4  fontawesome_0.3.0\n## \n## loaded via a namespace (and not attached):\n##  [1] tufte_0.12        svglite_2.1.0     lubridate_1.8.0   rprojroot_2.0.3  \n##  [5] assertthat_0.2.1  digest_0.6.29     utf8_1.2.2        R6_2.5.1         \n##  [9] cellranger_1.1.0  backports_1.4.1   reprex_2.0.1      evaluate_0.15    \n## [13] highr_0.9         httr_1.4.2        pillar_1.7.0      rlang_1.0.6      \n## [17] readxl_1.4.0      rstudioapi_0.13   jquerylib_0.1.4   rmarkdown_2.14   \n## [21] desc_1.4.1        webshot_0.5.4     htmlwidgets_1.5.4 munsell_0.5.0    \n## [25] broom_1.0.1       compiler_4.2.3    modelr_0.1.8      xfun_0.31        \n## [29] pkgconfig_2.0.3   systemfonts_1.0.4 htmltools_0.5.2   downlit_0.4.0    \n## [33] tidyselect_1.2.0  bookdown_0.26     fansi_1.0.3       viridisLite_0.4.0\n## [37] withr_2.5.0       crayon_1.5.1      tzdb_0.3.0        dbplyr_2.1.1     \n## [41] grid_4.2.3        jsonlite_1.8.0    gtable_0.3.0      lifecycle_1.0.3  \n## [45] DBI_1.1.2         magrittr_2.0.3    scales_1.2.0      cli_3.4.1        \n## [49] stringi_1.7.6     cachem_1.0.6      fs_1.5.2          xml2_1.3.3       \n## [53] bslib_0.3.1       ellipsis_0.3.2    generics_0.1.2    vctrs_0.5.1      \n## [57] tools_4.2.3       glue_1.6.2        hms_1.1.1         fastmap_1.1.0    \n## [61] yaml_2.3.5        colorspace_2.0-3  sessioninfo_1.2.2 rvest_1.0.2      \n## [65] memoise_2.0.1     knitr_1.39        haven_2.5.0       sass_0.4.1"},{"path":"introduction-to-r-and-rstudio.html","id":"introduction-to-r-and-rstudio","chapter":"1 Introduction to R and RStudio","heading":"1 Introduction to R and RStudio","text":"\nCreate save files RStudio\n\nUse R calculator\n\nCreate first objects R\n\nCall first functions R\n\nfirst plot\nchapter introduces R RStudio ‚Äôll using throughout course learn statistical concepts analyse real data. Often, R RStudio confused. However, : R name programming language RStudio -called integrated development interface (IDE), development environment make life easier.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"what-is","chapter":"1 Introduction to R and RStudio","heading":"1.1 What is ?","text":"R programming language data analysis statistical modelling. free (open-source software) belongs, together Python, popular programming languages data analysis. R introduced Ross Ihaka Robert Gentleman 1996 (Ihaka Gentleman 1996). many additional packages, extend functionality.can install R computer official R webpage. short installation instruction list packages course can found ILIAS. Additionally, can refer Ismay Kim (2021), Chapter 1. find packages R‚Äôs official webpage CRAN (Comprehensive R Archive Network). packages released CRAN. However, beginning good idea use CRAN find install packages.Packages sometimes organized topics, can explore via CRAN Task Views. environmental statistics, following topics relevant:Environmetrics: analysis environmental dataMultivariate: multivariate statisticsSpatial: analysis spatial dataTimeSeries: time series analysis.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"what-is-rstudio","chapter":"1 Introduction to R and RStudio","heading":"1.2 What is RStudio?","text":"RStudio Desktop IDE R (languages). can download install open-source version computer . RStudio interface split four main areas (Figure 1.1).top left, type commands text. focus course reproducible research, start using R Markdown next session.panel bottom left console R executes commands. proper R system. start RStudio, standard R text displayed inform R open source, version useful things.panel upper right contains workspace, .e.¬†objects generated working session R. Additionally, find history commands .panel bottom right show plots (case working simple scripts RMarkdown files). tab ‚ÄúFiles‚Äù helps browse files.\nFigure 1.1: RStudio interface\n","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"organize-your-work","chapter":"1 Introduction to R and RStudio","heading":"1.3 Organize your work","text":"better organize files, create subfolders data, scripts notebooks main folder.","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"practice-on-your-own","chapter":"1 Introduction to R and RStudio","heading":"1.4 Practice on your own!","text":"start exercises C.1.1 C.1.2 class. Finish exercises produce first plot exercise C.1.3 üòÑ. need type comments answer text, don‚Äôt forget use comment sign # beginning text line. Otherwise, R misinterpret text commands try execute .\nRemember save R script regularly! Click save button \nupper left-hand corner window hit Stg+s.\n","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"turning-in-your-work","chapter":"1 Introduction to R and RStudio","heading":"1.5 Turning in your work","text":"Save R script *.R file.Upload *.R file ILIAS. find upload option today‚Äôs session.receive solution file upload.\nsure upload deadline!\n","code":""},{"path":"introduction-to-r-and-rstudio.html","id":"reading-assignment","chapter":"1 Introduction to R and RStudio","heading":"1.6 Reading assignment","text":"Chapters 1.1 1.2 Ismay Kim (2021)","code":""},{"path":"using-r-markdown-for-reproducible-research.html","id":"using-r-markdown-for-reproducible-research","chapter":"2 Using R Markdown for reproducible research","heading":"2 Using R Markdown for reproducible research","text":"\nOpening saving R Notebook\n\nBasic layout R Markdown\n","code":""},{"path":"using-r-markdown-for-reproducible-research.html","id":"reproducibility-in-research","chapter":"2 Using R Markdown for reproducible research","heading":"2.1 Reproducibility in research","text":"Usually, analysing data generating report two separate tasks. First, analyse data (hopefully R üòÑ), describe methods, results conclusions text document. However, procedure error-prone reproducible. copy-paste results R tables include figures word processor. connection analysis code, results report lost.Donald Knuth, creator TEX, suggested idea literate programming, analysis code report combined one document (Knuth 1984). kind document human-centred allows better understand analysis. helps generate completely reproducible data analyses.","code":""},{"path":"using-r-markdown-for-reproducible-research.html","id":"combining-code-and-report-in-one-document","chapter":"2 Using R Markdown for reproducible research","heading":"2.2 Combining code and report in one document","text":"use R Markdown combine analysis code report one reproducible document. general, R Markdown can produce different output documents (html, word, pdf, slides). However, course concentrate html output use -called R Notebooks (mostly).","code":""},{"path":"using-r-markdown-for-reproducible-research.html","id":"create-a-new-r-notebook","chapter":"2 Using R Markdown for reproducible research","heading":"2.2.1 Create a new R Notebook","text":"create new R Notebook, click little green plus click File upper left hand select R Notebook image . Save notebook subfolder notebooks.\nFigure 2.1: create new R Notebook\ncontrast new R script, new notebook template text example R code grey boxes called chunks. look template text. provides basic example layout R code chunks.","code":""},{"path":"using-r-markdown-for-reproducible-research.html","id":"customize-the-header","chapter":"2 Using R Markdown for reproducible research","heading":"2.2.2 Customize the header","text":"Every R Markdown document starts header. enclosed two lines --- signs. Inside header, find (blue) keywords like title: output:. Let‚Äôs customize header needs:Change title top ‚ÄúGetting know R Notebooks‚Äù. sure keep quotation marks.Change title top ‚ÄúGetting know R Notebooks‚Äù. sure keep quotation marks.Add author line put name quotation marks.Add author line put name quotation marks.Additionally, might want add date. syntax date: \"date \".","code":""},{"path":"using-r-markdown-for-reproducible-research.html","id":"structure-your-notebook","chapter":"2 Using R Markdown for reproducible research","heading":"2.2.3 Structure your notebook","text":"Structure headers subheaders helps orginze content ideas. add header, put # followed header title. sure include space # text! subheader produced ## subsubheader ###. sure include space header text!Delete template text structure notebook. final result look something like :\nFigure 2.2: R Notebook\n","code":""},{"path":"using-r-markdown-for-reproducible-research.html","id":"preview","chapter":"2 Using R Markdown for reproducible research","heading":"2.2.4 Preview","text":"Notebooks great advantage offer preview work. Just click Preview button. preview refreshed every time save notebook.Inspect preview notebook see formatting headers subheaders affects output. layout elements, experiment exercises.","code":""},{"path":"using-r-markdown-for-reproducible-research.html","id":"other-output-options","chapter":"2 Using R Markdown for reproducible research","heading":"2.2.5 Other output options","text":"can also produce different outputs R Notebook normal R Markdown file supports different output formats. However, produce .html output, Preview button disappear! bring back, need edit header R Notebook file output: html_notebook.\nNote now R Notebook file (.Rmd) \nhtml file (nb.html) Notebooks\nfolder.\n","code":""},{"path":"using-r-markdown-for-reproducible-research.html","id":"entering-and-running-commands","chapter":"2 Using R Markdown for reproducible research","heading":"2.3 Entering and running commands","text":"contrast text, headings etc. R code typed special boxes called chunks. create empty chunk type code, click little green C top type Str + Alt + . international keyboard, Str equals Control Mac Command key.Using first code chunk, type following command create new variable called x value 42.Remember arrow <- assignment operator. generates (overwrites, already exists) object x assigns value 42.\nNote direction arrow! points value \nobject name.\nRun command console, can either:click green triangle code chunk right orhighlight code chunk hit Str + Enter (R script).\nNote now new object workspace, called x!\n\nFigure 2.3: Global environment contains variable x now\n","code":"\nx <- 42"},{"path":"using-r-markdown-for-reproducible-research.html","id":"a-brief-recap-of-data-types","chapter":"2 Using R Markdown for reproducible research","heading":"2.4 A brief recap of data types","text":"created numeric variable x. However, restricted numbers. R can also handle types objects, like characters, example. tell R want generate variable containing characters contrast numbers, need enclose assigned content quotes.Create following chunk notebook let run.generate complicated object, namely numeric vector, use command c() concatenate several numbers.Note Environment pane vector v contains numbers (listed num). information [1:3] shows vector three elements, indexed 1 3. Indices indicate place element vector. access change particular element, use index likeNow second element vector equals 4.5. Remember R warn changing objects!can calculate objects can numbers. Let‚Äôs divide evely single element v 2.","code":"\nday_of_week <- \"Sunday\"\nv <- c(4.5, 6.234, 10)\nv[2] <- 4.5\nv / 2"},{"path":"using-r-markdown-for-reproducible-research.html","id":"practice-on-your-own-1","chapter":"2 Using R Markdown for reproducible research","heading":"2.5 Practice on your own!","text":"work exercises, please structure R Notebook e.g.¬†headers subheaders exercise. exercises require code explanation!\nRemember save work go along! Click save button \nupper left hand corner R Markdown window.\nAnswer following code code chunk (text necessary). Remember code just instructions R. need run code chunk make R execute instructions!\nCreate variable called y value 13.\nMultiply x y, store answer variable named z like : z <- x * y\nnow see day_of_week, x, v, y, z Environment pane.\nAnswer following code code chunk (text necessary). Remember code just instructions R. need run code chunk make R execute instructions!Create variable called y value 13.Multiply x y, store answer variable named z like : z <- x * yYou now see day_of_week, x, v, y, z Environment pane.Run following mathematical operation code chunk: 6 + 3.\nanswer appear?\nRun following mathematical operation code chunk: 6 + 3.answer appear?Now add code chunk, save results 6 + 3 variable called .\nanswer appear?\nobject show ?\nNext type code chunk re-run code chunk. happens?\nNow add code chunk, save results 6 + 3 variable called .answer appear?object show ?Next type code chunk re-run code chunk. happens?Run following command new code chunk. ^2.\n^ operator ?\nRun following command new code chunk. ^2.^ operator ?Type following command new code chunk. sum(, x, y)\nsum function. Based output, think sum function ?\nType following command new code chunk. sum(, x, y)sum function. Based output, think sum function ?Click little broom icon upper right hand corner Environment pane. Click yes window opens.\nhappened?\nClick little broom icon upper right hand corner Environment pane. Click yes window opens.happened?Go Run button top right R Markdown pane, choose Run (last option)\nhappened?\nGo Run button top right R Markdown pane, choose Run (last option)happened?Recall vector v created earlier. Copy, paste run following code chunk. code accomplish?\nv + 2\nRecall vector v created earlier. Copy, paste run following code chunk. code accomplish?\nv + 2\nCopy, paste, run following code make vector called music, contains music genres. Recall vector data object multiple elements type. data type character. Look environment pane. R tell us vector contains characters, numbers?\nmusic <- c(\"bluegrass\", \"funk\", \"folk\")\nCopy, paste, run following code make vector called music, contains music genres. Recall vector data object multiple elements type. data type character. Look environment pane. R tell us vector contains characters, numbers?\nmusic <- c(\"bluegrass\", \"funk\", \"folk\")\nNow let‚Äôs practice basic formatting. Using formatting tips page figure put following lab report. can get typed white section, text goes. Hint: put line! hit hard return line text.\nItalicize like \nBold like \n\nsuperscript: R2Now let‚Äôs practice basic formatting. Using formatting tips page figure put following lab report. can get typed white section, text goes. Hint: put line! hit hard return line text.\nItalicize like \nBold like \n\nsuperscript: R2","code":""},{"path":"using-r-markdown-for-reproducible-research.html","id":"turning-in-your-work-1","chapter":"2 Using R Markdown for reproducible research","heading":"2.6 Turning in your work","text":"Save R Notebook *.Rmd file.Upload R Notebook ILIAS. don‚Äôt need upload .nb.html file. find upload option today‚Äôs session.receive solution file upload. sure upload deadline!","code":""},{"path":"what-is-data.html","id":"what-is-data","chapter":"3 What is data?","heading":"3 What is data?","text":"\nInstall R package\n\nLoad installed data set\n\nExplore data set recognize type variables\nData can anything üòÑ. Usually, store data rectangular form, .e.¬†variables columns observations rows. two dedicated object formats store data, namely data.frame() tibble(). similar characteristics, however, tibble considered modern form data frame offers advantages (details later).chapter, look data set called palmerpenguins. provided dedicated package, let‚Äôs install package first.","code":""},{"path":"what-is-data.html","id":"installing-r-packages","chapter":"3 What is data?","heading":"3.1 Installing R packages","text":"Packages available official CRAN (Comprehensive R Archive Network) can installed function install.packages('name_of_the_package'). important provide name package quotes (single double).load package, use function library(name_of_the_package), time without quotes!","code":"\ninstall.packages('palmerpenguins')\nlibrary(palmerpenguins)"},{"path":"what-is-data.html","id":"welcome-the-penguins","chapter":"3 What is data?","heading":"3.2 Welcome the penguins!","text":"\nFigure 3.1: Artwork @allison_horst\npackage dedicated website really worth visiting. package contains two data sets, explore shorter one, called penguins. load data set installed package, use function data(\"name_of_data_set\"). sure put name data set quotes (single double).Let‚Äôs look object penguins.object tibble contains data set 344 rows 8 columns, meaning 8 variables measured 344 animals. first column contains variable species , guessed , shows species animal. variable -called factor (indicated <fct> species). means, contains categorical information certain number (usually small one) distinct values called levels. levels case areThe code uses $ sign access whole column (.e.¬†variable) data set. handy alternative square bracket method. syntax name_of_data_set$name_of_variable.also numerical variables tibble. numerical variable can continuous, e.g.¬†bill_length_mm (indicated <dbl> meaning double), meaning contains decimal numbers discrete, e.g.¬†year (indicated <int> meaning integer), meaning contains integers (whole numbers).summarize data set, can use function summary().","code":"\ndata(\"penguins\")\npenguins## # A tibble: 344 √ó 8\n##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n##    <fct>   <fct>              <dbl>         <dbl>             <int>       <int>\n##  1 Adelie  Torgersen           39.1          18.7               181        3750\n##  2 Adelie  Torgersen           39.5          17.4               186        3800\n##  3 Adelie  Torgersen           40.3          18                 195        3250\n##  4 Adelie  Torgersen           NA            NA                  NA          NA\n##  5 Adelie  Torgersen           36.7          19.3               193        3450\n##  6 Adelie  Torgersen           39.3          20.6               190        3650\n##  7 Adelie  Torgersen           38.9          17.8               181        3625\n##  8 Adelie  Torgersen           39.2          19.6               195        4675\n##  9 Adelie  Torgersen           34.1          18.1               193        3475\n## 10 Adelie  Torgersen           42            20.2               190        4250\n## # ‚Ä¶ with 334 more rows, and 2 more variables: sex <fct>, year <int>\nlevels(penguins$species)## [1] \"Adelie\"    \"Chinstrap\" \"Gentoo\"\nsummary(penguins)##       species          island    bill_length_mm  bill_depth_mm  \n##  Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n##  Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n##  Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n##                                  Mean   :43.92   Mean   :17.15  \n##                                  3rd Qu.:48.50   3rd Qu.:18.70  \n##                                  Max.   :59.60   Max.   :21.50  \n##                                  NA's   :2       NA's   :2      \n##  flipper_length_mm  body_mass_g       sex           year     \n##  Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n##  1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n##  Median :197.0     Median :4050   NA's  : 11   Median :2008  \n##  Mean   :200.9     Mean   :4202                Mean   :2008  \n##  3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n##  Max.   :231.0     Max.   :6300                Max.   :2009  \n##  NA's   :2         NA's   :2"},{"path":"what-is-data.html","id":"the-square-braces-revisited","chapter":"3 What is data?","heading":"3.3 The square braces revisited","text":"already know access certain position inside vector. tibble tow-dimensional object, rows columns. access particular measurement, need provide , row column index. following code picks value first row third column:","code":"\npenguins[1, 3]## # A tibble: 1 √ó 1\n##   bill_length_mm\n##            <dbl>\n## 1           39.1"},{"path":"what-is-data.html","id":"lets-look-at-them","chapter":"3 What is data?","heading":"3.4 Let‚Äôs look at them","text":"talk much data visualization later. now, just use code visualize relationship flipper length body mass animals.","code":"\nlibrary(ggplot2)\n\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, col = species)) +\n  geom_point() +\n  xlab('Flipper length (mm)') +\n  ylab('Body mass (g)')"},{"path":"what-is-data.html","id":"practice-on-your-own-2","chapter":"3 What is data?","heading":"3.5 Practice on your own!","text":"many categorical many numerical variables ? Consult help.many categorical many numerical variables ? Consult help.many Gentoo penguins present data set?many Gentoo penguins present data set?time span measurements?time span measurements?Find levels variable island.Find levels variable island.challenge ü§ì. Take code produced visualization flipper length body mass animals. Make educated guess change code produces visualization bill depth vs.¬†body mass. Can also guess adjust label \\(x\\)-axis?challenge ü§ì. Take code produced visualization flipper length body mass animals. Make educated guess change code produces visualization bill depth vs.¬†body mass. Can also guess adjust label \\(x\\)-axis?","code":""},{"path":"what-is-data.html","id":"reading-assignment-1","chapter":"3 What is data?","heading":"3.6 Reading assignment","text":"Chapter 1.3 Ismay Kim (2021).","code":""},{"path":"what-is-data.html","id":"turning-in-your-work-2","chapter":"3 What is data?","heading":"3.7 Turning in your work","text":"Save R Notebook *.Rmd file.Upload R Notebook ILIAS. don‚Äôt need upload .nb.html file. find upload option today‚Äôs session.receive solution file upload. sure upload deadline!","code":""},{"path":"what-is-data.html","id":"additional-reading","chapter":"3 What is data?","heading":"3.8 Additional reading","text":"case prefer flights penguins, can look data exploration Chapter 1.4 Ismay Kim (2021)","code":""},{"path":"import-visualize-and-explore-data.html","id":"import-visualize-and-explore-data","chapter":"4 Import, visualize and explore data","heading":"4 Import, visualize and explore data","text":"\nImport data R\n\nExplain general call function ggplot()\n\nPlot 5 frequently used types graphics\n","code":""},{"path":"import-visualize-and-explore-data.html","id":"data-import-from-text-files","chapter":"4 Import, visualize and explore data","heading":"4.1 Data import from text files","text":"import data set text file (e.g.¬†csv, .txt, .dat) R, use library readr part tydiverse. first load library.Let‚Äôs assume data stored folder data. case, change path accordingly. load data, can choose among several functions start read_. generic one read_delim() can specify columns separated (delimited) data file.Let‚Äôs look data. data set greenhouse gas emissions source sector EU downloaded eurostat 2021-04-30. contains greenhouse gas emissions CO2 equivalent, Mio tonnes, per vehicle type. database great source data reports üòÑ.result reading data function library readr always tibble. can see none variables factor. default behaviour readr. want variable coded factor transform manually, preferably functions package forcats.Let‚Äôs brief look data set.character variables, summary() count frequency different values. However, can get information function unique().data set contains measurements 33 EU countries. can also ask different types vehicle recorded.","code":"\nlibrary(tidyverse)\nemissions <- read_delim(file = 'data/emissions.csv', delim = ';')## Rows: 2871 Columns: 6\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \";\"\n## chr  (4): unit, airpol, vehicle, geo\n## dbl  (1): values\n## date (1): time\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nemissions## # A tibble: 2,871 √ó 6\n##    unit           airpol                         vehicle geo   time       values\n##    <chr>          <chr>                          <chr>   <chr> <date>      <dbl>\n##  1 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Aust‚Ä¶ 2018-01-01  14.4 \n##  2 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Belg‚Ä¶ 2018-01-01  14.4 \n##  3 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Bulg‚Ä¶ 2018-01-01   5.78\n##  4 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Swit‚Ä¶ 2018-01-01  11.0 \n##  5 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Cypr‚Ä¶ 2018-01-01   1.38\n##  6 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Czec‚Ä¶ 2018-01-01  11.9 \n##  7 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Germ‚Ä¶ 2018-01-01  97.8 \n##  8 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Denm‚Ä¶ 2018-01-01   6.85\n##  9 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Esto‚Ä¶ 2018-01-01   1.52\n## 10 Million tonnes Greenhouse gases (CO2, N2O in‚Ä¶ Fuel c‚Ä¶ Gree‚Ä¶ 2018-01-01   7.61\n## # ‚Ä¶ with 2,861 more rows\nsummary(emissions)##      unit              airpol            vehicle              geo           \n##  Length:2871        Length:2871        Length:2871        Length:2871       \n##  Class :character   Class :character   Class :character   Class :character  \n##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n##                                                                             \n##                                                                             \n##                                                                             \n##                                                                             \n##       time                values         \n##  Min.   :1990-01-01   Min.   :  0.00609  \n##  1st Qu.:1997-01-01   1st Qu.:  0.25564  \n##  Median :2004-01-01   Median :  1.92403  \n##  Mean   :2004-01-01   Mean   :  8.52836  \n##  3rd Qu.:2011-01-01   3rd Qu.:  6.93899  \n##  Max.   :2018-01-01   Max.   :119.77824  \n##                       NA's   :232\nlength(unique(emissions$geo))## [1] 33\nunique(emissions$vehicle)## [1] \"Fuel combustion in cars\"                       \n## [2] \"Fuel combustion in heavy duty trucks and buses\"\n## [3] \"Fuel combustion in railways\""},{"path":"import-visualize-and-explore-data.html","id":"visualization-with-the-library-ggplot2","chapter":"4 Import, visualize and explore data","heading":"4.2 Visualization with the library ggplot2","text":"library ggplot2 powerful package data visualization. name comes grammar graphics hints systematic approach visualization. nutshell, ggplot2 defines statistical graphic follows:statistical graphic mapping variables data set aesthetic attributes geometric objects.ggplot2, graphic built step step, starting call core function ggplot(). specify following elements:data: data set containing variables visualized.data: data set containing variables visualized.aes: (aesthetic) attributes geometric object visualized. can x y variables, colour, shape, grouping variable etc.aes: (aesthetic) attributes geometric object visualized. can x y variables, colour, shape, grouping variable etc.geom: geometric object want plot, .e.¬†lines, points, bars, boxes etc.geom: geometric object want plot, .e.¬†lines, points, bars, boxes etc.","code":""},{"path":"import-visualize-and-explore-data.html","id":"line-plot","chapter":"4 Import, visualize and explore data","heading":"4.2.1 Line plot","text":"start line plot particularly suited time series. plotting 33 countries one graph much, first filter France emissions cars.call ggplot() prepares plotting area requested, show anything specify geometric object. geometric objects begin geom_. Every step building plot appended core call +.call can verbalized like following:Take data set emissions map following attributes:\n\\(x\\)-axis, variable time\n\\(y\\)-axis, variable values\nTake data set emissions map following attributes:\\(x\\)-axis, variable timeon \\(y\\)-axis, variable valuesPlot data line (geom_line())Plot data line (geom_line())order plot useful, label axes correctly (give title, figure caption shown). done adding function labs().","code":"\nemissions_france <- emissions %>% \n  filter(geo == 'France' & vehicle == 'Fuel combustion in cars')\nggplot(data = emissions_france, mapping = aes(x = time, y = values))\nggplot(data = emissions_france, mapping = aes(x = time, y = values)) +\n  geom_line()\nggplot(data = emissions_france, mapping = aes(x = time, y = values)) + \n  geom_line() +\n  labs(x = 'Time', y = 'Emissions (Mio tons)', title = 'Emissions in France')"},{"path":"import-visualize-and-explore-data.html","id":"point-plot","chapter":"4 Import, visualize and explore data","heading":"4.2.2 Point plot","text":"can add points plot geom_point(). Normally, wouldn‚Äôt time series, want show geom üòÑ.select two countries, aesthetic required distinguish time series. Let‚Äôs select France Italy.plot countries using different colours. Note (yet) select colours manually, specify variable used distinguish time series. colours chosen one country automatically.legend comes free! can change title legend setting colour = 'Country' call labs().","code":"\nggplot(data = emissions_france, mapping = aes(x = time, y = values)) + \n  geom_line() +\n  geom_point() +\n  labs(x = 'Time', y = 'Emissions (Mio tons)', title = 'Emissions in France')\nemissions_france_italy <- emissions %>% \n  filter(geo %in% c('France', 'Italy') & vehicle == 'Fuel combustion in cars')\nggplot(data = emissions_france_italy, mapping = aes(x = time, y = values, colour = geo)) + \n  geom_line() +\n  geom_point() +\n  labs(x = 'Time', y = 'Emissions (Mio tons)', title = 'Emissions in France and Italy', colour = 'Country')"},{"path":"import-visualize-and-explore-data.html","id":"histogram","chapter":"4 Import, visualize and explore data","heading":"4.3 Histogram","text":"Let‚Äôs look distribution emissions year 2018. filter data first.plot data histogram shows absolute frequencies data (.e.¬†many data points fall particular interval emissions). shows distribution continuous variable. histogram, specify x variable, frequencies calculated geom_histogram() directly. specify 25 bins (intervals). familiar kind statistical summaries, please look Appendix Ismay Kim (2021) read part .1.5 Distribution.","code":"\nemissions_2018 <- emissions %>% \n  filter(time == '2018-01-01')\nggplot(data = emissions_2018, mapping = aes(x = values)) +\n  geom_histogram(bins = 25)## Warning: Removed 8 rows containing non-finite values (stat_bin)."},{"path":"import-visualize-and-explore-data.html","id":"boxplot","chapter":"4 Import, visualize and explore data","heading":"4.4 Boxplot","text":"boxplot calculates prominent statistics data set plots form box ‚Äòwhiskers‚Äô (thus also called box--whiskers plot). Basically, calculating summary() (five-numbers: min, max, 25%, 50% 75% quantiles), figure. familiar kind statistical summaries, please look Appendix Ismay Kim (2021) read part .1.4 Five-number summary.Let‚Äôs look kind summary plot. emissions distributed year? convert time factor variable display data correctly (try happens don‚Äôt convert ).Hmmm, labels \\(x\\)-axis ugly. Let‚Äôs tune little (tuning later sessions).","code":"\nggplot(data = emissions, mapping = aes(x = factor(time), y = values)) +\n  geom_boxplot()## Warning: Removed 232 rows containing non-finite values (stat_boxplot).\nggplot(data = emissions, mapping = aes(x = factor(time), y = values)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90))## Warning: Removed 232 rows containing non-finite values (stat_boxplot)."},{"path":"import-visualize-and-explore-data.html","id":"barplot","chapter":"4 Import, visualize and explore data","heading":"4.4.1 Barplot","text":"last geom want see geom_bar(). like know many data entries emissions contain per vehicle.Admittedly, boring plot üòÑ, number entries identical.","code":"\nggplot(data = emissions, mapping = aes(x = vehicle)) +\n  geom_bar()"},{"path":"import-visualize-and-explore-data.html","id":"practice-on-your-own-3","chapter":"4 Import, visualize and explore data","heading":"4.5 Practice on your own!","text":"histogram, boxplot barplot plotted labelled correctly. Correct axis labels find good titles graphs.histogram, boxplot barplot plotted labelled correctly. Correct axis labels find good titles graphs.Plot time series GDP data set gapminder France Germany. Filter data like :\nfrance_germany <- gapminder %>% filter(country %% c('France', 'Germany'))Plot time series GDP data set gapminder France Germany. Filter data like :\nfrance_germany <- gapminder %>% filter(country %% c('France', 'Germany'))Plot life expectancy vs.¬†GDP 2007, use data set gapminder. Pick code filtering data task C.1.3. Use aesthetics colour size. educated guess change title legends (google üòÑ).Plot life expectancy vs.¬†GDP 2007, use data set gapminder. Pick code filtering data task C.1.3. Use aesthetics colour size. educated guess change title legends (google üòÑ).GDP distributed Africa Europe 2007? Use data set gapminder. Filter data like :\nafrica_europe <- gapminder2007 %>% filter(continent %% c('Africa', 'Europe')).\n\nPlot data histogram use aesthetic fill instead colour distinguish continents.GDP distributed Africa Europe 2007? Use data set gapminder. Filter data like :\nafrica_europe <- gapminder2007 %>% filter(continent %% c('Africa', 'Europe')).\n\nPlot data histogram use aesthetic fill instead colour distinguish continents.GDP distributed different continents 2007? Use data set gapminder. Plot data boxplot.GDP distributed different continents 2007? Use data set gapminder. Plot data boxplot.many data points gapminder contain per continent? Visualize barplot.many data points gapminder contain per continent? Visualize barplot.","code":""},{"path":"import-visualize-and-explore-data.html","id":"reading-assignment-2","chapter":"4 Import, visualize and explore data","heading":"4.6 Reading assignment","text":"Chapter 2.1 Ismay Kim (2021)","code":""},{"path":"import-visualize-and-explore-data.html","id":"turning-in-your-work-3","chapter":"4 Import, visualize and explore data","heading":"4.7 Turning in your work","text":"Save R Notebook *.Rmd file.Upload R Notebook ILIAS. don‚Äôt need upload .nb.html file. find upload option today‚Äôs session.receive solution file upload. sure upload deadline!","code":""},{"path":"import-visualize-and-explore-data.html","id":"additional-reading-1","chapter":"4 Import, visualize and explore data","heading":"4.8 Additional reading","text":"Chapters 2.2 2.9 Ismay Kim (2021)","code":""},{"path":"tidyverse.html","id":"tidyverse","chapter":"5 Explorative workflow with tidyverse","heading":"5 Explorative workflow with tidyverse","text":"\nName core packages tidyverse\n\nApply simple explorative workflow (read, summarize, plot) \ntidyverse\n\nUse functions dplyr data wrangling\ntidyverse collection R packages data analysis (https://www.tidyverse.org/). shares common philosophy data structure grammar data manipulation visualization. Although might sound like something alien, tidyverse regular part R functions can mixed base R functions.best introduction tidyverse r4ds: ‚ÄúR Data Science‚Äù (Wickham Grolemund 2021). can read free (https://r4ds..co.nz/).","code":""},{"path":"tidyverse.html","id":"core-packages","chapter":"5 Explorative workflow with tidyverse","heading":"5.1 Core packages","text":"tidyverse comprises 8 core packages installed call install.packages('tidyverse'):packages Cheat Sheet, overview functions. get package‚Äôs cheat sheet, click name (https://www.tidyverse.org/packages/), scroll section Cheatsheet.Besides core packages, tidyverse also installs long list supplementary packages can find : https://www.tidyverse.org/packages/","code":""},{"path":"tidyverse.html","id":"exploratory-data-analysis","chapter":"5 Explorative workflow with tidyverse","heading":"5.2 Exploratory data analysis","text":"Exploratory data analysis essential first step data analysis. using advanced statistical method, exploratory analysis must-. comprises roughly following steps:import inspect dataclean (tidy) data necessarysummarize create new variables necessaryplot many plots possible get good overview patterns data distribution","code":""},{"path":"tidyverse.html","id":"read-data-revisited","chapter":"5 Explorative workflow with tidyverse","heading":"5.2.1 Read data, revisited","text":"load library tidyverse first.Last time used function read_delim() import data R. function general whole family functions, starting read_*: read_csv(), read_csv2() etc. parameters need verify respective help pages want use .exploratory data analysis, use data German Meteorological Service (Deutscher Wetterdienst) downloaded 2020-05-24 (https://www.dwd.de/DE/leistungen/klimadatendeutschland/klimadatendeutschland.html). data set contains hourly measurements relative air humidity (%), air temperature (¬∞C) three weather stations, namely Hof, Frankfurt K√∂ln-Bonn. data named meteo.csv.read_delim() reports reading data variables recognizes. good hint spot possible problems. numerical variables read <dbl>? characters recognized <char> etc. code , parameter trim_ws = T removes leading zeroes.Let‚Äôs short glimpse data.data set contains following variables:read_* always returns tibble.","code":"\nlibrary(tidyverse)\ntemp_humid <- read_delim('data/meteo.csv', delim = ';',    trim_ws = T)## Rows: 39600 Columns: 6\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \";\"\n## chr (1): eor\n## dbl (5): STATIONS_ID, MESS_DATUM, QN_9, TT_TU, RF_TU\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\ntemp_humid## # A tibble: 39,600 √ó 6\n##    STATIONS_ID MESS_DATUM  QN_9 TT_TU RF_TU eor  \n##          <dbl>      <dbl> <dbl> <dbl> <dbl> <chr>\n##  1        2261 2018111900     3  -2.8    99 eor  \n##  2        2261 2018111901     3  -2.5   100 eor  \n##  3        2261 2018111902     3  -2.3   100 eor  \n##  4        2261 2018111903     3  -2     100 eor  \n##  5        2261 2018111904     3  -1.9    99 eor  \n##  6        2261 2018111905     3  -2.1    99 eor  \n##  7        2261 2018111906     3  -1.8    99 eor  \n##  8        2261 2018111907     3  -1.5    99 eor  \n##  9        2261 2018111908     3  -1.1    99 eor  \n## 10        2261 2018111909     3  -0.6    97 eor  \n## # ‚Ä¶ with 39,590 more rows\nclass(temp_humid)## [1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\""},{"path":"tidyverse.html","id":"date-and-time-made-easy","chapter":"5 Explorative workflow with tidyverse","heading":"5.2.2 Date and time made easy","text":"useful package handle date time called lubridate. part core packages tidyverse installed long list additional packages. use convert variable MESS_DATUM real date-time variable.function ymd_h() converts character vectors date-time objects provided format year, month, day, hour. function different formats; consult help.conversion, variables recognized <dttm> date-time.","code":"\nlibrary(lubridate)\n\ntemp_humid$MESS_DATUM <- ymd_h(temp_humid$MESS_DATUM)\n\ntemp_humid## # A tibble: 39,600 √ó 6\n##    STATIONS_ID MESS_DATUM           QN_9 TT_TU RF_TU eor  \n##          <dbl> <dttm>              <dbl> <dbl> <dbl> <chr>\n##  1        2261 2018-11-19 00:00:00     3  -2.8    99 eor  \n##  2        2261 2018-11-19 01:00:00     3  -2.5   100 eor  \n##  3        2261 2018-11-19 02:00:00     3  -2.3   100 eor  \n##  4        2261 2018-11-19 03:00:00     3  -2     100 eor  \n##  5        2261 2018-11-19 04:00:00     3  -1.9    99 eor  \n##  6        2261 2018-11-19 05:00:00     3  -2.1    99 eor  \n##  7        2261 2018-11-19 06:00:00     3  -1.8    99 eor  \n##  8        2261 2018-11-19 07:00:00     3  -1.5    99 eor  \n##  9        2261 2018-11-19 08:00:00     3  -1.1    99 eor  \n## 10        2261 2018-11-19 09:00:00     3  -0.6    97 eor  \n## # ‚Ä¶ with 39,590 more rows"},{"path":"tidyverse.html","id":"summarize","chapter":"5 Explorative workflow with tidyverse","heading":"5.2.3 Summarize","text":"three weather station following IDs:want know many measurements per station data set contains.operator %>% called pipe pronounced . code temp_humid %>% group_by(STATIONS_ID) %>% count() can read : take object temp_humid, group STATIONS_ID count measurements group. pipe operator comes package magrittr (https://magrittr.tidyverse.org/). core operator tidyverse makes code readable easier follow humans. Perhaps beginning, soon ü§ì.","code":"\nstation_ids <-  c('2261' = 'Hof', '1420' = 'Frankfurt', '2667' = 'Koeln')\ntemp_humid %>% \n  group_by(STATIONS_ID) %>% \n  count()## # A tibble: 3 √ó 2\n## # Groups:   STATIONS_ID [3]\n##   STATIONS_ID     n\n##         <dbl> <int>\n## 1        1420 13200\n## 2        2261 13200\n## 3        2667 13200"},{"path":"tidyverse.html","id":"the-grammar-of-data-manipulation-dplyr","chapter":"5 Explorative workflow with tidyverse","heading":"5.2.4 The grammar of data manipulation ‚Äì dplyr","text":"function count() part library dplyr, collection functions named verbs. Thus, easy imagine function üòÑ). 5 core functions :want know many measurements recorded particular weather station, first filter ID:function filter() accepts logical tests. every row STATION_ID, == checks whether entry equals 2667. == logical operator means left side equals right sight. case, == returns TRUE otherwise returns FALSE. filter() selects rows TRUE returned. useful logical operators :logical boolean operators handled tutorials (see ) help pages filter().can combine several tests operator |, example. , want filter rows containing either ID 2667 ID 2261:can achieved excluding third station:alternative, can use operator %% checks whether row contains one entries vector.","code":"\ntemp_humid %>% \n  filter(STATIONS_ID == '2667') %>%\n  count()## # A tibble: 1 √ó 1\n##       n\n##   <int>\n## 1 13200\ntemp_humid %>% \n  filter(STATIONS_ID == '2667' | STATIONS_ID == '2261') %>%\n  group_by(STATIONS_ID) %>% \n  count()## # A tibble: 2 √ó 2\n## # Groups:   STATIONS_ID [2]\n##   STATIONS_ID     n\n##         <dbl> <int>\n## 1        2261 13200\n## 2        2667 13200\ntemp_humid %>% \n  filter(STATIONS_ID != '1420') %>%\n  group_by(STATIONS_ID) %>% \n  count()## # A tibble: 2 √ó 2\n## # Groups:   STATIONS_ID [2]\n##   STATIONS_ID     n\n##         <dbl> <int>\n## 1        2261 13200\n## 2        2667 13200\ntemp_humid %>% \n  filter(STATIONS_ID %in% c('2667', '2261')) %>%\n  group_by(STATIONS_ID) %>% \n  count()## # A tibble: 2 √ó 2\n## # Groups:   STATIONS_ID [2]\n##   STATIONS_ID     n\n##         <dbl> <int>\n## 1        2261 13200\n## 2        2667 13200"},{"path":"tidyverse.html","id":"visualize","chapter":"5 Explorative workflow with tidyverse","heading":"5.2.5 Visualize","text":"plot time series use trick split along three different plots function facet_wrap(). needs variable separate data plots, chose STATIONS_ID. splitting variable must preceded ~.","code":"\nggplot(data = temp_humid, aes(x = MESS_DATUM, y = TT_TU)) + \n  geom_line() +\n  facet_wrap(~STATIONS_ID, nrow = 3) +\n  labs(x = 'Time', y = 'Temperature (¬∞C)')"},{"path":"tidyverse.html","id":"create-new-variables-with-mutate","chapter":"5 Explorative workflow with tidyverse","heading":"5.2.6 Create new variables with mutate()","text":"want calculate monthly means standard deviations air temperature humidity. First, need generate temporal information, namely year month, used group temperature values calculate mean() sd(). can achieved functions year()month() library lubridate. function mutate() can create new variables data object.next step, create new data set calculate means standard deviations. get station, year month, group data accordingly. group several variables, just enumerate comma (quotation c() necessary).new object monthly_means grouped tibble, indicated grouped_df output str() shows structure object.calculations better done ungrouped data. Therefore, remove grouping. change data .plot monthly data, need proper monthly date object. attribute monthly means first respective month. , lubridate helps task. function parse_dat_time() general function taking character string returning date-time object. need ‚Äúglue‚Äù variables year month together paste0() (yes, zero, O!) form string specify orders = 'ym', .e.¬†year month. Finally, relocate() new variable year_month variable year convenience (, created last variable data set).Now, can plot mean air temperature.can also visualize standard deviations.use semi-transparent band show variability (standard deviation).One last detail. titles top facets show station IDs. employee German Meteorological Service, probably know hart. better use city names. vector station_ids called named vector right structure change titles facets: assigns every id city name, .e.¬†2261 = ‚ÄòHof‚Äô.use station_ids change titles:","code":"\ntemp_humid <- temp_humid %>% \n  mutate(year = year(MESS_DATUM),\n         month = month(MESS_DATUM))\n\ntemp_humid## # A tibble: 39,600 √ó 8\n##    STATIONS_ID MESS_DATUM           QN_9 TT_TU RF_TU eor    year month\n##          <dbl> <dttm>              <dbl> <dbl> <dbl> <chr> <dbl> <dbl>\n##  1        2261 2018-11-19 00:00:00     3  -2.8    99 eor    2018    11\n##  2        2261 2018-11-19 01:00:00     3  -2.5   100 eor    2018    11\n##  3        2261 2018-11-19 02:00:00     3  -2.3   100 eor    2018    11\n##  4        2261 2018-11-19 03:00:00     3  -2     100 eor    2018    11\n##  5        2261 2018-11-19 04:00:00     3  -1.9    99 eor    2018    11\n##  6        2261 2018-11-19 05:00:00     3  -2.1    99 eor    2018    11\n##  7        2261 2018-11-19 06:00:00     3  -1.8    99 eor    2018    11\n##  8        2261 2018-11-19 07:00:00     3  -1.5    99 eor    2018    11\n##  9        2261 2018-11-19 08:00:00     3  -1.1    99 eor    2018    11\n## 10        2261 2018-11-19 09:00:00     3  -0.6    97 eor    2018    11\n## # ‚Ä¶ with 39,590 more rows\nmonthly_means <- temp_humid %>%\n  group_by(STATIONS_ID, year, month) %>% \n  summarize(mean_T = mean(TT_TU), mean_RH = mean(RF_TU),\n            sd_T = sd(TT_TU), sd_RH = sd(RF_TU))## `summarise()` has grouped output by 'STATIONS_ID', 'year'. You can override\n## using the `.groups` argument.\nmonthly_means## # A tibble: 57 √ó 7\n## # Groups:   STATIONS_ID, year [9]\n##    STATIONS_ID  year month mean_T mean_RH  sd_T sd_RH\n##          <dbl> <dbl> <dbl>  <dbl>   <dbl> <dbl> <dbl>\n##  1        1420  2018    11   4.00    79.7  1.82  9.96\n##  2        1420  2018    12   4.73    83.7  4.20 11.7 \n##  3        1420  2019     1   2.12    79.3  3.76 10.0 \n##  4        1420  2019     2   4.48    74.1  4.69 17.7 \n##  5        1420  2019     3   8.28    68.5  4.08 16.1 \n##  6        1420  2019     4  11.7     61.0  5.52 21.8 \n##  7        1420  2019     5  12.7     67.5  4.64 20.1 \n##  8        1420  2019     6  21.4     60.6  6.05 21.2 \n##  9        1420  2019     7  21.6     55.6  5.90 21.8 \n## 10        1420  2019     8  20.7     65.6  4.94 20.8 \n## # ‚Ä¶ with 47 more rows\nstr(monthly_means)## gropd_df [57 √ó 7] (S3: grouped_df/tbl_df/tbl/data.frame)\n##  $ STATIONS_ID: num [1:57] 1420 1420 1420 1420 1420 1420 1420 1420 1420 1420 ...\n##  $ year       : num [1:57] 2018 2018 2019 2019 2019 ...\n##  $ month      : num [1:57] 11 12 1 2 3 4 5 6 7 8 ...\n##  $ mean_T     : num [1:57] 4 4.73 2.12 4.48 8.28 ...\n##  $ mean_RH    : num [1:57] 79.7 83.7 79.3 74.1 68.5 ...\n##  $ sd_T       : num [1:57] 1.82 4.2 3.76 4.69 4.08 ...\n##  $ sd_RH      : num [1:57] 9.96 11.68 10.04 17.73 16.1 ...\n##  - attr(*, \"groups\")= tibble [9 √ó 3] (S3: tbl_df/tbl/data.frame)\n##   ..$ STATIONS_ID: num [1:9] 1420 1420 1420 2261 2261 ...\n##   ..$ year       : num [1:9] 2018 2019 2020 2018 2019 ...\n##   ..$ .rows      : list<int> [1:9] \n##   .. ..$ : int [1:2] 1 2\n##   .. ..$ : int [1:12] 3 4 5 6 7 8 9 10 11 12 ...\n##   .. ..$ : int [1:5] 15 16 17 18 19\n##   .. ..$ : int [1:2] 20 21\n##   .. ..$ : int [1:12] 22 23 24 25 26 27 28 29 30 31 ...\n##   .. ..$ : int [1:5] 34 35 36 37 38\n##   .. ..$ : int [1:2] 39 40\n##   .. ..$ : int [1:12] 41 42 43 44 45 46 47 48 49 50 ...\n##   .. ..$ : int [1:5] 53 54 55 56 57\n##   .. ..@ ptype: int(0) \n##   ..- attr(*, \".drop\")= logi TRUE\nmonthly_means <- ungroup(monthly_means)\nmonthly_means <- monthly_means %>%\n  mutate(year_month = parse_date_time(paste0(year, month), orders = 'ym', tz = 'CET')) %>% \n  relocate(year_month, .before = year)\n\nmonthly_means## # A tibble: 57 √ó 8\n##    STATIONS_ID year_month           year month mean_T mean_RH  sd_T sd_RH\n##          <dbl> <dttm>              <dbl> <dbl>  <dbl>   <dbl> <dbl> <dbl>\n##  1        1420 2018-11-01 00:00:00  2018    11   4.00    79.7  1.82  9.96\n##  2        1420 2018-12-01 00:00:00  2018    12   4.73    83.7  4.20 11.7 \n##  3        1420 2019-01-01 00:00:00  2019     1   2.12    79.3  3.76 10.0 \n##  4        1420 2019-02-01 00:00:00  2019     2   4.48    74.1  4.69 17.7 \n##  5        1420 2019-03-01 00:00:00  2019     3   8.28    68.5  4.08 16.1 \n##  6        1420 2019-04-01 00:00:00  2019     4  11.7     61.0  5.52 21.8 \n##  7        1420 2019-05-01 00:00:00  2019     5  12.7     67.5  4.64 20.1 \n##  8        1420 2019-06-01 00:00:00  2019     6  21.4     60.6  6.05 21.2 \n##  9        1420 2019-07-01 00:00:00  2019     7  21.6     55.6  5.90 21.8 \n## 10        1420 2019-08-01 00:00:00  2019     8  20.7     65.6  4.94 20.8 \n## # ‚Ä¶ with 47 more rows\nggplot(data = monthly_means, aes(x = year_month, y = mean_T, col = factor(STATIONS_ID))) + \n  geom_line() + \n  labs(x = 'Time', y = 'Temperature (¬∞C)', color = 'Meteo station')\nggplot(monthly_means, aes(x = year_month, y = mean_T, ymin = mean_T - sd_T, ymax = mean_T + sd_T)) +\n  geom_errorbar() +\n  geom_point() +\n  facet_wrap(~STATIONS_ID, nrow = 3) + \n  labs(x = 'Time', y = 'Temperature (¬∞C)')\nggplot(monthly_means, aes(x = year_month, y = mean_T, ymin = mean_T - sd_T, ymax = mean_T + sd_T)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line() +\n  facet_wrap(~STATIONS_ID, nrow = 3) + \n  labs(x = 'Time', y = 'Temperature (¬∞C)')\nstation_ids##        2261        1420        2667 \n##       \"Hof\" \"Frankfurt\"     \"Koeln\"\nggplot(monthly_means, aes(x = year_month, y = mean_T, ymin = mean_T - sd_T, ymax = mean_T + sd_T)) +\n  geom_ribbon(alpha = 0.5) +\n  geom_line() +\n  facet_wrap(~STATIONS_ID, nrow = 3, labeller = labeller(STATIONS_ID = station_ids)) + \n  labs(x = 'Time', y = 'Temperature (¬∞C)')"},{"path":"tidyverse.html","id":"practice-on-your-own-4","chapter":"5 Explorative workflow with tidyverse","heading":"5.3 Practice on your own!","text":"Plot means standard deviations air humidity instead air temperature.Plot means standard deviations air humidity instead air temperature.tutorials ‚ÄúWork data‚Äù Primers collection RStudio Cloud. can access tutorials : https://rstudio.cloud/learn/primers/2Do tutorials ‚ÄúWork data‚Äù Primers collection RStudio Cloud. can access tutorials : https://rstudio.cloud/learn/primers/2Do tutorials ‚ÄúVisualize Data‚Äù Primers collection RStudio Cloud. can access tutorials :https://rstudio.cloud/learn/primers/3Do tutorials ‚ÄúVisualize Data‚Äù Primers collection RStudio Cloud. can access tutorials :https://rstudio.cloud/learn/primers/3","code":""},{"path":"tidyverse.html","id":"reading-assignment-3","chapter":"5 Explorative workflow with tidyverse","heading":"5.4 Reading assignment","text":"Read chapter 3 (3.5) Ismay Kim (2021)","code":""},{"path":"tidyverse.html","id":"additional-reading-and-videos","chapter":"5 Explorative workflow with tidyverse","heading":"5.5 Additional reading and videos","text":"information statistical graphical summaries geoms: R4DS Wickham Grolemund (2021): Chapter 5 ‚ÄúData transformation‚Äùinformation statistical graphical summaries geoms: R4DS Wickham Grolemund (2021): Chapter 5 ‚ÄúData transformation‚Äùlive exploratory data analysis main author tidyverse, Hadley Wickham. Really informative, Dr.¬†Wickham types fast üòÑ.live exploratory data analysis main author tidyverse, Hadley Wickham. Really informative, Dr.¬†Wickham types fast üòÑ.","code":""},{"path":"sampling.html","id":"sampling","chapter":"6 Sampling and variability","heading":"6 Sampling and variability","text":"\nConduct random sampling using computer\n\nExplain variability random sampling\n\nCalculate sampling distributions\n\nCalculate standard error\nchapter, start journey statistical inference. Statistical inference simply inference goes beyond analysis single data sets generalizes patterns observed single data set larger context. Often, context called population. generalization techniques nothing else estimation population parameters. example, want know mean income large group people, can either ask every person (time money ) ask cleverly chosen group , sample, try estimate mean income, mean income whole group.Another setting, want use inference, experiments, e.g.¬†lab. Imagine want study influence increased temperature growth plant species. design experiment plants species grown ambient temperature (control group) increased temperature (treatment group). measure growth want know whether difference observed due chance whether real difference, effect treatment. real difference, large precisely can estimate effect. questions can answered using inference.inference based data science computer. Nowadays, computational power usually longer problem cool statistical inference can done based computer simulations called resampling techniques.chapter, learn use computer experiment draw samples simulated data set. see every random sample different. experience elucidate concept randomness chance variability.","code":""},{"path":"sampling.html","id":"random-sampling-of-data","chapter":"6 Sampling and variability","heading":"6.1 Random sampling of data","text":"use following libraries. library infer dedicated package tidy inference.simulate data set, population, invent getsmarter university 12000 students. code draws random numbers simulates following variables:student_id: 1 12000gender: male femaletravel_time: time students travel university minutesresidence: type place residents, either town = urban countryside = ruraltransport: students travel/come universitytime_lib: time students spend university library minutesWe organize variables tibble call getsmarter_pop.draw random numbers using R, every time rerun code, new numbers generated. reproducible population, use function set.seed(). accepts parameter integer. integer use really matter long use every time rerun code. setting seed, set random number generate R certain reproducible state. numbers still random, reproducible üòÑ.conduct survey among students getsmarter record values variables listed . select students randomly. Therefore, data set generate survey random sample. simulate survey, use function rep_sample_n(). samples repeatedly n data points (students case) given data set (population getsmarter_pop).sample randomly (use random number generator R ), need set seed reproducibility. simulate survey 50 students define survey size survey_size <- 50. survey done , parameter reps = 1, students can interviewed , parameter replace = FALSE, function rep_sample_n().function rep_sample_n returns grouped tibble. variable replicate contained number 1 rep = 1. indicator variable replication.many students survey live town many countryside?Instead actual numbers, want express residence proportions.42% survey students live countryside 58% town.proportions obtain repeated survey? Let‚Äôs repeat survey 33 times observe variability residence. unrealistic scenario real life, however, computer experiment, problem. set reps = 33.Now survey_reps shows 33 replicates variable replicate ranges 1 33. data set 1650 = 33 \\(\\times\\) survey_size rows.proportions residence vary survey survey? Now must group residence replicate.numbers vary replicate replicate every survey replicate different students drawn random. Let‚Äôs look distribution proportions histogram.frequent proportions rural around 40% urban around 60%. chosen binwidth = 0.05, .e.¬†width columns 5%, can precise. Among surveyed students, 35‚Äì40% live countryside 55‚Äì60% town.","code":"\nlibrary(tidyverse)\nlibrary(moderndive)\nset.seed(123)\n\nstudent_id <- 1:12000\n  \ntravel_time <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n             runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngender <- sample(c('m', 'f'), size = 12000, replace = TRUE)\n\nresidence <- sapply(travel_time, function(x) {\n  if(x < 30) 'urban'\n  else 'rural'\n})\n\ntransport <- sapply(travel_time, function(x) {\n  if(x <= 10) 'foot'\n  else if(x > 10 & x <= 15) sample(c('foot', 'bike'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'bike', 'car'), size = 1)\n  else sample(c('bus', 'car'), size = 1)\n})\n\ntime_lib <- 5 * 60 - 0.7 * travel_time + rnorm(length(travel_time), 0, 20)\n\ngetsmarter_pop <- tibble(student_id, gender, residence, transport, travel_time, time_lib)\n\ngetsmarter_pop## # A tibble: 12,000 √ó 6\n##    student_id gender residence transport travel_time time_lib\n##         <int> <chr>  <chr>     <chr>           <dbl>    <dbl>\n##  1          1 f      urban     bus             15.1      294.\n##  2          2 f      rural     bike            32.6      254.\n##  3          3 f      urban     bike            19.3      231.\n##  4          4 m      rural     car             35.9      245.\n##  5          5 m      rural     bus             37.9      234.\n##  6          6 f      urban     foot             6.59     303.\n##  7          7 f      urban     bus             23.5      284.\n##  8          8 m      rural     car             36.2      274.\n##  9          9 m      urban     bike            24.3      299.\n## 10         10 f      urban     bus             21.0      282.\n## # ‚Ä¶ with 11,990 more rows\nset.seed(345)\n\nsurvey_size <- 50\n\nsurvey <- rep_sample_n(getsmarter_pop, size = survey_size, replace = FALSE, reps = 1)\nsurvey## # A tibble: 50 √ó 7\n## # Groups:   replicate [1]\n##    replicate student_id gender residence transport travel_time time_lib\n##        <int>      <int> <chr>  <chr>     <chr>           <dbl>    <dbl>\n##  1         1       1623 m      urban     foot             7.06     299.\n##  2         1       9171 m      urban     bike            11.3      278.\n##  3         1      10207 f      rural     bus            107.       199.\n##  4         1       3506 f      urban     bus             25.0      326.\n##  5         1       8892 f      urban     bus             28.1      259.\n##  6         1       5460 m      urban     bus             23.6      299.\n##  7         1       6120 f      urban     bus             20.0      268.\n##  8         1        865 f      urban     bike            26.6      290.\n##  9         1      11586 m      rural     bus            114.       207.\n## 10         1       8153 f      urban     foot             8.06     297.\n## # ‚Ä¶ with 40 more rows\nsurvey %>% \n  group_by(residence) %>% \n  count()## # A tibble: 2 √ó 2\n## # Groups:   residence [2]\n##   residence     n\n##   <chr>     <int>\n## 1 rural        21\n## 2 urban        29\nsurvey %>% \n  group_by(residence) %>% \n  summarise(prop = n()/survey_size)## # A tibble: 2 √ó 2\n##   residence  prop\n##   <chr>     <dbl>\n## 1 rural      0.42\n## 2 urban      0.58\nset.seed(234)\n\nsurvey_reps <- rep_sample_n(getsmarter_pop, size = survey_size, replace = FALSE, reps = 33)\nsurvey_reps## # A tibble: 1,650 √ó 7\n## # Groups:   replicate [33]\n##    replicate student_id gender residence transport travel_time time_lib\n##        <int>      <int> <chr>  <chr>     <chr>           <dbl>    <dbl>\n##  1         1       2079 m      rural     car              38.8     262.\n##  2         1       1314 m      urban     bike             13.3     301.\n##  3         1       1710 m      urban     car              26.5     272.\n##  4         1       4386 f      urban     bus              23.9     269.\n##  5         1       9490 m      rural     car              34.2     262.\n##  6         1      11757 f      rural     bus             102.      227.\n##  7         1      11649 f      rural     bus             111.      202.\n##  8         1       2244 m      rural     bus              38.9     256.\n##  9         1       3652 f      urban     bike             10.3     254.\n## 10         1       3127 m      urban     bike             29.6     271.\n## # ‚Ä¶ with 1,640 more rows\nresidence_props <- survey_reps %>% \n  group_by(replicate, residence) %>% \n  summarise(prop = n()/survey_size)\n\nresidence_props## # A tibble: 66 √ó 3\n## # Groups:   replicate [33]\n##    replicate residence  prop\n##        <int> <chr>     <dbl>\n##  1         1 rural      0.42\n##  2         1 urban      0.58\n##  3         2 rural      0.36\n##  4         2 urban      0.64\n##  5         3 rural      0.4 \n##  6         3 urban      0.6 \n##  7         4 rural      0.38\n##  8         4 urban      0.62\n##  9         5 rural      0.4 \n## 10         5 urban      0.6 \n## # ‚Ä¶ with 56 more rows\nggplot(data = residence_props, aes(x = prop)) + \n  geom_histogram(binwidth = 0.05, boundary = 0.4, col = 'white') +\n  facet_wrap(~ residence) +\n  labs(x = 'Proportion of residence', title = 'Distribution of residences', y = 'Frequency')"},{"path":"sampling.html","id":"number-of-replications-and-variability","chapter":"6 Sampling and variability","heading":"6.2 Number of replications and variability","text":"histograms show proportions residence vary survey survey. 33 large number real life, small number statistics. patterns observe histograms repeat survey 1000 times? use variable reps_num define number repetitions.histograms show now nice symmetrical pattern around 40‚Äì42% rural 58‚Äì60% urban residences. parameter scales = 'free_x' allows scaling histogram‚Äôs \\(x\\) axis separately. distribution called sampling distribution. shows distribution possible values statistics (‚Äúproportions residence‚Äù case) one obtains repeated sampling population.statistics ‚Äúproportion residence‚Äù random variable. Every new survey brings new values. sampling distribution can tell us values frequent, .e.¬†probable occur survey students randomly.summarize sampling distribution, can calculate mean value standard deviation. latter special name, standard error.standard errors rural urban identical dependent: rural = 1 - urban.","code":"\nset.seed(345)\n\nreps_num <- 1000\n\nsurvey_reps <- rep_sample_n(getsmarter_pop, size = survey_size, replace = FALSE, reps = reps_num)\n\nresidence_props <- survey_reps %>% \n  group_by(replicate, residence) %>% \n  summarise(prop = n()/survey_size)\n\nggplot(data = residence_props, aes(x = prop)) + \n  geom_histogram(binwidth = 0.02, boundary = 0.4, col = 'white') +\n  facet_wrap(~ residence, scales = 'free_x') +\n  labs(x = 'Proportion of residence', title = 'Distribution of residences', y = 'Frequency')\nresidence_props %>% \n  group_by(residence) %>% \n  summarise(prop_sd = sd(prop))## # A tibble: 2 √ó 2\n##   residence prop_sd\n##   <chr>       <dbl>\n## 1 rural      0.0701\n## 2 urban      0.0701"},{"path":"sampling.html","id":"sample-size","chapter":"6 Sampling and variability","heading":"6.3 Sample size","text":"standard error depends sample size. Let‚Äôs repeat survey 25, 50 100 students, respectively observe standard error vary.repetitive tasks like , better define function job instead copy-pasting code vary sample size. define function without many comments. talk functions later, time allows.Let‚Äôs survey.plot resulting sampling distributions.compare standard errors three sampling distributions.see standard error decreases increasing sample size. makes sense larger sample size, information contains population, .e.¬†representative. Accordingly, variability information (measured standard error) decreases.time, pre-formulated take-home messages sampling variability crucial topics üòé.\nrandom sample\\(^*\\) can used obtain information\nlarger group, population.\n\nstatistics calculated random sample sometimes called \nsampling statistics.\n\ndistribution statistics calculated random samples \ncalled sampling distribution.\n\nsampling distribution obtained repeated random\nsampling.\n\nrandom samples drawn, better one can characterize\nsampling distribution (.e.¬†shape).\n\nstandard error standard deviation \nsampling statistics.\n\nstandard error decreases increasing sample\nsize.\n\\(^*\\) Sometimes, random sampling appropriate stratified sampling complex sampling designs used. case, subgroups different properties relevant sampling, statistics exist population. However, topic beyond scope course.","code":"\ncalculate_props <- function(population = getsmarter_pop, survey_size, reps_num = 1000) {\n  \n  survey <- rep_sample_n(population, size = survey_size, replace = FALSE, reps = reps_num)\n\nresidence_props <- survey %>% \n  group_by(replicate, residence) %>% \n  summarise(prop = n()/survey_size)\n\nresidence_props\n}\nset.seed(123)\n\n# Sample size 25\nresidence_props_25 <- calculate_props(population = getsmarter_pop, survey_size = 25, reps_num = 1000)\n  \n# Sample size 50\nresidence_props_50 <- calculate_props(population = getsmarter_pop, survey_size = 50, reps_num = 1000)\n\n# Sample size 100\nresidence_props_100 <- calculate_props(population = getsmarter_pop, survey_size = 100, reps_num = 1000)\nggplot(data = residence_props_25, aes(x = prop)) + \n  geom_histogram(binwidth = 0.04, boundary = 0.4, col = 'white') +\n  facet_wrap(~ residence, scales = 'free_x') +\n  labs(x = 'Proportion of residence', title = 'Distribution of residences, sample size = 25', y = 'Frequency')\nggplot(data = residence_props_50, aes(x = prop)) + \n  geom_histogram(binwidth = 0.02, boundary = 0.4, col = 'white') +\n  facet_wrap(~ residence, scales = 'free_x') +\n  labs(x = 'Proportion of residence', title = 'Distribution of residences, sample size = 50', y = 'Frequency')\nggplot(data = residence_props_100, aes(x = prop)) + \n  geom_histogram(binwidth = 0.02, boundary = 0.4, col = 'white') +\n  facet_wrap(~ residence, scales = 'free_x') +\n  labs(x = 'Proportion of residence', title = 'Distribution of residences, sample size = 100', y = 'Frequency')\nresidence_props_25 %>% \n  group_by(residence) %>% \n  summarise(prop_sd = sd(prop))## # A tibble: 2 √ó 2\n##   residence prop_sd\n##   <chr>       <dbl>\n## 1 rural       0.102\n## 2 urban       0.102\nresidence_props_50 %>% \n  group_by(residence) %>% \n  summarise(prop_sd = sd(prop))## # A tibble: 2 √ó 2\n##   residence prop_sd\n##   <chr>       <dbl>\n## 1 rural      0.0679\n## 2 urban      0.0679\nresidence_props_100 %>% \n  group_by(residence) %>% \n  summarise(prop_sd = sd(prop))## # A tibble: 2 √ó 2\n##   residence prop_sd\n##   <chr>       <dbl>\n## 1 rural      0.0458\n## 2 urban      0.0458"},{"path":"sampling.html","id":"practice-on-your-own-5","chapter":"6 Sampling and variability","heading":"6.4 Practice on your own!","text":"large proportions rural urban population students getsmarter. large mean values sampling distributions sample sizes 25, 50 100. Compare figures population parameters comment.large proportions rural urban population students getsmarter. large mean values sampling distributions sample sizes 25, 50 100. Compare figures population parameters comment.student representatives like see students using bike bus instead car. first step, need know many students actually use car.\n\nConduct repeated survey (1000 repetitions) 50 students estimate proportion people using car. large standard error proportion? Compare proportion population. Hint: calculate mean proportions estimated 1000 replicates, use ungroup() delete grouping replicates.student representatives like see students using bike bus instead car. first step, need know many students actually use car.\n\nConduct repeated survey (1000 repetitions) 50 students estimate proportion people using car. large standard error proportion? Compare proportion population. Hint: calculate mean proportions estimated 1000 replicates, use ungroup() delete grouping replicates.","code":""},{"path":"sampling.html","id":"reading-assignment-4","chapter":"6 Sampling and variability","heading":"6.5 Reading assignment","text":"Chapter 7 Ismay Kim (2021)","code":""},{"path":"sampling.html","id":"turning-in-your-work-4","chapter":"6 Sampling and variability","heading":"6.6 Turning in your work","text":"Save R Notebook *.Rmd file.Upload R Notebook ILIAS. don‚Äôt need upload .nb.html file. find upload option today‚Äôs session.receive solution file upload. sure upload deadline!","code":""},{"path":"bootstrap.html","id":"bootstrap","chapter":"7 Bootstrap and confidence intervals","heading":"7 Bootstrap and confidence intervals","text":"\nExplain idea behind bootstrap\n\nCalculate bootstrap confidence intervals mean\nchapter 6 studied variability repeated random sampling population. computer experiments sampled simulated population repeatedly without replacement (.e.¬†data appear sample ) calculated statistics, proportion rural urban residents. statistics random variable, characterized sampling distribution. shows values can expect randomly sampling population.learned shape sampling distribution size standard error depended sample size. chapter, learn quantify standard error calculate plausible values (confidence intervals) real-life applications, one sample .","code":""},{"path":"bootstrap.html","id":"the-formulas","chapter":"7 Bootstrap and confidence intervals","heading":"7.1 The formulas","text":"never opportunity sample/survey repeatedly estimate information (.e.¬†statistics parameter) population interest. real-life applications, one (hopefully cleverly obtained) random sample. , can access shape sampling distribution? several standard statistics, formulas describe standard error. probably learned standard error estimated mean equalled \\(\\sigma/\\sqrt(n)\\), \\(\\sigma\\) variance population \\(n\\) sample size. don‚Äôt know variance population, estimate sample . formula come ? crucial Central Limit Theorem tells us random variable mean can estimate random sample normally distributed mean equalled true mean population standard deviation , guessed , \\(\\sigma/\\sqrt(n)\\). Based normal distribution, can calculate plausible values, -called confidence interval. Informally, define confidence interval follows:plausible range values obtain randomly sample population? Plausible means repeated sampling often, range contain true mean let‚Äôs say 95% time. plausible range called 95% confidence interval.know theoretical distribution mean can calculate 95% confidence interval \\(\\hat\\mu \\pm 1.96 \\cdot SE\\), \\(\\hat\\mu\\) estimated mean, \\(SE\\) standard error \\(\\sigma/\\sqrt(n)\\) magic factor 1.96 comes fact normal distribution, 95% values fall interval \\(\\mu \\pm 1.96 \\cdot sd\\), \\(sd\\) standard deviation.","code":""},{"path":"bootstrap.html","id":"bootstrap-use-your-computer","chapter":"7 Bootstrap and confidence intervals","heading":"7.2 Bootstrap ‚Äì use your computer","text":"Formulas date early time statistics, computational power limited unavailable approximations tools one use estimate variability. Central Limit Theorem useful, theoretical distributions always exist. cases, can use computer calculate confidence intervals using re-sampling, .e.¬†repeated sampling random sample. method called bootstrap (bootstrapping) sounds like self-deception √† la Baron Munchausen first glance (Figure 7.1). However, solid mathematical foundation (Efron 1979).\nFigure 7.1: M√ºnchhausen removes swamp using braids (Theodor Hosemann (1807-1875), Public domain, via Wikimedia Commons) Link figure\nFigure 7.2 shows setup practised chapter 6. wanted estimate parameter (proportion rural urban residents) called \\(\\mu\\) figure, sampled randomly several times population. random samples, calculated statistics obtained sampling distribution.\nFigure 7.2: Calculation sampling distribution repeated random sampling population. Figure Hesterberg (2015), Figure 4. publication open-access can used non-commercial purposes. Link licence\nFigure 7.3 looks nearly , except one important difference. don‚Äôt access several random samples population, one random sample . now remember invested time energy thinking best way obtain sample, believe representative population. Thus, can think sample mini population. replace population miniature, .e.¬†sample, sample sample true population. However, sample limited size, sample replacement. random samples obtain sampling replacement original random sample called bootstrap samples. bootstrap samples, calculate statistics obtain bootstrap sampling distribution. confidence intervals often calculated -called percentile method: take 2.5% quantile 97.5% quantile sampling distribution. range values -equals 95% bootstrap confidence interval.\nFigure 7.3: Calculation sampling distribution repeated random sampling replacement sample. Figure Hesterberg (2015), Figure 5. publication open-access can used non-commercial purposes. Link licence\n","code":""},{"path":"bootstrap.html","id":"confidence-intervals-for-the-mean-travel-time","chapter":"7 Bootstrap and confidence intervals","heading":"7.3 Confidence intervals for the mean travel time","text":"Let‚Äôs come back simulated getsmarter university population estimate mean travel time bootstrap confidence interval.survey 200 students.true mean travel time university mean survey equalTo calculate bootstrap distribution, sample data set survey replacement. size bootstrap samples always equal size original sample. use function rep_sample_n chapter 6, change replace = TRUE.sample replacement, students can now appear several times one bootstrap sample. Let‚Äôs check first 50 bootstrap samples. Student 4787 sampled 8 times replicate 43, example.Now can calculate mean travel times bootstrap samples.standard error 95% confidence interval based bootstrap calculated follows:","code":"\nlibrary(tidyverse)\nlibrary(infer)\nset.seed(123)\n\nstudent_id <- 1:12000\n  \ntravel_time <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n             runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngender <- sample(c('m', 'f'), size = 12000, replace = TRUE)\n\nresidence <- sapply(travel_time, function(x) {\n  if(x < 30) 'urban'\n  else 'rural'\n})\n\ntransport <- sapply(travel_time, function(x) {\n  if(x <= 10) 'foot'\n  else if(x > 10 & x <= 15) sample(c('foot', 'bike'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'bike', 'car'), size = 1)\n  else sample(c('bus', 'car'), size = 1)\n})\n\ntime_lib <- 5 * 60 - 0.7 * travel_time + rnorm(length(travel_time), 0, 20)\n\ngetsmarter_pop <- tibble(student_id, gender, residence, transport, travel_time, time_lib)\n\ngetsmarter_pop## # A tibble: 12,000 √ó 6\n##    student_id gender residence transport travel_time time_lib\n##         <int> <chr>  <chr>     <chr>           <dbl>    <dbl>\n##  1          1 f      urban     bus             15.1      294.\n##  2          2 f      rural     bike            32.6      254.\n##  3          3 f      urban     bike            19.3      231.\n##  4          4 m      rural     car             35.9      245.\n##  5          5 m      rural     bus             37.9      234.\n##  6          6 f      urban     foot             6.59     303.\n##  7          7 f      urban     bus             23.5      284.\n##  8          8 m      rural     car             36.2      274.\n##  9          9 m      urban     bike            24.3      299.\n## 10         10 f      urban     bus             21.0      282.\n## # ‚Ä¶ with 11,990 more rows\nset.seed(345)\n\nsurvey_size <- 200\n\nsurvey <- rep_sample_n(getsmarter_pop, size = survey_size, replace = FALSE, reps = 1)\nmean_pop <- getsmarter_pop %>% \n  summarise(mean = mean(travel_time))\n\nmean_pop## # A tibble: 1 √ó 1\n##    mean\n##   <dbl>\n## 1  36.0\nmean_survey <- survey %>% \n  summarise(mean = mean(travel_time))\n\nmean_survey## # A tibble: 1 √ó 2\n##   replicate  mean\n##       <int> <dbl>\n## 1         1  34.1\nset.seed(345)\n\nreps_num <- 10000\n\nsurvey_reps_bootstrap <- rep_sample_n(survey, size = survey_size, replace = TRUE, reps = reps_num)\n\nsurvey_reps_bootstrap## # A tibble: 2,000,000 √ó 7\n## # Groups:   replicate [10,000]\n##    replicate student_id gender residence transport travel_time time_lib\n##        <int>      <int> <chr>  <chr>     <chr>           <dbl>    <dbl>\n##  1         1       7038 m      urban     bus             29.4      243.\n##  2         1        493 m      urban     bus             21.1      303.\n##  3         1       3442 m      rural     car             30.3      311.\n##  4         1       4920 f      urban     bus             21.1      290.\n##  5         1       3178 f      urban     foot             8.07     325.\n##  6         1       7694 f      rural     car             31.3      295.\n##  7         1       9479 m      rural     bus             39.5      302.\n##  8         1       3367 m      rural     car             36.4      272.\n##  9         1       5985 f      urban     bike            15.9      303.\n## 10         1        843 m      urban     bike            26.4      300.\n## # ‚Ä¶ with 1,999,990 more rows\nsurvey_reps_bootstrap %>% \n  filter(replicate %in% (1:50)) %>% \n  group_by(replicate, student_id) %>% \n  tally() %>% \n  filter(n != 1) %>% \n  arrange(desc(n))## # A tibble: 2,657 √ó 3\n## # Groups:   replicate [50]\n##    replicate student_id     n\n##        <int>      <int> <int>\n##  1        43       4787     8\n##  2        20       5985     6\n##  3        34       7749     6\n##  4        38       5641     6\n##  5        41       8456     6\n##  6         3       7083     5\n##  7         3      10943     5\n##  8         5       8994     5\n##  9         6      10118     5\n## 10         8      11425     5\n## # ‚Ä¶ with 2,647 more rows\nres_means_bootstrap <- survey_reps_bootstrap %>%\n  group_by(replicate) %>% \n  summarise(mean_tt = mean(travel_time))\n\nres_means_bootstrap## # A tibble: 10,000 √ó 2\n##    replicate mean_tt\n##        <int>   <dbl>\n##  1         1    32.6\n##  2         2    31.9\n##  3         3    38.9\n##  4         4    33.5\n##  5         5    35.4\n##  6         6    37.2\n##  7         7    34.4\n##  8         8    33.4\n##  9         9    35.5\n## 10        10    31.0\n## # ‚Ä¶ with 9,990 more rows\nstat_bootstrap <- res_means_bootstrap %>% \n  summarize(mean_bootstrap = mean(mean_tt), se = sd(mean_tt), ci_2.5 = quantile(mean_tt, probs = 0.025), ci_97.5 = quantile(mean_tt, probs = 0.975))\n\nstat_bootstrap## # A tibble: 1 √ó 4\n##   mean_bootstrap    se ci_2.5 ci_97.5\n##            <dbl> <dbl>  <dbl>   <dbl>\n## 1           34.1  2.06   30.1    38.3"},{"path":"bootstrap.html","id":"bootstrap-with-the-library-infer","chapter":"7 Bootstrap and confidence intervals","heading":"7.4 Bootstrap with the library infer","text":"library infer offers convenient framework calculating bootstrap confidence intervals hypothesis tests. talk latter later session. infer dedicated package tidy inference organized around 5 verbs:specify() variables relationships themhypothesize() define null hypothesis (hypothesis tests )generate() generate data either confidence intervals null hypothesiscalculate() sampling distributionvisualize() visualize sampling distributionHow example calculating bootstrap confidence intervals mean travel time look like infer workflow?first calculate bootstrap distribution., calculate confidence intervals based percentile method.visualize results. function visualize based ggplot2, can add custom axis labels.","code":"\nset.seed(345)\n\nbootstrap_distribution <- survey %>%\n  specify(response = travel_time) %>% \n  generate(reps = 10000, type = 'bootstrap') %>% \n  calculate(stat = 'mean')\npercentile_ci <- bootstrap_distribution %>% \n  get_confidence_interval(level = 0.95, type = \"percentile\")\npercentile_ci## # A tibble: 1 √ó 2\n##   lower_ci upper_ci\n##      <dbl>    <dbl>\n## 1     30.1     38.3\nvisualize(bootstrap_distribution) + \n  shade_confidence_interval(endpoints = percentile_ci, color = \"orange\", fill = \"khaki\") +\n  geom_vline(xintercept = mean_pop$mean, linetype = 'dashed') +\n  labs(x = 'Mean trave time (min)', y = 'Frequency')"},{"path":"bootstrap.html","id":"interpreting-the-confidence-intervals","chapter":"7 Bootstrap and confidence intervals","heading":"7.5 Interpreting the confidence intervals","text":"upper lower limit confidence interval depend sample, .e.¬†random variables. Definitely, random variables everywhere üòÑ. means confidence interval fail include true population parameter. now ready formal definition confidence interval:repeat sampling often recalculate 95% confidence intervals, expect contain true population parameters 95% time.means confidence intervals won‚Äôt contain true population parameter.confidence interval best guess plausible values population parameter. case theoretical distribution statistics, bootstrap theoretical confidence intervals coincide. Often, interpretation short-handed : 95% confident confidence interval captures true parameter. incorrect. better state 95% time, confidence interval captures true population parameter. now know meant ‚Äú95% time‚Äù üòÑ.","code":""},{"path":"bootstrap.html","id":"practice-on-your-own-6","chapter":"7 Bootstrap and confidence intervals","heading":"7.6 Practice on your own!","text":"Repeat analysis mean travel time now proportion urban residents. Use workflow infer. Hint: specify(response = residence, success = 'urban').Repeat analysis mean travel time now proportion urban residents. Use workflow infer. Hint: specify(response = residence, success = 'urban').width confidence interval depend sample size? Repeat analysis mean travel time survey 30 students.width confidence interval depend sample size? Repeat analysis mean travel time survey 30 students.","code":""},{"path":"bootstrap.html","id":"reading-assignment-5","chapter":"7 Bootstrap and confidence intervals","heading":"7.7 Reading assignment","text":"Chapter 8 Ismay Kim (2021)","code":""},{"path":"bootstrap.html","id":"turning-in-your-work-5","chapter":"7 Bootstrap and confidence intervals","heading":"7.8 Turning in your work","text":"Save R Notebook *.Rmd file.Upload R Notebook ILIAS. don‚Äôt need upload .nb.html file. find upload option today‚Äôs session.receive solution file upload. sure upload deadline!","code":""},{"path":"hypothesis.html","id":"hypothesis","chapter":"8 Hypothesis tests versus effect size estimation","heading":"8 Hypothesis tests versus effect size estimation","text":"\nExplain idea behind simulation-based hypothesis tests\n\nCalculate hypothesis tests infer\n\nExplain difference test effect size\nestimation\nchapter 7 learned calculate confidence intervals, range plausible values. formally, interval expect contain true population parameters 95% time, repeat sampling often. chapter, learn hypothesis tests often better estimate effect sizes.","code":""},{"path":"hypothesis.html","id":"hypothesis-tests-with-infer","chapter":"8 Hypothesis tests versus effect size estimation","heading":"8.1 Hypothesis tests with infer","text":"Let‚Äôs start old study done 1970. Rosen Jerdee (1974) analysed whether ‚Äúsex role stereotypes [] influence personnel decisions‚Äù. modern language, wanted know whether women discriminated promotion decisions. asked 95 bank supervisors decide based personnel file whether person promoted . files identical except gender files distributed randomly participants. random assignment files, procedure experiment, can draw conclusions research question. , look one part data, namely promotion decision simple job (c.f. original publication details.)Let‚Äôs plot data analyse .large difference proportions promotions men women? difference due chance, women discriminated ?hypothesis test helps decide whether observed effect, case difference proportions promotions, can attributed chance. hypothesis test, two different statements contrasted, null hypothesis alternative hypothesis:Statement 1: real difference, observed difference due chance.H\\(_0\\): Null hypothesis. variables gender decision independent; observed difference promotions random.Statement 2: Women discriminated .H\\(_A\\): Alternative hypothesis. variables gender decision dependent. Women discriminated decisions promotion.use general framework package infer hypothesis tests. offers state---art simulation-based approach based following steps (Figure 8.1):specify() variables relationships themhypothesize() define null hypothesisgenerate() generate data null hypothesiscalculate() sampling distribution null hypothesisvisualize() visualize sampling distribution null hypothesisTwo additional functions, shade_p_value get_p_value visualize calculate \\(p\\) value (see definition), respectively.\nFigure 8.1: General framework infer. (Source: https://infer.netlify.app/).\nLet‚Äôs proceed step step. First, calculate observed difference data using infer. parameter order = c(\"male\", \"female\") tells us calculate difference male - female.observe 29.2% less women promoted men.Let‚Äôs see whether difference can attributed chance. generate data null hypothesis, .e.¬†world gender decision promote completely independent. compare data obtain world observed 29.2% difference men women.generate data null hypotheses permutation. means gender decision unrelated, observed combinations . Therefore, permuted combination agreement null hypothesis independence variables. generate 10000 permuted samples.object null_distn contains 10000 differences proportions promotion permuted data. can visualize distribution highlight observed difference compare. parameter direction = \"greater\" defines alternative hypothesis women discriminated , .e.¬†difference larger zero. function shade_p_value shades values larger observed statistics. red bar shows observed statistic .\\(p\\) value probability obtain data extreme extreme observed, null hypothesis true.\\(p\\) value example equalsThis \\(p\\) value really small. rare observe difference large 29.2%, assume women men promoted equally often. reject null hypothesis independence conclude women discriminated promotion. cautionary note: experiment, assumed participating bank supervisors representative profession 1970s. Thus, cautious conclude women discriminated 1970 (provided white male bank supervisors representative -population decision-makers).","code":"\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(knitr)\nstudy <- tibble('gender' = c(rep('male', 24),\n                             rep('female', 24)),\n                'decision' = c(rep('promoted', 21),\n                               rep('not_promoted', 3),\n                               rep('promoted', 14),\n                               rep('not_promoted', 10)))\n\nstudy %>% \n  table() %>% \n  addmargins() %>% \n  kable()\nggplot(study, aes(x = gender, fill = decision)) +\n  geom_bar() +\n  labs(x = 'Gender on personnel file', y = 'Abs. frequency',\n       title = 'Decisions of bank supervisors from study by Rosen & Jerdee (1974)')\nprop_hat <- study %>% \n  specify(formula = decision ~ gender, success = \"promoted\") %>%\n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\n\nprop_hat## Response: decision (factor)\n## Explanatory: gender (factor)\n## # A tibble: 1 √ó 1\n##    stat\n##   <dbl>\n## 1 0.292\nset.seed(123)\n\nnull_distn <- study %>%\n  specify(formula = decision ~ gender, success = \"promoted\") %>%\n  hypothesize(null = \"independence\") %>% \n  generate(reps = 10000) %>% \n  calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))## Setting `type = \"permute\"` in `generate()`.\nnull_distn## Response: decision (factor)\n## Explanatory: gender (factor)\n## Null Hypothesis: independence\n## # A tibble: 10,000 √ó 2\n##    replicate    stat\n##        <int>   <dbl>\n##  1         1  0.125 \n##  2         2  0.292 \n##  3         3 -0.0417\n##  4         4 -0.0417\n##  5         5 -0.125 \n##  6         6 -0.0417\n##  7         7  0.0417\n##  8         8  0.0417\n##  9         9  0.0417\n## 10        10  0.0417\n## # ‚Ä¶ with 9,990 more rows\nvisualize(null_distn, bins = 12) +\n  shade_p_value(obs_stat = prop_hat, direction = \"greater\")\nnull_distn %>% \n  get_p_value(obs_stat = prop_hat, direction = 'greater')## # A tibble: 1 √ó 1\n##   p_value\n##     <dbl>\n## 1  0.0268"},{"path":"hypothesis.html","id":"there-is-only-one-test","chapter":"8 Hypothesis tests versus effect size estimation","heading":"8.2 There is only one test!","text":"read literature, find many tests. difficult keep names remember use . good news unnecessarily complicated can simplified test procedure based computer simulations infer. general logic behind simulation-based tests shown Figure 8.2.\nFigure 8.2: General logic behind simulation-based hypothesis test. (Source: http://allendowney.blogspot.com/2016/06/--still--one-test.html. Used permission author Prof.¬†Allen Downey).\ntest procedure based steps, regardless test want apply.Step 1: calculate test statistic data\nSummarize data test statistic. can mean difference proportions. Figure 8.2 statistic called \\(\\sigma*\\).Step 2: formulate null hypothesis\nThink research question. want know? Derive model world effect absent. null hypothesis, \\(H_0\\). model can permutation variables theoretical model complicated model üòÑ.Step 3: generate data null hypothesis\nGenerate data, .e.¬†many data samples, using model \\(H_0\\) calculate statistic observed data samples.Step 4: calculate sampling distribution\nstatistics null give sampling distribution can visualize. Additionally, visualize observed statistic \\(\\sigma*\\) compare. value often appear sampling distribution rare? \\(p\\) value gives frequency often sampling distribution values least large \\(\\sigma*\\).Step 5: Decide!\n\\(p\\) values small? Use domain knowledge frame result. need refer statistical significance ! Formulate conclusion plain language. Think obtained data. always uncertainty, modest overstate finding!much say misuse \\(p\\) values term ‚Äústatistical significance‚Äù. good starting point paper Wasserstein, Schirm, Lazar (2019).","code":""},{"path":"hypothesis.html","id":"hypothesis-test-versus-effect-size-estimation","chapter":"8 Hypothesis tests versus effect size estimation","heading":"8.3 Hypothesis test versus effect size estimation","text":"Compared statistical test, estimation -called effect size can much interesting. Effect size can difference means proportions elaborated (standardized) effects. statistical test gives ‚Äúfeeling‚Äù whether observed effect attributed chance, estimation effect size confidence intervals gives range plausible values. much interesting relevant discuss mean real life just state something () due chance. Prefer effect size confidence intervals possible hypothesis tests.","code":""},{"path":"hypothesis.html","id":"practice-on-your-own-7","chapter":"8 Hypothesis tests versus effect size estimation","heading":"8.4 Practice on your own!","text":"travel time time spent library correlate getsmarter population? statistical test based survey 200 students. Formulate null alternative hypothesis test. Hint: specify(travel_time ~ time_lib) calculate(stat = \"correlation\"). Consult website library infer (c.f. ).travel time time spent library correlate getsmarter population? statistical test based survey 200 students. Formulate null alternative hypothesis test. Hint: specify(travel_time ~ time_lib) calculate(stat = \"correlation\"). Consult website library infer (c.f. ).travel time time spent library correlate getsmarter population? time, estimate correlation calculate confidence interval. Compare hypothesis test.travel time time spent library correlate getsmarter population? time, estimate correlation calculate confidence interval. Compare hypothesis test.","code":""},{"path":"hypothesis.html","id":"reading-assignment-6","chapter":"8 Hypothesis tests versus effect size estimation","heading":"8.5 Reading assignment","text":"Chapter 9 Ismay Kim (2021)","code":""},{"path":"hypothesis.html","id":"additional-resources","chapter":"8 Hypothesis tests versus effect size estimation","heading":"8.6 Additional resources","text":"Webpage infer: https://infer.netlify.app/Great talk author infer.","code":""},{"path":"hypothesis.html","id":"turning-in-your-work-6","chapter":"8 Hypothesis tests versus effect size estimation","heading":"8.7 Turning in your work","text":"Save R Notebook *.Rmd file.Upload R Notebook ILIAS. don‚Äôt need upload .nb.html file. find upload option today‚Äôs session.receive solution file upload. sure upload deadline!","code":""},{"path":"lin-reg.html","id":"lin-reg","chapter":"9 Linear regression","heading":"9 Linear regression","text":"\nExplain general structure linear model.\n\nName assumption linear model.\n\nCalculate simple linear regression R.\nchapter, start journey statistical modelling. Basically, two main reasons one like build statistical model.Explicative modelling: suppose relationship variables like quantify .Predictive modelling: want build model can (accurately possible) predict future values.course, focus explicative modelling work linear regression models.","code":""},{"path":"lin-reg.html","id":"what-is-a-linear-regression-model","chapter":"9 Linear regression","heading":"9.1 What is a linear regression model?","text":"term regression coined Francis Galton (1822-1911) studied relationship height parents children (Fahrmeir, Kneib, Lang 2009). showed taller--average parents tended children smaller parents (towards average) smaller--average parents tended taller children. called phenomenon regression towards mean.regression model estimates relationship dependent variable, often called \\(y\\) independent variables (predictors) often termed \\(X\\). Note capital letter \\(X\\) indicate can collection independent variables (matrix). try definition:\nregression model form:\n\n\\[y = f(X) + \\varepsilon\\]\n\n\\(y\\): dependent variable\n\n\\(f\\): type relationship\n(function)\n\n\\(X\\): independent variables\n(predictors, explanatory variables)\n\n\\(\\varepsilon\\): error term\n\n:\n\n\\(f\\) linear (effect predictors\nadditive), model called linear regression\nmodel\n\n\\(X\\) single predictor, \nmodel called simple linear regression model,\notherwise multiple linear regression model\n\nComponents model:\n\n\\(f(X)\\): systematic \ndeterministic component\n\n\\(\\varepsilon\\): stochastic\ncomponent (error term, residuals)\nregression model, exact values dependent variable, systematic influence predictors mean value dependent variable. error term (can contain random fluctuations, measurement errors etc.), dependent variable \\(y\\) random variable.","code":""},{"path":"lin-reg.html","id":"simple-linear-regression","chapter":"9 Linear regression","heading":"9.2 Simple linear regression","text":"case one predictor , model simple linear regression model geometric form line. line defined intercept (point cuts \\(y\\) axis) slope. precisely form simple linear regression model:\nGiven data pairs: \\((y_i,x_i), \\quad =1,\\dots,n\\)\n\n\\(y\\) \\(x\\) numeric variables.\n\ncall model \\[y_i=\\beta_0 +\n\\beta_1x_i + \\varepsilon_i, \\qquad =1,\\dots,n.\\]\n\nsimple linear regression model, errors\n\\(\\varepsilon_1,\\dots, \\varepsilon_n\\)\nindependent identically distributed (iid) \n\n\\[\\mathrm{E}(\\varepsilon_i) = 0, \\qquad\n\\mathrm{Var}(\\varepsilon_i)=\\sigma^2.\\]\n\nadditionally \\[\\varepsilon_i \\sim\nN(0,\\sigma^2)\\]\n\n.e.¬†residuals normally distributed, call model \nnormal linear regression model.\n\n\\(\\beta_0\\) intercept \n\\(\\beta_1\\) slope model.\nassumptions normal linear regression model can summarized LINE:Linear relationship variablesIndependence residualsNormal residualsEquality variance (called homoscedasticity) mean zero residualsBefore look model results, confidence intervals slope intercept, ensure assumptions met!","code":""},{"path":"lin-reg.html","id":"example-relationship-between-the-time-in-the-library-and-the-travel-time","chapter":"9 Linear regression","heading":"9.3 Example: Relationship between the time in the library and the travel time","text":"come back lovely getsmarter university want study relationship time student needs come university time spends library.Let‚Äôs generate population.survey 200 students.","code":"\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(moderndive)\nlibrary(knitr)\nset.seed(123)\n\nstudent_id <- 1:12000\n  \ntravel_time <- c(runif(n = 12000 * 0.8, min = 5, max = 40),\n             runif(n = 12000 * 0.2, min = 60, max = 120))\n\ngender <- sample(c('m', 'f'), size = 12000, replace = TRUE)\n\nresidence <- sapply(travel_time, function(x) {\n  if(x < 30) 'urban'\n  else 'rural'\n})\n\ntransport <- sapply(travel_time, function(x) {\n  if(x <= 10) 'foot'\n  else if(x > 10 & x <= 15) sample(c('foot', 'bike'), size = 1)\n  else if(x > 15 & x <= 45) sample(c('bus', 'bike', 'car'), size = 1)\n  else sample(c('bus', 'car'), size = 1)\n})\n\ntime_lib <- 5 * 60 - 0.7 * travel_time + rnorm(length(travel_time), 0, 20)\n\ngetsmarter_pop <- tibble(student_id, gender, residence, transport, travel_time, time_lib)\n\ngetsmarter_pop## # A tibble: 12,000 √ó 6\n##    student_id gender residence transport travel_time time_lib\n##         <int> <chr>  <chr>     <chr>           <dbl>    <dbl>\n##  1          1 f      urban     bus             15.1      294.\n##  2          2 f      rural     bike            32.6      254.\n##  3          3 f      urban     bike            19.3      231.\n##  4          4 m      rural     car             35.9      245.\n##  5          5 m      rural     bus             37.9      234.\n##  6          6 f      urban     foot             6.59     303.\n##  7          7 f      urban     bus             23.5      284.\n##  8          8 m      rural     car             36.2      274.\n##  9          9 m      urban     bike            24.3      299.\n## 10         10 f      urban     bus             21.0      282.\n## # ‚Ä¶ with 11,990 more rows\nset.seed(345)\n\nsurvey_size <- 200\n\nsurvey <- rep_sample_n(getsmarter_pop, size = survey_size, replace = FALSE, reps = 1)"},{"path":"lin-reg.html","id":"fit-a-simple-normal-regression-model","chapter":"9 Linear regression","heading":"9.3.1 Fit a simple normal regression model","text":"first check linear model makes sense relationship two variables (approximately) linear.looks reasonably linear, can proceed modelling.fit linear model R, use function lm(). dependent independent variables joined formula tilde sign ~. can also omit word formula just type time_lib ~ travel_time.‚Äôs üòÑ.Let‚Äôs look estimated intercept slope. function get_regression_table provides tidy form model results, function kable() layouts nicely.estimated intercept equals 302.094 minutes slope -0.766 minutes, respectively. can write model :\\[\\widehat{\\text{time_lib}_i} = 302.094 - 0.766 \\cdot \\text{travel_time}_i + \\varepsilon_i\\]\nmodel estimates time_lib value every student \\(\\), \\(\\) arbitrary student index running 1 200 surveyed 200 students. ‚Äúhat‚Äù \\(\\text{time_lib}_i\\) indicates estimate. estimated time library systematic component model, namely \\(302.094 - 0.766 \\cdot \\text{travel_time}_i\\). difference actual time library estimated value error term residuum \\(\\varepsilon_i\\).simplify analysis, put original data, estimated values (also called fitted values) residuals one tibble. respective values can extracted model object lin_mod functions fitted() residuals().Let‚Äôs look residuals fitted values.","code":"\nggplot(data = survey, aes(x = travel_time, y = time_lib)) +\n  geom_point() +\n  labs(x = 'Travel time (min)', y = 'Time in the library (min)')\nlin_mod <- lm(formula = time_lib ~ travel_time, data = survey)\nget_regression_table(lin_mod) %>% kable()\nmodel_res <- survey %>%\n  mutate(fitted = fitted(lin_mod), residuals = residuals(lin_mod))\n\nmodel_res## # A tibble: 200 √ó 9\n## # Groups:   replicate [1]\n##    replicate student_id gender residence transport travel_time time_lib fitted\n##        <int>      <int> <chr>  <chr>     <chr>           <dbl>    <dbl>  <dbl>\n##  1         1       1623 m      urban     foot             7.06     299.   297.\n##  2         1       9171 m      urban     bike            11.3      278.   293.\n##  3         1      10207 f      rural     bus            107.       199.   220.\n##  4         1       3506 f      urban     bus             25.0      326.   283.\n##  5         1       8892 f      urban     bus             28.1      259.   281.\n##  6         1       5460 m      urban     bus             23.6      299.   284.\n##  7         1       6120 f      urban     bus             20.0      268.   287.\n##  8         1        865 f      urban     bike            26.6      290.   282.\n##  9         1      11586 m      rural     bus            114.       207.   215.\n## 10         1       8153 f      urban     foot             8.06     297.   296.\n## # ‚Ä¶ with 190 more rows, and 1 more variable: residuals <dbl>\nggplot(model_res, aes(x = travel_time, y = time_lib)) +\n  geom_segment(aes(xend = travel_time, yend = fitted, lty = 'Residuals'), alpha = 0.2)  + \n  geom_abline(intercept = coef(lin_mod)[1], slope = coef(lin_mod)[2], color = \"lightblue\") +\n  geom_point(aes(col = 'observed')) +\n  geom_point(aes(y = fitted, col = 'fitted'), shape = 1, size = 2) +\n  labs(x = 'Travel time (min)', y = 'Time in the library (min)') +\n  scale_color_manual(name = '', values = c(observed = 'black', fitted = 'blue'), breaks = c('observed', 'fitted'), label = c('Observed values', 'Fitted values')) +\n  scale_linetype_manual(name = '', values = ('Residuals' = 'solid')) +\n  theme(legend.position = 'bottom')"},{"path":"lin-reg.html","id":"model-assumptions","chapter":"9 Linear regression","heading":"9.3.2 Model assumptions","text":"interpret meaning model parameters intercept slope, check model assumptions. already saw relationship reasonably linear. Let‚Äôs look rest assumptions.Independent residuals. difficult check come study design. , like example, independently surveyed students, can reasonably assume , data residuals independent. Conversely, means use normal linear regression model time -dependent data (time series) spatially dependent data. line fit still correct, confidence intervals won‚Äôt.Normal residuals. check normality residuals, use graphical tool, namely -called qq-plot. compares random data normal distribution residuals. data fall strait line (least approximately) can conclude residuals normally distributed.looks good üòÑ. need practice judge qq-plots. need worry see large deviations line small large values.Equal variance zero mean. last assumption, plot residuals versus fitted values. important residuals remain (approximately) equally large small large fitted values fluctuate around zero.slightly larger residuals larger fitted values. However, also large small fitted values (.e., travel times). distorts picture bit. Overall, serious problem .","code":"\nggplot(model_res, aes(sample = residuals)) + \n  stat_qq() + \n  stat_qq_line() +\n  labs(x = 'Quantiles of the normal distribution',\n       y = 'Qauntiles of the residuals')\nggplot(data = model_res, aes(x = fitted, y = residuals)) + \n  geom_point() +\n  geom_hline(yintercept=0, col = 'red') + \n  labs(x = 'Fitted values', y = 'Residuals')"},{"path":"lin-reg.html","id":"interprete-your-model","chapter":"9 Linear regression","heading":"9.3.3 Interprete your model","text":"can conclude model meets assumptions. Therefore, can interpret estimated parameters confidence intervals now.see time student spend library decreases increasing travel time. precisely, travel time increases one minute, time spent library changes -0.8 [-0.9, -0.7]. confidence interval narrow. means estimation precise.Now, need frame result. decrease less minute per minute travel time sounds rather small. However, student needs one hour come university, spends roughly 45 minutes less library. Basically, time spent travelling spent working library. Stated like , result important students‚Äô time management.","code":"\nget_regression_table(lin_mod) %>% kable()"},{"path":"lin-reg.html","id":"practice-on-your-own-8","chapter":"9 Linear regression","heading":"9.4 Practice on your own!","text":"Predictors need numeric. Work Chapter 5.2 Ismay Kim (2021) see example categorical predictor.","code":""},{"path":"lin-reg.html","id":"reading-assignment-7","chapter":"9 Linear regression","heading":"9.5 Reading assignment","text":"Chapter 5 Ismay Kim (2021)","code":""},{"path":"lin-reg-inference.html","id":"lin-reg-inference","chapter":"10 Inference in linear regression","heading":"10 Inference in linear regression","text":"\nCalculate confidence intervals model parameters\n\nInterpret summary linear regression model\n\nUse bootstrap confidence intervals\nlast chapter, learned fit simple linear model. Remember assumptions model:Linear relationship variablesIndependence residualsNormal residualsEquality variance (called homoscedasticity) mean zero residualsIn chapter, see judge quality model. learn case normality homoscedasticity assumptions violated.","code":""},{"path":"lin-reg-inference.html","id":"how-good-is-the-model","chapter":"10 Inference in linear regression","heading":"10.1 How good is the model?","text":"data model certain variability quantify e.g.¬†variance. judge good model captures relationship dependent variable predictors, quantify much variability dependent variable can explained model. Thus, split variability dependent variable (.e.¬†observed data) :\\[\n\\begin{align*}\n\\mathit{SQT} &= \\mathit{SQE} + \\mathit{SQR}\\\\\n\\sum^{n}_{= 1} (y_i-\\bar{y})^2 &= \\sum^{n}_{=1} (\\hat{y}_i - \\bar{y})^2 + \\sum^{n}_{=1} (y_i - \\hat{y}_i)^2\\\\\n\\end{align*}\n\\]\\(y_i\\): observed data, \\(\\bar{y}\\): mean, \\(\\hat{y}_i\\): fitted values\\(\\mathit{SQT}\\) Sum squares total: variability variance data\\(\\mathit{SQE}\\) Sum squares explained: variability explained model\\(\\mathit{SQR}\\) Sum squares residual: variability explained modelThe \\(\\mathit{SQE}\\) quantifies variability fitted values around mean data \\(\\mathit{SQR}\\) shows much variability model fails capture. smaller residual variability \\(\\mathit{SQR}\\) better model! -called coefficient determination calculates proportion explained variability. larger better model üòÑ:\\[R^2 = \\frac{\\mathit{SQE}}{\\mathit{SQT}} = 1 - \\frac{SQR}{SQT} = 1- \\frac{\\sum^{n}_{=1} (y_i - \\hat{y}_i)^2}{\\sum^{n}_{= 1} (y_i - \\bar{y}_i)^2}\\]Let‚Äôs go back example look coefficient determination.lot information coming summary. details, find:r_squared: Coefficient determination \\(R^2\\)adj_r_squared:\n\\(R^2_\\text{ajd} = 1 - (1 - R^2) \\frac{n-1}{n - p - 1}\\), \\(n\\): number data points, \\(p\\): number predictors (without intercept); robust \\(R^2\\) multiple linear regressionmse: mean standard error mean(residuals(lin_mod)^2)rmse: square root msesimga: standard error error term \\(\\varepsilon\\)statistic: value \\(F\\) statistics hypothesis test, \\(H_0\\): model parameters equal zerop-value: \\(p\\) value hypothesis testdf: degrees freedom, number predictorsnobst: number data pointsThus, conclude hat model explained 53% variance data.","code":"\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(knitr)\nlibrary(moderndive)\nlin_mod <- lm(formula = time_lib ~ travel_time, data = survey)\n\nget_regression_summaries(lin_mod) %>% kable()"},{"path":"lin-reg-inference.html","id":"bootstrap-with-infer-confidence-interval-for-the-slope","chapter":"10 Inference in linear regression","heading":"10.2 Bootstrap with infer: Confidence interval for the slope","text":"case, residuals non-normally distributed /heteroscedastic, confidence intervals model parameters wrong. particular small data sets, violation assumptions problematic. avoid interpreting (possibly) wrong confidence intervals, can use bootstrap construct confidence intervals require normality homoscedasticity residuals. However, still require independent data (always case ordinary bootstrap).simple linear regression, interesting parameter slope. can use usual framework infer determine confidence interval.Step 1: Bootstrap data calculate statistic slopeStep 2: Calculate confidence intervalStep 3: Visualise resultCompared standard confidence interval based normality homoscedasticity assumptions, bootstrap confidence interval similar.model, assumptions valid. case, confidence intervals similar can safely use standard confidence interval. Otherwise prefer bootstrap.","code":"\nbootstrap_distn_slope <- survey %>% \n  specify(formula = time_lib ~ travel_time) %>%\n  generate(reps = 10000, type = \"bootstrap\") %>% \n  calculate(stat = \"slope\")\npercentile_ci <- bootstrap_distn_slope %>% \n  get_confidence_interval(type = \"percentile\", level = 0.95)\n\npercentile_ci## # A tibble: 1 √ó 2\n##   lower_ci upper_ci\n##      <dbl>    <dbl>\n## 1   -0.848   -0.687\nvisualize(bootstrap_distn_slope) +\n    shade_confidence_interval(endpoints = percentile_ci) \nget_regression_table(lin_mod) %>% \nfilter(term == 'travel_time') %>%\n  kable()"},{"path":"lin-reg-inference.html","id":"practice-on-your-own-9","chapter":"10 Inference in linear regression","heading":"10.3 Practice on your own!","text":"analyse relationship number plant species number endemic species Galapagos islands. data set called gala part library faraway.\nLoad data set read help pages understand meaning variables.\nwant know number endemic species depends number plant species islands. Fit linear regression model takes number species predictor number endemic species dependent variable.\nCheck model assumptions.\nUse workflow infer calculate confidence interval slope model.\nCompare confidence interval based normality assumption bootstrap confidence intervals\nLoad data set read help pages understand meaning variables.want know number endemic species depends number plant species islands. Fit linear regression model takes number species predictor number endemic species dependent variable.Check model assumptions.Use workflow infer calculate confidence interval slope model.Compare confidence interval based normality assumption bootstrap confidence intervals","code":""},{"path":"lin-reg-inference.html","id":"reading-assignment-8","chapter":"10 Inference in linear regression","heading":"10.4 Reading assignment","text":"Chapter 10 Ismay Kim (2021)","code":""},{"path":"lin-reg-inference.html","id":"turning-in-your-work-7","chapter":"10 Inference in linear regression","heading":"10.5 Turning in your work","text":"Save R Notebook *.Rmd file.Upload R Notebook ILIAS. don‚Äôt need upload .nb.html file. find upload option today‚Äôs session.receive solution file upload. sure upload deadline!","code":""},{"path":"tsa.html","id":"tsa","chapter":"11 Introduction to time series analysis","heading":"11 Introduction to time series analysis","text":"","code":""},{"path":"tsa.html","id":"what-are-time-series","chapter":"11 Introduction to time series analysis","heading":"11.1 What are time series?","text":"Time series sequences values ordered time. order values crucial. Often, subsequent values similar , .e.¬†correlated. implies many statistical techniques require independent observations work.","code":""},{"path":"tsa.html","id":"an-example-from-the-german-meteorological-service-dwd","chapter":"11 Introduction to time series analysis","heading":"11.2 An example from the German Meteorological Service (DWD)","text":"analyse daily temperature precipitation data. goal see whether () can detect trend data (ii) data fluctuates year. trend long-term change data due obvious cyclic processes like yearly fluctuation. contrast, seasonality shows exactly , (less) regular fluctuation repeats certain time.downloaded data using library\nrdwd (https://bookdown.org/brry/rdwd/) 20 June 2022. data measured daily station ‚ÄúKoeln-Bonn‚Äù starting 1957 end 2021. can find complete data description :Data descriptionWe need following variables:load necessary libraries.rdwddownloads zipped file. First, need unzip read . Note NAs coded ‚Äú-999‚Äù.date measurement proper date format must convert first.plot temperature data.want analyse complete years . years deleted data points?year 1957 incomplete exclude .","code":"\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(Kendall)\nlibrary(ggfortify)\nunzip('data/daily_kl_historical_tageswerte_KL_02667_19570701_20211231_hist.zip', exdir = \"data\")\n\n\nclim <- read_delim('data/produkt_klima_tag_19570701_20211231_02667.txt', delim = ';', na = '-999', trim_ws = TRUE)## Rows: 23560 Columns: 19\n## ‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n## Delimiter: \";\"\n## chr  (1): eor\n## dbl (18): STATIONS_ID, MESS_DATUM, QN_3, FX, FM, QN_4, RSK, RSKF, SDK, SHK_T...\n## \n## ‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n## ‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nclim## # A tibble: 23,560 √ó 19\n##    STATIONS_ID MESS_DATUM  QN_3    FX    FM  QN_4   RSK  RSKF   SDK SHK_TAG\n##          <dbl>      <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n##  1        2667   19570701     5  16     2.9    NA    NA    NA    NA      NA\n##  2        2667   19570702     5   4.7   1.8    NA    NA    NA    NA      NA\n##  3        2667   19570703     5  10.3   3.5    NA    NA    NA    NA      NA\n##  4        2667   19570704     5   9.6   3.4    NA    NA    NA    NA      NA\n##  5        2667   19570705     5  10.4   2.9    NA    NA    NA    NA      NA\n##  6        2667   19570706     5   9.2   2.9    NA    NA    NA    NA      NA\n##  7        2667   19570707     5  10.5   3.5    NA    NA    NA    NA      NA\n##  8        2667   19570708     5  15.9   2.2    NA    NA    NA    NA      NA\n##  9        2667   19570709     5  13.2   3.1    NA    NA    NA    NA      NA\n## 10        2667   19570710     5  18.2   2.6    NA    NA    NA    NA      NA\n## # ‚Ä¶ with 23,550 more rows, and 9 more variables: NM <dbl>, VPM <dbl>, PM <dbl>,\n## #   TMK <dbl>, UPM <dbl>, TXK <dbl>, TNK <dbl>, TGK <dbl>, eor <chr>\nclim <- clim %>% \n  mutate(MESS_DATUM = ymd(MESS_DATUM))\n\nclim## # A tibble: 23,560 √ó 19\n##    STATIONS_ID MESS_DATUM  QN_3    FX    FM  QN_4   RSK  RSKF   SDK SHK_TAG\n##          <dbl> <date>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n##  1        2667 1957-07-01     5  16     2.9    NA    NA    NA    NA      NA\n##  2        2667 1957-07-02     5   4.7   1.8    NA    NA    NA    NA      NA\n##  3        2667 1957-07-03     5  10.3   3.5    NA    NA    NA    NA      NA\n##  4        2667 1957-07-04     5   9.6   3.4    NA    NA    NA    NA      NA\n##  5        2667 1957-07-05     5  10.4   2.9    NA    NA    NA    NA      NA\n##  6        2667 1957-07-06     5   9.2   2.9    NA    NA    NA    NA      NA\n##  7        2667 1957-07-07     5  10.5   3.5    NA    NA    NA    NA      NA\n##  8        2667 1957-07-08     5  15.9   2.2    NA    NA    NA    NA      NA\n##  9        2667 1957-07-09     5  13.2   3.1    NA    NA    NA    NA      NA\n## 10        2667 1957-07-10     5  18.2   2.6    NA    NA    NA    NA      NA\n## # ‚Ä¶ with 23,550 more rows, and 9 more variables: NM <dbl>, VPM <dbl>, PM <dbl>,\n## #   TMK <dbl>, UPM <dbl>, TXK <dbl>, TNK <dbl>, TGK <dbl>, eor <chr>\nggplot(data = clim, aes(x = MESS_DATUM, y = TMK)) +\n  geom_line() +\n  labs(y = 'Daily mean temperature (¬∞C)', \n       x = 'Date')\nclim %>% \n  group_by(year(MESS_DATUM)) %>% \n  tally()## # A tibble: 65 √ó 2\n##    `year(MESS_DATUM)`     n\n##                 <dbl> <int>\n##  1               1957   184\n##  2               1958   365\n##  3               1959   365\n##  4               1960   366\n##  5               1961   365\n##  6               1962   365\n##  7               1963   365\n##  8               1964   366\n##  9               1965   365\n## 10               1966   365\n## # ‚Ä¶ with 55 more rows\nclim <- clim %>% \n  filter(year(MESS_DATUM) != '1957')"},{"path":"tsa.html","id":"trend-analysis-of-yearly-data","chapter":"11 Introduction to time series analysis","heading":"11.3 Trend analysis of yearly data","text":"summarize temperature yearly values exclude seasonal fluctuations. helps analyse trend.Plot yearly mean temperature data.seem upward trend, significant due chance. answer question use Mann-Kendall test trend. null alternative hypothesis test :\\(H_0\\): data trend.\\(H_1\\): data trend.test calculates statistics called \\(\\tau\\). based comparison possible pairs variables succeeding neighbours. \\(\\tau < 0\\), neighbouring points smaller, data negative trend. \\(\\tau > 0\\) neighbouring points larger, data positive trend. \\(\\tau \\approx 0\\), smaller larger neighbouring points balance, trend.\\(p\\) value small. plausible conclude positive trend \\(\\tau > 0\\).much mean temperature increase per year? calculated differences yearly temperature consecutive years.Plot distribution differences.see mean increase 0.01 degrees per year.","code":"\ntemp_yearly <- clim %>% \n  group_by(year(MESS_DATUM)) %>% \n  summarize(mean_temp = mean(TMK, na.rm = T)) %>% \n  rename(year = `year(MESS_DATUM)`)\n\ntemp_yearly## # A tibble: 64 √ó 2\n##     year mean_temp\n##    <dbl>     <dbl>\n##  1  1958      9.76\n##  2  1959     10.6 \n##  3  1960     10.1 \n##  4  1961     10.5 \n##  5  1962      8.56\n##  6  1963      8.48\n##  7  1964      9.86\n##  8  1965      9.26\n##  9  1966     10.2 \n## 10  1967     10.5 \n## # ‚Ä¶ with 54 more rows\nggplot(temp_yearly, aes(x = year, y = mean_temp)) +\n  geom_line() +\n  geom_smooth(formula = 'y ~ x', method = 'loess', se = FALSE) +\n  ylim(5, 15) +\n  labs(y = 'Yearly mean temperature (¬∞C)', \n       x = 'Date')\nMannKendall(temp_yearly$mean_temp)## tau = 0.452, 2-sided pvalue =1.1921e-07\ntemp_diff <- temp_yearly %>% \n  summarize(temp_diff = diff(mean_temp)) %>% \n  mutate(year = temp_yearly$year[-1])\n\nmean_diff <- temp_diff %>% \n  summarise(mean = mean(temp_diff))\nggplot(temp_diff, aes(x = temp_diff)) +\n  geom_histogram(bins = 15, col = 'white')"},{"path":"tsa.html","id":"analysis-of-seasonality","chapter":"11 Introduction to time series analysis","heading":"11.4 Analysis of seasonality","text":"Summarize monthPlot monthly values. seasonality seems rather constant time.overall seasonality.","code":"\ntemp_monthly <- clim %>% \n  group_by(year(MESS_DATUM), month(MESS_DATUM)) %>% \n  summarize(mean_temp = mean(TMK, na.rm = T)) %>% \n  rename(year = `year(MESS_DATUM)`, month = `month(MESS_DATUM)`) %>% \n  mutate(date = ymd(paste(year, month, '15', sep = '-'))) %>% \n  relocate(date)## `summarise()` has grouped output by 'year(MESS_DATUM)'. You can override using\n## the `.groups` argument.\ntemp_monthly## # A tibble: 768 √ó 4\n## # Groups:   year [64]\n##    date        year month mean_temp\n##    <date>     <dbl> <dbl>     <dbl>\n##  1 1958-01-15  1958     1      2.08\n##  2 1958-02-15  1958     2      4.46\n##  3 1958-03-15  1958     3      1.89\n##  4 1958-04-15  1958     4      6.74\n##  5 1958-05-15  1958     5     13.8 \n##  6 1958-06-15  1958     6     15.5 \n##  7 1958-07-15  1958     7     17.7 \n##  8 1958-08-15  1958     8     18.0 \n##  9 1958-09-15  1958     9     16.1 \n## 10 1958-10-15  1958    10     10.4 \n## # ‚Ä¶ with 758 more rows\nggplot(temp_monthly, aes(x = date, y = mean_temp)) +\n  geom_line() +\n  labs(x = 'Date',\n       y = 'Mean daily tempearture (¬∞C)')\nggplot(temp_monthly, aes(x = as_factor(month), y = mean_temp)) +\n  geom_boxplot() +\n  scale_x_discrete(labels = month.abb) +\n  labs(x = 'Month',\n       y = 'Variation of the mean yearly temperature (¬∞C)')"},{"path":"tsa.html","id":"advanced-simultaneous-analysis-of-trend-and-seasonality","chapter":"11 Introduction to time series analysis","heading":"11.5 Advanced: Simultaneous analysis of trend and seasonality","text":"can use model decompose time series trend, seasonality rest (remainder) component. model additive, meaning sum components, get back original time series. model assumes seasonality remains stable time, .e.¬†inappropriate climate change studies. However, still good starting point.need convert data ts (time series) object first. type objects keeps data along time.use function decompose() decompose time series.Plot trend using ggfortify. library helps transform complicated object created decompose() data.frame can easily plotted ggplot().large peak due highest monthly mean July 2007, don‚Äôt -interpret change trend!General advice:want analyse trend, need get rid seasonality. simple way calculate yearly data (already provided) Mann-Kendall test test trend.Calculate mean change years analyse changes details. Report changes \\(p\\) value test.seasonality, can use decompose() decompose data estimate trend seasonality. However, remember model assumes seasonality remains stable time.","code":"\ntime_series <- ts(temp_monthly$mean_temp, start = c(1958, 1), end = c(2021, 12), frequency = 12)\n\nstr(time_series)##  Time-Series [1:768] from 1958 to 2022: 2.08 4.46 1.89 6.74 13.82 ...\ncomponents <- decompose(time_series)\n\nplot(components)\nggplot(data = fortify(components), aes(x = Index, y = trend)) +\n  geom_line() +\n  annotate('rect', xmin = as.Date('2006-01-01'), xmax = as.Date('2007-12-31'), ymin = -Inf, ymax = Inf, fill = 'red', alpha = 1/3) +\n  ylim(5, 15) +\n  labs(x = 'Date',\n       y = 'Trend component of the temperature (¬∞C)',\n       title = 'Red bar highlights the year 2007.')"},{"path":"tsa.html","id":"practice-on-your-own-10","chapter":"11 Introduction to time series analysis","heading":"11.6 Practice on your own!","text":"Select another location repete analysis temperature.Select another location repete analysis temperature.Analyse precipitation data. trend? Instead calculating mean values, must sum precipitations! Analyse yearly sums , decomposition seasonality less important.Analyse precipitation data. trend? Instead calculating mean values, must sum precipitations! Analyse yearly sums , decomposition seasonality less important.","code":""},{"path":"tsa.html","id":"turning-in-your-work-8","chapter":"11 Introduction to time series analysis","heading":"11.7 Turning in your work","text":"Save R Notebook *.Rmd file.Upload R Notebook ILIAS. don‚Äôt need upload .nb.html file. find upload option today‚Äôs session.receive solution file upload. sure upload deadline!","code":""},{"path":"stat-background.html","id":"stat-background","chapter":"A Statistics refresher","heading":"A Statistics refresher","text":"\nRefresh basic statistics\nchapter short refresher basic statistics replace statistics book. use penguins data set calculate examples.\nFigure .1: Artwork @allison_horst\n","code":"\nlibrary(palmerpenguins)\nlibrary(tidyverse)"},{"path":"stat-background.html","id":"descriptive-statistics","chapter":"A Statistics refresher","heading":"A.1 Descriptive statistics","text":"","code":""},{"path":"stat-background.html","id":"mean","chapter":"A Statistics refresher","heading":"A.1.1 Mean","text":"mean one common summary statistics can calculate data. calculate mean sum values data set divide sum number values.\\[\\bar{x} = \\sum_{= 1}^{= n} \\frac{x_i}{n}\\]\n:\\(\\bar{x}\\): mean value data set \\(x\\)\\(\\): index running 1 \\(n\\), number data (sample size)\\(x_i\\): single data points data sets \\(x\\)simple example first, turn penguins. Let‚Äôs calculate mean data set containing values 2, 4.3, 5 10 hand compare values obtained function mean().expected, values identical. Let‚Äôs now calculate mean bill length penguins.function mean() returns NA missing values data set. Remember exclude NAs na.rm = TRUE get meaningful result.","code":"\na <- c(2, 4.3, 5, 10)\n\nhand_mean <- (2 + 4.3 + 5 + 10)/4\nr_mean <- mean(a)\n\nhand_mean## [1] 5.325\nr_mean## [1] 5.325\nmean(x = penguins$bill_length_mm)## [1] NA\nmean(x = penguins$bill_length_mm, na.rm = TRUE)## [1] 43.92193"},{"path":"stat-background.html","id":"variance-and-standard-deviation","chapter":"A Statistics refresher","heading":"A.1.2 Variance and standard deviation","text":"Variance standard deviation measures variability data. show far data deviates mean value average. deviation defined squared difference data point mean data. variance defined :\\[v = \\frac{1}{n-1} \\sum_{= 1}^{= n} (x_i - \\bar{x})^2\\]standard deviation square root variance defined :\\[s = \\sqrt{\\frac{1}{n-1} \\sum_{= 1}^{= n} (x_i - \\bar{x})^2}\\]\n:\\(\\bar{x}\\): mean value data set \\(x\\)\\(\\): index running 1 \\(n\\), number data points (sample size)\\(x_i\\): single data points data sets \\(x\\)large standard deviation data set ? , compare results -hand calculation output function sd()., identical üòÑ. large standard deviation penguins‚Äô bill lengths? Don‚Äôt forget exclude missing values!","code":"\nsd_hand <- sqrt(((2 - r_mean)^2 + (4.3 - r_mean)^2 + (5 - r_mean)^2 + (10 - r_mean)^2)/(4 - 1))\n\nr_sd <- sd(a)\n\nsd_hand## [1] 3.369842\nr_sd## [1] 3.369842\nsd(x = penguins$bill_length_mm, na.rm = TRUE)## [1] 5.459584"},{"path":"stat-background.html","id":"measures-of-association","chapter":"A Statistics refresher","heading":"A.2 Measures of association","text":"","code":""},{"path":"stat-background.html","id":"linear-correlation-coefficent","chapter":"A Statistics refresher","heading":"A.2.1 Linear correlation coefficent","text":"Aka Pearson correlation coefficient Pearson product-moment correlation coefficient measures linear association two numeric variables:\\[ r = \\frac{\\sum_{= 1}^{= n} (x_i - \\bar{x})(y_i - \\bar{y})}{ \\sqrt{\\sum_{= 1}^{= n} (x_i - \\bar{x})^2} \\sqrt{\\sum_{= 1}^{= n} (y_i - \\bar{y})^2}}\\]\n:\\(\\bar{x}\\) \\(\\bar{y}\\): mean values data set \\(x\\) \\(y\\), respectively\\(\\): index running 1 \\(n\\), number data points (sample size)\\(x_i\\) \\(y_i\\): single data points data sets \\(x\\) \\(y\\), respectivelyThe Pearson correlation coefficients varies -1 (perfect negative correlation) 1 (perfect positive correlation). value around zero shows linear correlation. However, different kind assiciation might exist data. Remember always plot data!\nFigure .2: Artwork @allison_horst\nneed distinguish species correlations differ .","code":"\nadelie <- penguins %>% \n  filter(species == 'Adelie')\n\ncor(adelie$bill_length_mm, adelie$bill_depth_mm, method = 'pearson', use = 'pairwise.complete.obs')## [1] 0.3914917\npenguins %>%\n  group_by(species) %>% \n  summarise(cor = cor(bill_length_mm, bill_depth_mm, method = 'pearson', use = 'pairwise.complete.obs'))## # A tibble: 3 √ó 2\n##   species     cor\n##   <fct>     <dbl>\n## 1 Adelie    0.391\n## 2 Chinstrap 0.654\n## 3 Gentoo    0.643\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, col = species)) +\n  geom_point() +\n  geom_smooth(se = FALSE, method = lm)"},{"path":"daten-und-bericht.html","id":"daten-und-bericht","chapter":"B Data sources and final report","heading":"B Data sources and final report","text":"\nKnow different data sources\n\nUse libraries direct access databases\n","code":""},{"path":"daten-und-bericht.html","id":"data-sources","chapter":"B Data sources and final report","heading":"B.1 Data sources","text":"chapter, introduce interesting data sources (databases) can use final report. offer technical assistance package eurostat. can discover packages (possible challenge final report, see üòÑ).","code":""},{"path":"daten-und-bericht.html","id":"federal-statistical-office","chapter":"B Data sources and final report","heading":"B.1.1 Federal Statistical Office","text":"Federal Statistical Office offers data Germany different areas database\nGENESIS. can navigate data sets download . Pay attention select format flat downloading get tidy data set.","code":""},{"path":"daten-und-bericht.html","id":"eurostat","chapter":"B Data sources and final report","heading":"B.1.2 eurostat","text":"eurostat statistical office European Union. find many different data sets Europe . However, strongly recommend using dedicated R library eurostat download data. experience shows students underestimate error-prone manual download large database can ! R library eurostat informative web site many tutorials. show use eurostat data visualize maps (possible report challenge).","code":""},{"path":"daten-und-bericht.html","id":"gapminder","chapter":"B Data sources and final report","heading":"B.1.3 gapminder","text":"already worked excerpt data gapminder. complete data set contains much . can download data . gapminder organises data -called DDF Format (data description format) tidy .csv files. possible report challenge understand format. complete data set availagle via GitHub .","code":""},{"path":"daten-und-bericht.html","id":"national-oceanic-and-atmospheric-administration-noaa","chapter":"B Data sources and final report","heading":"B.1.4 National Oceanic and Atmospheric Administration (NOAA)","text":"NOAA offers data oceans, climate weather. can download data sets via library rnoaa.","code":""},{"path":"daten-und-bericht.html","id":"more-data-sources","chapter":"B Data sources and final report","heading":"B.1.5 More data sources","text":"World Bank Open DataWorld Happiness ReportGlobal Carbon Budget 2020\nPublication data set\nGlobal Carbon Project\nPublication data setGlobal Carbon ProjectSoil organic carbon contents sites perennial cultivation\nPublication\nData set\nPublicationData setPANGAEA: one largest databases environmental datasetsOverview data libraries ROpenScie:\nData\nLibraries","code":""},{"path":"daten-und-bericht.html","id":"research-proposal","chapter":"B Data sources and final report","heading":"B.2 Research proposal","text":"write final report, need submit research proposal ILIAS. sure upload deadline!. receive feedback . avoid misunderstanding final report. Please use template provided ILIAS research proposal.","code":""},{"path":"daten-und-bericht.html","id":"structure-of-the-final-report","chapter":"B Data sources and final report","heading":"B.3 Structure of the final report","text":"","code":""},{"path":"daten-und-bericht.html","id":"sturcture-of-the-working-directory","chapter":"B Data sources and final report","heading":"B.3.1 Sturcture of the working directory","text":"Create new R project final report. can find using projects . project helps structure work properly.Inside project folder, create folder data, figures (help) scripts (appropriate). Save notebooks root directory project.","code":""},{"path":"daten-und-bericht.html","id":"download-and-save-data","chapter":"B Data sources and final report","heading":"B.3.2 Download and save data","text":"want use libraries download data, create notebook task. Don‚Äôt download analyse data --fly. Data can change time delay analysis assessment report. Download save data data folder. Use saved data analysis. ensure report fully reproducible.","code":""},{"path":"daten-und-bericht.html","id":"structure-of-the-report","chapter":"B Data sources and final report","heading":"B.3.3 Structure of the report","text":"Please structure report follows:Introduction (research question end)Material Methods:\nData description:\ndate download, reproducibility crucial), description variables units\nDescription method references, description research area appropriate\nData description:\ndate download, reproducibility crucial), description variables unitsDescription method references, description research area appropriateResults:\nExplorative data analysis (mandatory!)\nanalysis respond research question(s)\nExplorative data analysis (mandatory!)analysis respond research question(s)Discussion: use peer-reviewed literature discuss resultsConclusionsBibliographyYou can combine results discussion one section.report contain challenge. can extensive data tidying wrangling use new interesting library (e.g.¬†plot spatial data eurostat) etc.can write report groups two, even recommend . case, please add names respective parts responsible receive individual grades. Every part graded separately. Every group member need analysis writing. Alternatively, give consent receive grade analysed data wrote report together.don‚Äôt impose particular length report. However, please concise.Knit final report html-document. Think whether need show chunks, avoid redundancy. Compress whole project submit everything! report analysis must run computer observe paths!\nSubmit compressed project ILIAS deadline.\n","code":""},{"path":"daten-und-bericht.html","id":"evaluation-critera","chapter":"B Data sources and final report","heading":"B.4 Evaluation critera","text":"evaluate report along following criteria:notebook run without errors?notebook structured prescribed ?research questions stated clearly?methods described necessary literature cited.data downloaded saved outside main report extra notebook full reproducibility? case data downloaded hand, must explained main report.research area described?Quality data preparation exploratory analysis.Quality main analysis, respond research questions.Quality figures.results discussed additional literature cited? literature help evaluating understanding results.conclusions sound supported data results?receive corrected report feedback.","code":""},{"path":"additional-exercises.html","id":"additional-exercises","chapter":"C Additional exercises","heading":"C Additional exercises","text":"","code":""},{"path":"additional-exercises.html","id":"introduction-to-r-and-rstudio-1","chapter":"C Additional exercises","heading":"C.1 Introduction to R and RStudio","text":"","code":""},{"path":"additional-exercises.html","id":"rob1","chapter":"C Additional exercises","heading":"C.1.1 Rob‚Äôs account book","text":"young master student Rob Stat thinks seriously mother‚Äôs advice monitor expenses. begins writing spends week Mensa:\nTable C.1: Rob‚Äôs account book\nGenerate vector Rob‚Äôs expenses assign variable expenses. Use function c() use numeric expenses , days week.much Rob spend week? Use function sum().Rob seems spent smallest amount Tuesday. much spent paid much every day week? Use array notation square brackets.Unfortunately, Rob misspelled amount Tuesday. Actually, invited girl friend lunch paid 7.95 ‚Ç¨ instead 2.90 ‚Ç¨.Correct Rob‚Äôs typo.result change?","code":""},{"path":"additional-exercises.html","id":"rob2","chapter":"C Additional exercises","heading":"C.1.2 Missing values","text":"R codes missing values NA. Rob ate Mensa last Monday, forgot write amount.\nTable C.2: Rob‚Äôs account book, cont.\nNA change calculated sum?Read happens data contains NAs calling help sum, .e.¬†type ?sum R console.Correct call sum() accordingly.","code":""},{"path":"additional-exercises.html","id":"firstplot","chapter":"C Additional exercises","heading":"C.1.3 Your very first plot","text":"particular beginning learning R forget . R really beautiful want analyse learn real data.Even don‚Äôt fully understand following code, just copy paste .R file let run!data set . Use help like ?gapminder.colours represent?size circles represent?describe relationship GDP per capita Life expectancy?","code":"\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder2007 <- gapminder %>% \n  filter(year == 2007)\n\nggplot(gapminder2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) +\n  geom_point() +\n  scale_x_log10() +\n  xlab('GDP per capita') +\n  ylab('Life expectancy') +\n  labs(title = 'Gapminder data for the year 2007')"},{"path":"additional-exercises.html","id":"the-big-practical-importing-wrangling-summerizing-and-plotting","chapter":"C Additional exercises","heading":"C.2 The big practical: importing, wrangling, summerizing and plotting","text":"","code":""},{"path":"additional-exercises.html","id":"temperature-along-the-dutch-coast","chapter":"C Additional exercises","heading":"C.2.1 Temperature along the Dutch coast","text":"file Temperatur.csv book Zuur, Ieno, Meesters (2009) contains measurements temperature, salinity content chlorophyll 31 locations along Dutch coast. can download data set . data provided Dutch institute RIKZ (monitoring program MWTL: Monitoring Waterstaatkundige Toestand des Lands), measured 1990 2005 0 4 times per month depending season.Read file Temperatur.csv R.Convert column Date proper date format. Use library lubridate.Calculate number measurements, mean standard deviations temperature per monitoring station. Hint: use n() inside summarize() get number measurements.Calculate number measurements, mean standard deviations temperature per month.Plot mean monthly temperature line add standard deviation band around .Label axis appropriately.Save graph pdf file.","code":""},{"path":"additional-exercises.html","id":"temperature-along-the-dutch-coast-revisited","chapter":"C Additional exercises","heading":"C.2.2 Temperature along the Dutch coast, revisited","text":"Calculate monthly means standard deviations per monitoring station. Hint group_by(Station, Month).Plot means error band different plots. Hint: use facet_wrap()).Save graph pdf file.","code":""},{"path":"additional-exercises.html","id":"excel-data-turns-tidy","chapter":"C Additional exercises","heading":"C.2.3 Excel data turns tidy","text":"import tidy World Development Indicators data downloaded World Bank 2021-06-09 20 countries. extract data available.exercise show load excel data directly without converting .csv file. format data typical non-tidy one wrangle tidy tibble. file called Data_Extract_From_World_Development_Indicators.xlsx.goal exercise learn read data excel file using tidyverse functions. use library readxl function read_xlsx() reading files. Read help pages read_xlsx(), find set parameter reading particular table sheet correctly.Open excel sheet look data carefully. NAs coded? data sheet need read?Read excel file R. Call wdi.data set tidy. particular, year coded column name. column names contain year twice, number [YR NUMBER]. rename columns first.code mean? Read help pages functions rename_with(), str_sub() starts_with().Pivot data set tidy format: variables columns measurements rows. Use pivot_longer.code mean? Read help pages functions pivot_longer() .numeric().\nnecessary convert year .numeric()?Filter indicator GDP (current US$) plot data time series. Hint: can also filter indicator‚Äôs code; look excel file. Label axis appropriately.","code":"\nwdi <- wdi %>% \n  rename_with(.fn = function(x) str_sub(x, start = 1, end = 4), .cols = starts_with('20'))\nwdi <- wdi %>%\n  pivot_longer(names_to = 'year', values_to = 'indicator_value', cols = starts_with('20')) %>% \n  mutate(year = as.numeric(year))\n\nwdi"},{"path":"additional-exercises.html","id":"the-big-practical-statistical-inference","chapter":"C Additional exercises","heading":"C.3 The big practical: statistical inference","text":"","code":""},{"path":"additional-exercises.html","id":"species-richness-in-grasslands","chapter":"C Additional exercises","heading":"C.3.1 Species richness in grasslands","text":"work grassland species monitoring data Yellowstone National Park provided Zuur, Ieno, Meesters (2009) Sikkink et al. (2007). can download data set . researchers monitored changes grassland communities time related environmental factors. Biodiversity expressed number different species per site (variable R). Approximately 90 species identified 8 transects monitoring campaigns repeated every 4 10 years, resulting 58 observations. data saved file Vegetation2.xls.Read data explore structure. Describe type variables. type correspond expectation respective variable? Remember set name table sheet want read read_xls().Read data explore structure. Describe type variables. type correspond expectation respective variable? Remember set name table sheet want read read_xls().Short explorative data analysis: calculate number measurements, mean standard deviations species number R per transect.Short explorative data analysis: calculate number measurements, mean standard deviations species number R per transect.Plot species number versus variable BARESOIL (proportion bare soil). Colour dots transect. Hint: convert transect as_factor().Plot species number versus variable BARESOIL (proportion bare soil). Colour dots transect. Hint: convert transect as_factor().Add smoothing line without confidence band (geom_smooth(se = FALSE)) points independently transect. might want consult Section 4.2 book ggplot2 (Wickham 2020). Hint: set colour aes geom_point() instead ggplot().Add smoothing line without confidence band (geom_smooth(se = FALSE)) points independently transect. might want consult Section 4.2 book ggplot2 (Wickham 2020). Hint: set colour aes geom_point() instead ggplot().Add labels graph assign object.Add labels graph assign object.Plot species number time series transect. Add , points lines. size points reflect proportion bare soil. might want consult Section 12.1 book ggplot2 (Wickham 2020). Think set aesthetic size order scale points .Plot species number time series transect. Add , points lines. size points reflect proportion bare soil. might want consult Section 12.1 book ggplot2 (Wickham 2020). Think set aesthetic size order scale points .Add labels graph assign object.Add labels graph assign object.Put graphs side--side save pdf (ggsave()).Put graphs side--side save pdf (ggsave()).Calculate linear Pearson correlation coefficient species number proportion bare soil. Calculate 95% confidence interval. Use framework infer.Calculate linear Pearson correlation coefficient species number proportion bare soil. Calculate 95% confidence interval. Use framework infer.calculate 90% confidence interval instead 95% confidence interval, confidence interval increase decrease? ?calculate 90% confidence interval instead 95% confidence interval, confidence interval increase decrease? ?","code":""},{"path":"additional-exercises.html","id":"soil-compaction","chapter":"C Additional exercises","heading":"C.3.2 Soil compaction","text":"Heavy agricultural machines compact soil. randomized field trial, plots (variable plots) homogeneous agricultural field assigned randomly either control (control) treatment heavy agricultural machine used (compacted). Bulk density [g/cm¬≥] (mass dry soil divided soil volume) measured every plot. parameter soil structure can help spot soil compaction. data stored inbd_compaction.csv.Read data short explorative data analysis.bulk density increase due heavy machinery difference due chance? Use framework infer.Calculate effect size (difference means) 95% confidence interval.","code":""},{"path":"faq.html","id":"faq","chapter":"D Frequently and not-so-frequently asked questions","heading":"D Frequently and not-so-frequently asked questions","text":"","code":"\nlibrary(tidyverse)\nlibrary(palmerpenguins)"},{"path":"faq.html","id":"what-is-the-difference-between-double-and-single-quotes","chapter":"D Frequently and not-so-frequently asked questions","heading":"What is the difference between double \"\" and single '' quotes?","text":"difference. valid quote text/strings. Pay attention use quotes quotes like ‚Äúsolutions exercise ‚ÄòFind mean‚Äô‚Äù. quotes inside quotes must different (either ‚Äú‚Äù outside ‚Äô‚Äô inside way round). However, may rendered differently plain R R Markdown e.g.¬†title.","code":"\nprint('I am a text.')## [1] \"I am a text.\"\nprint(\"So am I.\")## [1] \"So am I.\"\nprint(\"I am a 'quote' in a text.\")## [1] \"I am a 'quote' in a text.\"\nprint('So am \"I\", but not so nice looking.')## [1] \"So am \\\"I\\\", but not so nice looking.\""},{"path":"faq.html","id":"when-do-we-use-the-pipe-operator-and-when-the-plus-sign-to-connect-lines-of-code","chapter":"D Frequently and not-so-frequently asked questions","heading":"When do we use the pipe operator %>% and when the plus sign + to connect lines of code?","text":"pipe operator %>% used compose functions. Instead saving result every function passing result explicitly next function, can omit intermediate saving ‚Äúpipe‚Äù results :details pipes .plus sign + used operator ggplot2 , construct graph.","code":"\npenguins %>% \n  filter(species == 'Adelie') %>% \n  summarise(mean = mean(bill_length_mm, na.rm = TRUE))## # A tibble: 1 √ó 1\n##    mean\n##   <dbl>\n## 1  38.8\n# The same result, but not as tibble!\nfiltered_data <- filter(penguins, species == 'Adelie')\nmean(filtered_data$bill_length_mm, na.rm = TRUE)## [1] 38.79139\nggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, col = species)) +\n  geom_point() +\n  geom_smooth(se = FALSE, method = lm)"},{"path":"literature-1.html","id":"literature-1","chapter":"Literature","heading":"Literature","text":"","code":""}]
