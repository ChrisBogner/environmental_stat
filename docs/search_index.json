[["index.html", "Environmental Statistics Summer term 2025 Preface Intended learning outcomes (ILOs) Literature Why these lecture notes Acknowledgements Reproducibility", " Environmental Statistics Summer term 2025 Christina Bogner, Marijn van der Meij 2025-04-15 Preface The most exciting phrase to hear in science, the one that heralds new discoveries, is not ‘Eureka!’ but ‘That’s funny…’ — Isaac Asimov In this course, we will use the following tools: ILIAS: the moodle platform at the UoC. You should all be registered there already. Campuswire: a chat platform to decrease the number of emails and allow for a more natural exchange between participants and lecturer. You should have received an invitation email, if not, email me, please. Intended learning outcomes (ILOs) At the end of this course you should be able to Import/read data into R. Prepare data for analysis. Visualize data. Explain and apply statistical methods learnt in this course. Combine code and report in a reproducible way. Apply selected methods learnt in this course to a new data set and write a reproducible report. Literature We will be using the book ModernDive: Statistical Inference via Data Science (Ismay and Kim 2021) mainly. Additionally, I will recommend from time to time R for Data Science (Wickham and Grolemund 2021) and OpenIntro Statistics (Diez, Çetinkaya-Rundel, and Barr 2019). For your report, you will do an additional literature search depending on your topic. Why these lecture notes This document is a working and live document that will be updated during the course. It is not comprehensive, but should help you to navigate through the introduction to R and statistics smoothly. I will use different colour boxes Infos and tips Learning outcomes This is important This is a definition This is an exercise inside a chapter. Acknowledgements This document draws on the free material provided by ModernDive: Ismay and Kim (2021) and their free Problem Sets authored by Jenny Smetzer, William Hopper, Albert Y. Kim, and Chester Ismay (https://moderndive.github.io/moderndive_labs/index.html) R for Data Science (r4ds): Wickham and Grolemund (2021) Data Science in a Box (https://datasciencebox.org/) and the free book by Diez, Çetinkaya-Rundel, and Barr (2019) One cannot thank those people enough for their contribution to the community ! Credit: https://xkcd.com/2400/ Reproducibility This book was written in RStudio using Bookdown and compiled in R version 4.4.3 (2025-02-28 ucrt). You will need the following packages to reproduce the examples and to work through the exercises: package version source dabestr NA NA emojifont NA NA fontawesome 0.5.3 CRAN (R 4.4.3) gapminder 1.0.0 CRAN (R 4.4.1) infer 1.0.7 CRAN (R 4.4.0) lubridate 1.9.4 CRAN (R 4.4.3) moderndive 0.7.0 CRAN (R 4.4.3) tidyverse 2.0.0 CRAN (R 4.4.0) The complete information on the last session to build the book: ## R version 4.4.3 (2025-02-28 ucrt) ## Platform: x86_64-w64-mingw32/x64 ## Running under: Windows 10 x64 (build 19045) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=English_United Kingdom.utf8 LC_CTYPE=English_United Kingdom.utf8 ## [3] LC_MONETARY=English_United Kingdom.utf8 LC_NUMERIC=C ## [5] LC_TIME=English_United Kingdom.utf8 ## ## time zone: Europe/Amsterdam ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] DT_0.33 lubridate_1.9.4 forcats_1.0.0 stringr_1.5.1 dplyr_1.1.4 purrr_1.0.4 ## [7] readr_2.1.5 tidyr_1.3.1 tibble_3.2.1 ggplot2_3.5.1 tidyverse_2.0.0 kableExtra_1.4.0 ## [13] fontawesome_0.5.3 ## ## loaded via a namespace (and not attached): ## [1] sass_0.4.9 generics_0.1.3 xml2_1.3.8 stringi_1.8.7 hms_1.1.3 digest_0.6.35 ## [7] magrittr_2.0.3 evaluate_1.0.3 grid_4.4.3 timechange_0.3.0 bookdown_0.42 fastmap_1.2.0 ## [13] jsonlite_2.0.0 sessioninfo_1.2.3 viridisLite_0.4.2 scales_1.3.0 jquerylib_0.1.4 cli_3.6.2 ## [19] rlang_1.1.3 munsell_0.5.1 withr_3.0.2 cachem_1.1.0 yaml_2.3.8 tools_4.4.3 ## [25] tzdb_0.5.0 colorspace_2.1-1 vctrs_0.6.5 R6_2.6.1 lifecycle_1.0.4 htmlwidgets_1.6.4 ## [31] desc_1.4.3 pkgconfig_2.0.3 pillar_1.10.2 bslib_0.9.0 gtable_0.3.6 glue_1.8.0 ## [37] systemfonts_1.2.2 xfun_0.52 tidyselect_1.2.1 tufte_0.13 rstudioapi_0.17.1 knitr_1.50 ## [43] htmltools_0.5.8.1 rmarkdown_2.29 svglite_2.1.3 compiler_4.4.3 This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. References Diez, David M, Mine Çetinkaya-Rundel, and Christopher D Barr. 2019. OpenIntro Statistics. Fourth. Ismay, Chester, and Albert Y. Kim. 2021. ModernDive: Statistical Inference via Data Science. https://moderndive.com/. Wickham, Hadley, and Garrett Grolemund. 2021. R for Data Science. https://r4ds.had.co.nz/. "],["introduction-to-r-and-rstudio.html", "1 Introduction to R and RStudio 1.1 What is ? 1.2 What is RStudio? 1.3 Organize your work 1.4 Practice on your own! 1.5 Turning in your work 1.6 Reading assignment", " 1 Introduction to R and RStudio Create and save files in RStudio Use R as a calculator Create first objects in R Call first functions in R Do you very first plot This chapter introduces you to R and RStudio which you’ll be using throughout the course both to learn statistical concepts and to analyse real data. Often, R and RStudio are confused. However, they are not the same: R is the name of the programming language itself and RStudio is a so-called integrated development interface (IDE), a development environment that will make your life easier. 1.1 What is ? R is a programming language for data analysis and statistical modelling. It is free (open-source software) and belongs, together with Python, to the most popular programming languages for data analysis. R has been introduced by Ross Ihaka and Robert Gentleman in 1996 (Ihaka and Gentleman 1996). It has many additional packages, which extend its functionality. You can and should install R on your computer through the official R webpage. A short installation instruction with a list of packages for this course can be found on ILIAS. Additionally, you can refer to Ismay and Kim (2021), Chapter 1. You will find packages on R’s official webpage under CRAN (The Comprehensive R Archive Network). Not all packages are released through CRAN. However, in the beginning it is a good idea to use CRAN to find and install packages. Packages are sometimes organized in topics, which you can explore via the CRAN Task Views. For environmental statistics, the following topics are relevant: Environmetrics: analysis of environmental data Multivariate: multivariate statistics Spatial: analysis of spatial data TimeSeries: time series analysis. 1.2 What is RStudio? RStudio Desktop is an IDE for R (and some other languages). You can download and install the open-source version for your computer here. The RStudio interface is split in four main areas (Figure 1.1). On the top left, you will type your commands and your text. The focus of this course is on reproducible research, and we will start using R Markdown in the next session. The panel on the bottom left is the console where R executes your commands. This is the proper R system. When you start RStudio, the standard R text will be displayed there to inform you about the R being open source, its version and some other useful things. The panel in the upper right contains your workspace, i.e. the objects that you generated during your working session in R. Additionally, you will find the history of your commands there. The panel in the bottom right will show your plots (in case you are working with simple scripts and not with RMarkdown files). The tab “Files” helps you to browse your files. Figure 1.1: RStudio interface 1.3 Organize your work To better organize your files, create a subfolders for data, scripts and notebooks in your main folder. 1.4 Practice on your own! We will start the exercises C.1.1 and C.1.2 in class. Finish the exercises on your own and produce your first plot in exercise C.1.3 . If you need to type comments or answer with text, don’t forget to use the comment sign # at the beginning of each text line. Otherwise, R will misinterpret your text as commands and will try to execute them. Remember to save your R script regularly! Click the save button in the upper left-hand corner of the window or hit Stg+s. 1.5 Turning in your work Save your R script as *.R file. Upload your *.R file to ILIAS. You will find an upload option in today’s session. You should receive a solution file after the upload. Be sure to upload before the deadline! 1.6 Reading assignment Chapters 1.1 and 1.2 in Ismay and Kim (2021) References Ihaka, Ross, and Robert Gentleman. 1996. “R: A Language for Data Analysis and Graphics.” Journal of Computational and Graphical Statistics 5 (3): 299–314. https://doi.org/10.1080/10618600.1996.10474713. Ismay, Chester, and Albert Y. Kim. 2021. ModernDive: Statistical Inference via Data Science. https://moderndive.com/. "],["using-r-markdown-for-reproducible-research.html", "2 Using R Markdown for reproducible research 2.1 Reproducibility in research 2.2 Combining code and report in one document 2.3 Entering and running commands 2.4 A brief recap of data types 2.5 Practice on your own! 2.6 Turning in your work", " 2 Using R Markdown for reproducible research Opening and saving an R Notebook Basic layout in R Markdown 2.1 Reproducibility in research Usually, analysing data and generating the report are two separate tasks. First, you analyse your data (hopefully in R ), and then you describe your methods, results and conclusions in a text document. However, this procedure is error-prone and not reproducible. You have to copy-paste results from R into tables or include figures in your word processor. The connection between the analysis code, the results and the report is lost. Donald Knuth, the creator of TEX, suggested the idea of literate programming, where analysis code and the report are combined in one document (Knuth 1984). This kind of document is human-centred and allows to better understand the analysis. It helps to generate completely reproducible data analyses. 2.2 Combining code and report in one document We will use R Markdown to combine analysis code and report in one reproducible document. In general, R Markdown can produce different output documents (html, word, pdf, or slides). However, in this course we will concentrate on html output and use so-called R Notebooks (mostly). 2.2.1 Create a new R Notebook To create a new R Notebook, click on the little green plus or click on File on the upper left hand and select R Notebook as in the image below. Save your notebook in the subfolder notebooks. Figure 2.1: How to create a new R Notebook In contrast to a new R script, a new notebook has some template text and example R code in grey boxes called chunks. Have a look at this template text. It provides basic example of layout and R code chunks. 2.2.2 Customize the header Every R Markdown document starts with a header. It is enclosed between two lines of --- signs. Inside the header, you find some (blue) keywords like title: and output:. Let’s customize the header for our needs: Change the title at the top to “Getting to know R Notebooks”. Be sure to keep the quotation marks. Add an author line and put your name there in quotation marks. Additionally, you might want to add the date. The syntax is date: \"some date here\". 2.2.3 Structure your notebook Structure with headers and subheaders helps to orginze content and ideas. To add a header, put # followed by the header title. Be sure to include a space between # and the text! A subheader is produced with ## and a subsubheader with ###. Be sure to include a space before the header text! Delete the template text and structure your notebook. Your final result should look something like this: Figure 2.2: An R Notebook 2.2.4 Preview Notebooks have the great advantage to offer the preview of your work. Just click the Preview button. The preview is refreshed every time you save your notebook. Inspect the preview of your notebook to see how your formatting with headers and subheaders affects the output. There more layout elements, and you will experiment with them in the exercises. 2.2.5 Other output options You can also produce different outputs from your R Notebook because it is a normal R Markdown file and supports different output formats. However, if you produce an .html output, the Preview button will disappear! To bring it back, you need to edit the header of your R Notebook file to output: html_notebook. Note that there is now an R Notebook file (.Rmd) and an html file (nb.html) in the Notebooks folder. 2.3 Entering and running commands In contrast to text, headings etc. R code is typed in special boxes called chunks. To create an empty chunk to type code, click the little green C on top or type Str + Alt + i. On an international keyboard, Str equals Control and on a Mac the Command key. Using your first code chunk, type the following command to create a new variable called x with the value of 42. x &lt;- 42 Remember that the arrow &lt;- is the assignment operator. It generates (or overwrites, if it already exists) the object x and assigns it the value of 42. Note the direction of the arrow! It points from the value to the object name. To Run this command in your console, you can either: click on the green triangle in the code chunk on the right or highlight the code in the chunk and hit Str + Enter (as in an R script). Note that you now have a new object in your workspace, called x! Figure 2.3: Global environment contains the variable x now 2.4 A brief recap of data types You have created a numeric variable x. However, you are not restricted to numbers. R can also handle other types of objects, like characters, for example. To tell R that you want to generate a variable containing characters in contrast to numbers, you need to enclose the assigned content in quotes. Create the following chunk in your notebook and let it run. day_of_week &lt;- &quot;Sunday&quot; To generate a more complicated object, namely a numeric vector, we use the command c() to concatenate several numbers. v &lt;- c(4.5, 6.234, 10) Note in the Environment pane that your vector v contains numbers (listed as num). The information [1:3] shows you that your vector has three elements, indexed from 1 to 3. Indices indicate the place of an element in the vector. To access and change a particular element, we use its index like v[2] &lt;- 4.5 Now the second element of your vector equals 4.5. Remember that R will not warn you when changing your objects! You can calculate with objects as you can with numbers. Let’s divide evely single element of v by 2. v / 2 2.5 Practice on your own! When you work on your exercises, please structure your R Notebook with e.g. headers and subheaders for each exercise. Some of the exercises require code and explanation! Remember to save your work as you go along! Click the save button in the upper left hand corner of the R Markdown window. Answer the following with code in a code chunk (no text necessary). Remember that the code is just instructions for R. You need to run the code chunk to make R execute those instructions! Create a variable called y with the value of 13. Multiply x by y, and store the answer in a variable named z like so: z &lt;- x * y You should now see day_of_week, x, v, y, and z all in your Environment pane. Run the following mathematical operation in a code chunk: 6 + 3. Where does the answer appear? Now add a code chunk, and save the results of 6 + 3 as a variable called a. Does the answer appear? Where does the object a show up? Next type a into the code chunk and re-run the code chunk. What happens? Run following command in a new code chunk. a^2. What does the ^ operator do? Type the following command into a new code chunk. sum(a, x, y) sum is a function. Based on the output, what do you think the sum function does? Click the little broom icon in the upper right hand corner of the Environment pane. Click yes on the window that opens. What happened? Go to the Run button at the top right of the R Markdown pane, and choose Run All (the last option) What happened? Recall the vector v we created earlier. Copy, paste and run the following in a code chunk. What does this code accomplish? v + 2 Copy, paste, and run the following code to make a vector called music, that contains music genres. Recall a vector is a data object that has multiple elements of the same type. Here the data type is a character. Look in the environment pane. How does R tell us that this vector contains characters, not numbers? music &lt;- c(\"bluegrass\", \"funk\", \"folk\") Now let’s practice some basic formatting. Using this formatting tips page figure out how to put the following into your lab report. These all can get typed into the white section, where text goes. Hint: To put each of these on its own line! hit a hard return between each line of text. Italicize like this Bold like this A superscript: R2 2.6 Turning in your work Save your R Notebook as an *.Rmd file. Upload your R Notebook to ILIAS. You don’t need to upload the .nb.html file. You will find an upload option in today’s session. You should receive a solution file after your upload. Be sure to upload before the deadline! References Knuth, D. E. 1984. “Literate Programming.” The Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97. "],["what-is-data.html", "3 What is data? 3.1 Installing R packages 3.2 Welcome the penguins! 3.3 The square braces revisited 3.4 Let’s look at them 3.5 Practice on your own! 3.6 Reading assignment 3.7 Turning in your work 3.8 Additional reading", " 3 What is data? Install an R package Load an installed data set Explore a data set and recognize the type of its variables Data can be anything . Usually, we will store data in a rectangular form, i.e. variables in columns and observations in rows. There are two dedicated object formats to store data, namely data.frame() and tibble(). They have both similar characteristics, however, the tibble is considered the modern form of a data frame and offers some advantages (details later). In this chapter, we will have a look at a data set called palmerpenguins. It is provided in a dedicated package, so let’s install this package first. 3.1 Installing R packages Packages that are available on the official CRAN (Comprehensive R Archive Network) can be installed with function install.packages('name_of_the_package'). It is important to provide the name of the package in quotes (single or double). install.packages(&#39;palmerpenguins&#39;) To load a package, use the function library(name_of_the_package), this time without quotes! library(palmerpenguins) 3.2 Welcome the penguins! Figure 3.1: Artwork by @allison_horst The package has a dedicated website that is really worth visiting. The package contains two data sets, we will explore the shorter one, called penguins. To load a data set installed with a package, use the function data(\"name_of_data_set\"). Be sure to put the name of the data set in quotes (single or double). data(&quot;penguins&quot;) Let’s have a look at the object penguins. penguins ## # A tibble: 344 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex year ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 ## 2 Adelie Torgersen 39.5 17.4 186 3800 female 2007 ## 3 Adelie Torgersen 40.3 18 195 3250 female 2007 ## 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 ## 5 Adelie Torgersen 36.7 19.3 193 3450 female 2007 ## 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 ## 7 Adelie Torgersen 38.9 17.8 181 3625 female 2007 ## 8 Adelie Torgersen 39.2 19.6 195 4675 male 2007 ## 9 Adelie Torgersen 34.1 18.1 193 3475 &lt;NA&gt; 2007 ## 10 Adelie Torgersen 42 20.2 190 4250 &lt;NA&gt; 2007 ## # ℹ 334 more rows This object is a tibble and contains a data set with 344 rows and 8 columns, meaning we have 8 variables measured on 344 animals. The first column contains the variable species that, you guessed it, shows the species of the animal. This variable is a so-called factor (indicated by &lt;fct&gt; below species). It means, it contains categorical information and has a certain number (usually a small one) of distinct values called levels. The levels in this case are levels(penguins$species) ## [1] &quot;Adelie&quot; &quot;Chinstrap&quot; &quot;Gentoo&quot; The above code uses the $ sign to access a whole column (i.e. variable) in the data set. This is very handy and an alternative to the square bracket method. The syntax is name_of_data_set$name_of_variable. There are also numerical variables in the tibble. A numerical variable can be continuous, e.g. bill_length_mm (indicated by &lt;dbl&gt; meaning double), meaning that it contains decimal numbers or discrete, e.g. year (indicated by &lt;int&gt; meaning integer), meaning that it contains integers (whole numbers). To summarize the data set, we can use the function summary(). summary(penguins) ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex ## Adelie :152 Biscoe :168 Min. :32.10 Min. :13.10 Min. :172.0 Min. :2700 female:165 ## Chinstrap: 68 Dream :124 1st Qu.:39.23 1st Qu.:15.60 1st Qu.:190.0 1st Qu.:3550 male :168 ## Gentoo :124 Torgersen: 52 Median :44.45 Median :17.30 Median :197.0 Median :4050 NA&#39;s : 11 ## Mean :43.92 Mean :17.15 Mean :200.9 Mean :4202 ## 3rd Qu.:48.50 3rd Qu.:18.70 3rd Qu.:213.0 3rd Qu.:4750 ## Max. :59.60 Max. :21.50 Max. :231.0 Max. :6300 ## NA&#39;s :2 NA&#39;s :2 NA&#39;s :2 NA&#39;s :2 ## year ## Min. :2007 ## 1st Qu.:2007 ## Median :2008 ## Mean :2008 ## 3rd Qu.:2009 ## Max. :2009 ## 3.3 The square braces revisited You already know how to access a certain position inside a vector. A tibble is a tow-dimensional object, it has rows and columns. To access a particular measurement, you need to provide both, its row and its column index. The following code picks the value in the first row and third column: penguins[1, 3] ## # A tibble: 1 × 1 ## bill_length_mm ## &lt;dbl&gt; ## 1 39.1 3.4 Let’s look at them We will talk much more about data visualization later. For now, just use the code below to visualize the relationship between the flipper length and the body mass of the animals. library(ggplot2) ggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, col = species)) + geom_point() + xlab(&#39;Flipper length (mm)&#39;) + ylab(&#39;Body mass (g)&#39;) 3.5 Practice on your own! How many categorical and how many numerical variables are there? Consult help. How many Gentoo penguins are present in the data set? What is the time span of the measurements? Find the levels of the variable island. This is a challenge . Take the code that produced the visualization of flipper length and the body mass of the animals. Make an educated guess how to change the code such that it produces the visualization of the bill depth vs. body mass. Can you also guess how to adjust the label on the \\(x\\)-axis? 3.6 Reading assignment Chapter 1.3 in Ismay and Kim (2021). 3.7 Turning in your work Save your R Notebook as an *.Rmd file. Upload your R Notebook to ILIAS. You don’t need to upload the .nb.html file. You will find an upload option in today’s session. You should receive a solution file after your upload. Be sure to upload before the deadline! 3.8 Additional reading In case you prefer flights to penguins, you can have a look at data exploration in Chapter 1.4 in Ismay and Kim (2021) References Ismay, Chester, and Albert Y. Kim. 2021. ModernDive: Statistical Inference via Data Science. https://moderndive.com/. "],["import-visualize-and-explore-data.html", "4 Import, visualize and explore data 4.1 Data import from text files 4.2 Visualization with the library ggplot2 4.3 Histogram 4.4 Boxplot 4.5 Practice on your own! 4.6 Reading assignment 4.7 Turning in your work 4.8 Additional reading", " 4 Import, visualize and explore data Import data into R Explain the general call to the function ggplot() Plot 5 frequently used types of graphics 4.1 Data import from text files To import a data set from a text file (e.g. csv, .txt, .dat) into R, we will use the library readr which is part of the tydiverse. We first load the library. library(tidyverse) Let’s assume that the data is stored in the folder data. If this is not your case, change the path accordingly. To load the data, we can choose among several functions that all start with read_. The most generic one is read_delim() where we can specify how the columns are separated (delimited) in the data file. emissions &lt;- read_delim(file = &#39;data/emissions.csv&#39;, delim = &#39;;&#39;) ## Rows: 2871 Columns: 6 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (4): unit, airpol, vehicle, geo ## dbl (1): values ## date (1): time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Let’s have a look at the data. This is a data set on greenhouse gas emissions by source sector in the EU that I downloaded from eurostat on 2021-04-30. It contains the greenhouse gas emissions in CO2 equivalent, in Mio tonnes, per vehicle type. The database could be a great source for data in your reports . emissions ## # A tibble: 2,871 × 6 ## unit airpol vehicle geo time values ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 Million tonnes Greenhouse gases (CO2, N2O in CO2 equivalent, CH4 in CO2 equiv… Fuel c… Aust… 2018-01-01 14.4 ## 2 Million tonnes Greenhouse gases (CO2, N2O in CO2 equivalent, CH4 in CO2 equiv… Fuel c… Belg… 2018-01-01 14.4 ## 3 Million tonnes Greenhouse gases (CO2, N2O in CO2 equivalent, CH4 in CO2 equiv… Fuel c… Bulg… 2018-01-01 5.78 ## 4 Million tonnes Greenhouse gases (CO2, N2O in CO2 equivalent, CH4 in CO2 equiv… Fuel c… Swit… 2018-01-01 11.0 ## 5 Million tonnes Greenhouse gases (CO2, N2O in CO2 equivalent, CH4 in CO2 equiv… Fuel c… Cypr… 2018-01-01 1.38 ## 6 Million tonnes Greenhouse gases (CO2, N2O in CO2 equivalent, CH4 in CO2 equiv… Fuel c… Czec… 2018-01-01 11.9 ## 7 Million tonnes Greenhouse gases (CO2, N2O in CO2 equivalent, CH4 in CO2 equiv… Fuel c… Germ… 2018-01-01 97.8 ## 8 Million tonnes Greenhouse gases (CO2, N2O in CO2 equivalent, CH4 in CO2 equiv… Fuel c… Denm… 2018-01-01 6.85 ## 9 Million tonnes Greenhouse gases (CO2, N2O in CO2 equivalent, CH4 in CO2 equiv… Fuel c… Esto… 2018-01-01 1.52 ## 10 Million tonnes Greenhouse gases (CO2, N2O in CO2 equivalent, CH4 in CO2 equiv… Fuel c… Gree… 2018-01-01 7.61 ## # ℹ 2,861 more rows The result of reading the data with any function in the library readr is always a tibble. You can see that none of the variables is a factor. This is the default behaviour of readr. If you want a variable to be coded as a factor then you have to transform it manually, preferably with functions from the package forcats. Let’s have a brief look at the data set. summary(emissions) ## unit airpol vehicle geo time ## Length:2871 Length:2871 Length:2871 Length:2871 Min. :1990-01-01 ## Class :character Class :character Class :character Class :character 1st Qu.:1997-01-01 ## Mode :character Mode :character Mode :character Mode :character Median :2004-01-01 ## Mean :2004-01-01 ## 3rd Qu.:2011-01-01 ## Max. :2018-01-01 ## ## values ## Min. : 0.00609 ## 1st Qu.: 0.25564 ## Median : 1.92403 ## Mean : 8.52836 ## 3rd Qu.: 6.93899 ## Max. :119.77824 ## NA&#39;s :232 For character variables, summary() does not count the frequency of different values. However, we can get this information with the function unique(). length(unique(emissions$geo)) ## [1] 33 The data set contains measurements for 33 EU countries. We can also ask which different types of vehicle are recorded. unique(emissions$vehicle) ## [1] &quot;Fuel combustion in cars&quot; &quot;Fuel combustion in heavy duty trucks and buses&quot; ## [3] &quot;Fuel combustion in railways&quot; 4.2 Visualization with the library ggplot2 The library ggplot2 is a powerful package for data visualization. The name comes from the grammar of graphics and hints at a systematic approach to visualization. In a nutshell, ggplot2 defines a statistical graphic as follows: A statistical graphic is a mapping of variables in a data set to aesthetic attributes of geometric objects. In ggplot2, a graphic is built up step by step, starting with a call to the core function ggplot(). We have to specify the following elements: data: the data set containing the variables to be visualized. aes: (aesthetic) attributes for the geometric object to be visualized. This can be the x and y variables, colour, shape, grouping variable etc. geom: the geometric object we want to plot, i.e. lines, points, bars, boxes etc. 4.2.1 Line plot We start with the line plot that is particularly suited for time series. Because plotting 33 countries in one graph is too much, we first filter for France and emissions from cars. emissions_france &lt;- emissions %&gt;% filter(geo == &#39;France&#39; &amp; vehicle == &#39;Fuel combustion in cars&#39;) ggplot(data = emissions_france, mapping = aes(x = time, y = values)) This call to ggplot() prepares the plotting area as requested, but does not show anything because we did not specify any geometric object. All geometric objects begin with geom_. Every further step in building up the plot is appended to the core call by a +. ggplot(data = emissions_france, mapping = aes(x = time, y = values)) + geom_line() This call can be verbalized like following: Take the data set emissions and map the following attributes: on the \\(x\\)-axis, the variable time on the \\(y\\)-axis, the variable values Plot the data as a line (geom_line()) In order for the plot to be useful, we should label the axes correctly (and give it a title, if no figure caption is shown). This is done by adding the function labs(). ggplot(data = emissions_france, mapping = aes(x = time, y = values)) + geom_line() + labs(x = &#39;Time&#39;, y = &#39;Emissions (Mio tons)&#39;, title = &#39;Emissions in France&#39;) 4.2.2 Point plot We can add points to this plot with geom_point(). Normally, I wouldn’t do it for this time series, but I want to show this geom . ggplot(data = emissions_france, mapping = aes(x = time, y = values)) + geom_line() + geom_point() + labs(x = &#39;Time&#39;, y = &#39;Emissions (Mio tons)&#39;, title = &#39;Emissions in France&#39;) If we select two countries, then a further aesthetic will be required to distinguish the time series. Let’s select France and Italy. emissions_france_italy &lt;- emissions %&gt;% filter(geo %in% c(&#39;France&#39;, &#39;Italy&#39;) &amp; vehicle == &#39;Fuel combustion in cars&#39;) We plot the countries using different colours. Note that we do not (yet) select the colours manually, but specify the variable that will be used to distinguish the time series. The colours are chosen one by country automatically. ggplot(data = emissions_france_italy, mapping = aes(x = time, y = values, colour = geo)) + geom_line() + geom_point() + labs(x = &#39;Time&#39;, y = &#39;Emissions (Mio tons)&#39;, title = &#39;Emissions in France and Italy&#39;, colour = &#39;Country&#39;) The legend comes for free! We can change the title of the legend by setting colour = 'Country' in the call to labs(). 4.3 Histogram Let’s have a look at the distribution of emissions in the year 2018. We have to filter the data first. emissions_2018 &lt;- emissions %&gt;% filter(time == &#39;2018-01-01&#39;) We plot the data in a histogram that shows the absolute frequencies of the data (i.e. how many data points fall in a particular interval of emissions). It shows the distribution of a continuous variable. For a histogram, we only specify the x variable, the frequencies are calculated by geom_histogram() directly. We specify 25 bins (intervals). If you are not familiar with this kind of statistical summaries, please have a look at the Appendix A in Ismay and Kim (2021) and read the part A.1.5 Distribution. ggplot(data = emissions_2018, mapping = aes(x = values)) + geom_histogram(bins = 25) ## Warning: Removed 8 rows containing non-finite outside the scale range (`stat_bin()`). 4.4 Boxplot A boxplot calculates some prominent statistics of a data set and plots them in the form of a box with ‘whiskers’ (thus also called box-and-whiskers plot). Basically, it is the same as calculating the summary() (five-numbers: min, max, 25%, 50% and 75% quantiles), but as a figure. If you are not familiar with this kind of statistical summaries, please have a look at the Appendix A in Ismay and Kim (2021) and read the part A.1.4 Five-number summary. Let’s have a look at this kind of summary plot. How are the emissions distributed by year? We have to convert time to a factor variable to display the data correctly (try out what happens if you don’t convert it). ggplot(data = emissions, mapping = aes(x = factor(time), y = values)) + geom_boxplot() ## Warning: Removed 232 rows containing non-finite outside the scale range (`stat_boxplot()`). Hmmm, the labels on the \\(x\\)-axis are ugly. Let’s tune them a little (we will do more tuning in later sessions). ggplot(data = emissions, mapping = aes(x = factor(time), y = values)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 90)) ## Warning: Removed 232 rows containing non-finite outside the scale range (`stat_boxplot()`). 4.4.1 Barplot The last geom we want to see is geom_bar(). We would like to know how many data entries does emissions contain per vehicle. ggplot(data = emissions, mapping = aes(x = vehicle)) + geom_bar() Admittedly, a boring plot , the number of entries is identical. 4.5 Practice on your own! The histogram, the boxplot and the barplot plotted above are not labelled correctly. Correct the axis labels and find good titles for the graphs. Plot the time series of GDP from the data set gapminder for France and Germany. Filter the data like this: france_germany &lt;- gapminder %&gt;% filter(country %in% c('France', 'Germany')) Plot the life expectancy vs. GDP in 2007, use the data set gapminder. Pick the code for filtering the data in task C.1.3. Use the aesthetics colour and size. Have an educated guess on how to change the title of the legends (or google for it ). How is the GDP distributed in Africa and Europe in 2007? Use the data set gapminder. Filter the data like this: africa_europe &lt;- gapminder2007 %&gt;% filter(continent %in% c('Africa', 'Europe')). Plot the data as a histogram and use the aesthetic fill instead of colour to distinguish between continents. How is the GDP distributed on different continents in 2007? Use the data set gapminder. Plot the data as a boxplot. How many data points does gapminder contain per continent? Visualize in a barplot. 4.6 Reading assignment Chapter 2.1 in Ismay and Kim (2021) 4.7 Turning in your work Save your R Notebook as an *.Rmd file. Upload your R Notebook to ILIAS. You don’t need to upload the .nb.html file. You will find an upload option in today’s session. You should receive a solution file after your upload. Be sure to upload before the deadline! 4.8 Additional reading Chapters 2.2 to 2.9 in Ismay and Kim (2021) References Ismay, Chester, and Albert Y. Kim. 2021. ModernDive: Statistical Inference via Data Science. https://moderndive.com/. "],["tidyverse.html", "5 Explorative workflow with tidyverse 5.1 Core packages 5.2 Exploratory data analysis 5.3 Practice on your own! 5.4 Reading assignment 5.5 Additional reading and videos", " 5 Explorative workflow with tidyverse Name core packages in tidyverse Apply a simple explorative workflow (read, summarize, plot) with tidyverse Use functions from dplyr for data wrangling tidyverse is a collection of R packages for data analysis (https://www.tidyverse.org/). It shares a common philosophy about how data should be structure and grammar of data manipulation and visualization. Although it might sound like something alien, tidyverse is a regular part of R and its functions can be mixed with base R functions. The best introduction to tidyverse is r4ds: “R for Data Science” (Wickham and Grolemund 2021). You can read it for free here (https://r4ds.had.co.nz/). 5.1 Core packages tidyverse comprises 8 core packages that are installed when you call install.packages('tidyverse'): Packages Description ggplot2 data visualization dplyr data transformation tidyr data cleaning readr importing data purrr functional programming tibble extension of data.frame stringr functions for strings, i.e. text variables forcats functions for factor All packages have a Cheat Sheet, an overview of its functions. To get a package’s cheat sheet, click on its name (https://www.tidyverse.org/packages/), scroll down to the section Cheatsheet. Besides its core packages, tidyverse also installs a long list of supplementary packages that you can find here: https://www.tidyverse.org/packages/ 5.2 Exploratory data analysis Exploratory data analysis is an essential first step in data analysis. Before using any advanced statistical method, exploratory analysis is a must-have. It comprises roughly the following steps: import and inspect data clean (tidy) data if necessary summarize it and create new variables if necessary plot as many plots as possible to get a good overview about patterns and data distribution 5.2.1 Read data, revisited We load the library tidyverse first. library(tidyverse) Last time we used the function read_delim() to import data into R. This function is the most general from a whole family of functions, all starting with read_*: read_csv(), read_csv2() etc. They all have their own parameters that you need to verify on the respective help pages if you want to use them. For this exploratory data analysis, we will use data from the German Meteorological Service (Deutscher Wetterdienst) that I downloaded on 2020-05-24 (https://www.dwd.de/DE/leistungen/klimadatendeutschland/klimadatendeutschland.html). The data set contains hourly measurements of the relative air humidity (%), and air temperature (°C) for three weather stations, namely Hof, Frankfurt and Köln-Bonn. The data is named meteo.csv. temp_humid &lt;- read_delim(&#39;data/meteo.csv&#39;, delim = &#39;;&#39;, trim_ws = T) ## Rows: 39600 Columns: 6 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (1): eor ## dbl (5): STATIONS_ID, MESS_DATUM, QN_9, TT_TU, RF_TU ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. read_delim() reports on reading the data about the variables it recognizes. This is a good hint to spot for possible problems. Are numerical variables read as &lt;dbl&gt;? Are characters recognized as &lt;char&gt; etc. In the code above, the parameter trim_ws = T removes leading zeroes. Let’s have a short glimpse of the data. temp_humid ## # A tibble: 39,600 × 6 ## STATIONS_ID MESS_DATUM QN_9 TT_TU RF_TU eor ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2261 2018111900 3 -2.8 99 eor ## 2 2261 2018111901 3 -2.5 100 eor ## 3 2261 2018111902 3 -2.3 100 eor ## 4 2261 2018111903 3 -2 100 eor ## 5 2261 2018111904 3 -1.9 99 eor ## 6 2261 2018111905 3 -2.1 99 eor ## 7 2261 2018111906 3 -1.8 99 eor ## 8 2261 2018111907 3 -1.5 99 eor ## 9 2261 2018111908 3 -1.1 99 eor ## 10 2261 2018111909 3 -0.6 97 eor ## # ℹ 39,590 more rows The data set contains the following variables: Variable Description STATIONS_ID ID of the weather station MESS_DATUM date and time of the measurement, formatted as yyyymmddhh QN_9 quality flag TT_TU air temperature in 2 m height in °C RF_TU relative air humidity in % eor end of record (i.e. end of line) read_* always returns a tibble. class(temp_humid) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 5.2.2 Date and time made easy A very useful package to handle date and time is called lubridate. It is not part of core packages in tidyverse but is installed with the long list of additional packages. We will use it to convert the variable MESS_DATUM to a real date-time variable. The function ymd_h() converts character vectors to date-time objects provided they have the format year, month, day, hour. There are other function for different other formats; consult help. library(lubridate) temp_humid$MESS_DATUM &lt;- ymd_h(temp_humid$MESS_DATUM) temp_humid ## # A tibble: 39,600 × 6 ## STATIONS_ID MESS_DATUM QN_9 TT_TU RF_TU eor ## &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2261 2018-11-19 00:00:00 3 -2.8 99 eor ## 2 2261 2018-11-19 01:00:00 3 -2.5 100 eor ## 3 2261 2018-11-19 02:00:00 3 -2.3 100 eor ## 4 2261 2018-11-19 03:00:00 3 -2 100 eor ## 5 2261 2018-11-19 04:00:00 3 -1.9 99 eor ## 6 2261 2018-11-19 05:00:00 3 -2.1 99 eor ## 7 2261 2018-11-19 06:00:00 3 -1.8 99 eor ## 8 2261 2018-11-19 07:00:00 3 -1.5 99 eor ## 9 2261 2018-11-19 08:00:00 3 -1.1 99 eor ## 10 2261 2018-11-19 09:00:00 3 -0.6 97 eor ## # ℹ 39,590 more rows After conversion, the variables is recognized as &lt;dttm&gt; for date-time. 5.2.3 Summarize The three weather station have the following IDs: station_ids &lt;- c(&#39;2261&#39; = &#39;Hof&#39;, &#39;1420&#39; = &#39;Frankfurt&#39;, &#39;2667&#39; = &#39;Koeln&#39;) We want to know how many measurements per station the data set contains. temp_humid %&gt;% count(STATIONS_ID) ## # A tibble: 3 × 2 ## STATIONS_ID n ## &lt;dbl&gt; &lt;int&gt; ## 1 1420 13200 ## 2 2261 13200 ## 3 2667 13200 The operator %&gt;% is called pipe and is pronounced as and then. The code temp_humid %&gt;% count(STATIONS_ID) can be read as: take the object temp_humid, group it by STATIONS_ID and count the measurements in each group. The pipe operator comes from the package magrittr (https://magrittr.tidyverse.org/). It is a core operator in tidyverse and makes the code more readable and easier to follow for humans. Perhaps not in the beginning, but very soon . 5.2.4 The grammar of data manipulation – dplyr The function count() is part of the library dplyr, a collection of functions all named after verbs. Thus, it is easy to imagine what the function does ). The 5 core functions are: Function Meaning filter() filter data according to their values arrange() arrange rows select() select variables according to their names mutate() create new variables, possibly using other variables summarize() summarize data with different functions If we want to know how many measurements were recorded for a particular weather station, we first filter for its ID: temp_humid %&gt;% filter(STATIONS_ID == &#39;2667&#39;) %&gt;% count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 13200 The function filter() accepts logical tests. For every row in STATION_ID, == checks whether the entry equals 2667. == is a logical operator and means is the left side equals the right sight. If this is the case, then == returns TRUE otherwise it returns FALSE. filter() selects only those rows where TRUE was returned. Other useful logical operators are: Operator Meaning &gt; is the left side larger than the right side? &gt;= is the left side larger or equal the right side? != are left and right sides unequal? More logical and boolean operators are handled in the tutorials (see below) and on help pages of filter(). We can combine several tests with the operator or |, for example. Here, we want to filter all rows containing either ID 2667 or ID 2261: temp_humid %&gt;% filter(STATIONS_ID == &#39;2667&#39; | STATIONS_ID == &#39;2261&#39;) %&gt;% count(STATIONS_ID) ## # A tibble: 2 × 2 ## STATIONS_ID n ## &lt;dbl&gt; &lt;int&gt; ## 1 2261 13200 ## 2 2667 13200 The same can be achieved by excluding the third station: temp_humid %&gt;% filter(STATIONS_ID != &#39;1420&#39;) %&gt;% count(STATIONS_ID) ## # A tibble: 2 × 2 ## STATIONS_ID n ## &lt;dbl&gt; &lt;int&gt; ## 1 2261 13200 ## 2 2667 13200 As an alternative, we can use the operator %in% which checks whether the row contains one of the entries in a vector. temp_humid %&gt;% filter(STATIONS_ID %in% c(&#39;2667&#39;, &#39;2261&#39;)) %&gt;% count(STATIONS_ID) ## # A tibble: 2 × 2 ## STATIONS_ID n ## &lt;dbl&gt; &lt;int&gt; ## 1 2261 13200 ## 2 2667 13200 5.2.5 Visualize We plot the time series and use a trick to split them along three different plots with the function facet_wrap(). It needs a variable to separate the data into plots, and we chose STATIONS_ID. The splitting variable must be preceded by ~. ggplot(data = temp_humid, aes(x = MESS_DATUM, y = TT_TU)) + geom_line() + facet_wrap(~STATIONS_ID, nrow = 3) + labs(x = &#39;Time&#39;, y = &#39;Temperature (°C)&#39;) 5.2.6 Create new variables with mutate() We want to calculate the monthly means and standard deviations of the air temperature and humidity. First, we need to generate the temporal information, namely year and month, that will be used to group the temperature values to calculate mean() and sd(). This can be achieved with the functions year()and month() from library lubridate. The function mutate() can create new variables in a data object. temp_humid &lt;- temp_humid %&gt;% mutate(year = year(MESS_DATUM), month = month(MESS_DATUM)) temp_humid ## # A tibble: 39,600 × 8 ## STATIONS_ID MESS_DATUM QN_9 TT_TU RF_TU eor year month ## &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2261 2018-11-19 00:00:00 3 -2.8 99 eor 2018 11 ## 2 2261 2018-11-19 01:00:00 3 -2.5 100 eor 2018 11 ## 3 2261 2018-11-19 02:00:00 3 -2.3 100 eor 2018 11 ## 4 2261 2018-11-19 03:00:00 3 -2 100 eor 2018 11 ## 5 2261 2018-11-19 04:00:00 3 -1.9 99 eor 2018 11 ## 6 2261 2018-11-19 05:00:00 3 -2.1 99 eor 2018 11 ## 7 2261 2018-11-19 06:00:00 3 -1.8 99 eor 2018 11 ## 8 2261 2018-11-19 07:00:00 3 -1.5 99 eor 2018 11 ## 9 2261 2018-11-19 08:00:00 3 -1.1 99 eor 2018 11 ## 10 2261 2018-11-19 09:00:00 3 -0.6 97 eor 2018 11 ## # ℹ 39,590 more rows In the next step, we create a new data set and calculate the means and standard deviations. To get them by station, year and month, we group the data accordingly. To group by several variables, just enumerate them with a comma (no quotation or c() necessary). monthly_means &lt;- temp_humid %&gt;% group_by(STATIONS_ID, year, month) %&gt;% summarize(mean_T = mean(TT_TU), mean_RH = mean(RF_TU), sd_T = sd(TT_TU), sd_RH = sd(RF_TU)) ## `summarise()` has grouped output by &#39;STATIONS_ID&#39;, &#39;year&#39;. You can override using the `.groups` argument. monthly_means ## # A tibble: 57 × 7 ## # Groups: STATIONS_ID, year [9] ## STATIONS_ID year month mean_T mean_RH sd_T sd_RH ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1420 2018 11 4.00 79.7 1.82 9.96 ## 2 1420 2018 12 4.73 83.7 4.20 11.7 ## 3 1420 2019 1 2.12 79.3 3.76 10.0 ## 4 1420 2019 2 4.48 74.1 4.69 17.7 ## 5 1420 2019 3 8.28 68.5 4.08 16.1 ## 6 1420 2019 4 11.7 61.0 5.52 21.8 ## 7 1420 2019 5 12.7 67.5 4.64 20.1 ## 8 1420 2019 6 21.4 60.6 6.05 21.2 ## 9 1420 2019 7 21.6 55.6 5.90 21.8 ## 10 1420 2019 8 20.7 65.6 4.94 20.8 ## # ℹ 47 more rows The new object monthly_means is a grouped tibble, indicated by grouped_df in the output of str() that shows the structure of an object. str(monthly_means) ## gropd_df [57 × 7] (S3: grouped_df/tbl_df/tbl/data.frame) ## $ STATIONS_ID: num [1:57] 1420 1420 1420 1420 1420 1420 1420 1420 1420 1420 ... ## $ year : num [1:57] 2018 2018 2019 2019 2019 ... ## $ month : num [1:57] 11 12 1 2 3 4 5 6 7 8 ... ## $ mean_T : num [1:57] 4 4.73 2.12 4.48 8.28 ... ## $ mean_RH : num [1:57] 79.7 83.7 79.3 74.1 68.5 ... ## $ sd_T : num [1:57] 1.82 4.2 3.76 4.69 4.08 ... ## $ sd_RH : num [1:57] 9.96 11.68 10.04 17.73 16.1 ... ## - attr(*, &quot;groups&quot;)= tibble [9 × 3] (S3: tbl_df/tbl/data.frame) ## ..$ STATIONS_ID: num [1:9] 1420 1420 1420 2261 2261 ... ## ..$ year : num [1:9] 2018 2019 2020 2018 2019 ... ## ..$ .rows : list&lt;int&gt; [1:9] ## .. ..$ : int [1:2] 1 2 ## .. ..$ : int [1:12] 3 4 5 6 7 8 9 10 11 12 ... ## .. ..$ : int [1:5] 15 16 17 18 19 ## .. ..$ : int [1:2] 20 21 ## .. ..$ : int [1:12] 22 23 24 25 26 27 28 29 30 31 ... ## .. ..$ : int [1:5] 34 35 36 37 38 ## .. ..$ : int [1:2] 39 40 ## .. ..$ : int [1:12] 41 42 43 44 45 46 47 48 49 50 ... ## .. ..$ : int [1:5] 53 54 55 56 57 ## .. ..@ ptype: int(0) ## ..- attr(*, &quot;.drop&quot;)= logi TRUE Some calculations are better done on ungrouped data. Therefore, we remove the grouping. This does not change the data itself. monthly_means &lt;- ungroup(monthly_means) To plot the monthly data, we need a proper monthly date object. We will attribute the monthly means to the first of the respective month. Again, lubridate helps with this task. The function parse_dat_time() is a general function taking a character string and returning a date-time object. We need to “glue” the variables year and month together with paste0() (yes, it is a zero, not an O!) to form such a string and specify that orders = 'ym', i.e. year before month. Finally, we relocate() the new variable year_month before the variable year for convenience (if not, it will be created as the last variable in the data set). monthly_means &lt;- monthly_means %&gt;% mutate(year_month = parse_date_time(paste0(year, month), orders = &#39;ym&#39;, tz = &#39;CET&#39;)) %&gt;% relocate(year_month, .before = year) monthly_means ## # A tibble: 57 × 8 ## STATIONS_ID year_month year month mean_T mean_RH sd_T sd_RH ## &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1420 2018-11-01 00:00:00 2018 11 4.00 79.7 1.82 9.96 ## 2 1420 2018-12-01 00:00:00 2018 12 4.73 83.7 4.20 11.7 ## 3 1420 2019-01-01 00:00:00 2019 1 2.12 79.3 3.76 10.0 ## 4 1420 2019-02-01 00:00:00 2019 2 4.48 74.1 4.69 17.7 ## 5 1420 2019-03-01 00:00:00 2019 3 8.28 68.5 4.08 16.1 ## 6 1420 2019-04-01 00:00:00 2019 4 11.7 61.0 5.52 21.8 ## 7 1420 2019-05-01 00:00:00 2019 5 12.7 67.5 4.64 20.1 ## 8 1420 2019-06-01 00:00:00 2019 6 21.4 60.6 6.05 21.2 ## 9 1420 2019-07-01 00:00:00 2019 7 21.6 55.6 5.90 21.8 ## 10 1420 2019-08-01 00:00:00 2019 8 20.7 65.6 4.94 20.8 ## # ℹ 47 more rows Now, we can plot the mean air temperature. ggplot(data = monthly_means, aes(x = year_month, y = mean_T, col = factor(STATIONS_ID))) + geom_line() + labs(x = &#39;Time&#39;, y = &#39;Temperature (°C)&#39;, color = &#39;Meteo station&#39;) We can also visualize the standard deviations. ggplot(monthly_means, aes(x = year_month, y = mean_T, ymin = mean_T - sd_T, ymax = mean_T + sd_T)) + geom_errorbar() + geom_point() + facet_wrap(~STATIONS_ID, nrow = 3) + labs(x = &#39;Time&#39;, y = &#39;Temperature (°C)&#39;) Or use a semi-transparent band to show the variability (standard deviation). ggplot(monthly_means, aes(x = year_month, y = mean_T, ymin = mean_T - sd_T, ymax = mean_T + sd_T)) + geom_ribbon(alpha = 0.5) + geom_line() + facet_wrap(~STATIONS_ID, nrow = 3) + labs(x = &#39;Time&#39;, y = &#39;Temperature (°C)&#39;) One last detail. The titles on top of the facets show the station IDs. When you are not an employee of German Meteorological Service, you probably do not know them by hart. It is better to use the city names. The vector station_ids is a so called named vector and has the right structure to change the titles in the facets: it assigns to every id its city name, i.e. 2261 = ‘Hof’. station_ids ## 2261 1420 2667 ## &quot;Hof&quot; &quot;Frankfurt&quot; &quot;Koeln&quot; We use station_ids to change the titles: ggplot(monthly_means, aes(x = year_month, y = mean_T, ymin = mean_T - sd_T, ymax = mean_T + sd_T)) + geom_ribbon(alpha = 0.5) + geom_line() + facet_wrap(~STATIONS_ID, nrow = 3, labeller = labeller(STATIONS_ID = station_ids)) + labs(x = &#39;Time&#39;, y = &#39;Temperature (°C)&#39;) 5.3 Practice on your own! Plot the means and standard deviations of the air humidity instead of air temperature. Do the tutorials “Work with data” from the Primers collection by RStudio Cloud. You can access the tutorials here: https://rstudio.cloud/learn/primers/2 Do the tutorials “Visualize Data” from the Primers collection by RStudio Cloud. You can access the tutorials here:https://rstudio.cloud/learn/primers/3 5.4 Reading assignment Read chapter 3 (to 3.5) in Ismay and Kim (2021) 5.5 Additional reading and videos More information on statistical graphical summaries and geoms: R4DS Wickham and Grolemund (2021): Chapter 5 “Data transformation” A live exploratory data analysis by the main author of tidyverse, Hadley Wickham. Really informative, but Dr. Wickham types too fast . References Ismay, Chester, and Albert Y. Kim. 2021. ModernDive: Statistical Inference via Data Science. https://moderndive.com/. Wickham, Hadley, and Garrett Grolemund. 2021. R for Data Science. https://r4ds.had.co.nz/. "],["sampling.html", "6 Sampling and variability 6.1 Random sampling of data 6.2 Number of replications and variability 6.3 Sample size 6.4 Practice on your own! 6.5 Reading assignment 6.6 Turning in your work", " 6 Sampling and variability Conduct random sampling using a computer Explain variability in random sampling Calculate sampling distributions Calculate the standard error With this chapter, we start our journey into statistical inference. Statistical inference or simply inference goes beyond the analysis of single data sets and generalizes patterns which we observed in a single data set to a larger context. Often, this context is called population. And such generalization techniques are nothing else than estimation of population parameters. For example, if you want to know the mean income of a large group of people, you can either ask every person (if you have time and money to do so) or you ask a cleverly chosen group only, a sample, and try to estimate from their mean income, the mean income of the whole group. Another setting, where you will want to use inference, is when doing experiments, e.g. in the lab. Imagine you want to study the influence of increased temperature on the growth of a plant species. Then you would design an experiment where some plants of this species are grown at ambient temperature (control group) and some at increased temperature (treatment group). You measure their growth and want to know whether the difference observed is due to chance or whether this is a real difference, an effect of the treatment. And if this is a real difference, how large it is and how precisely we can estimate this effect. All these questions can be answered using inference. We will do inference based on data science and a computer. Nowadays, computational power is usually no longer a problem and cool statistical inference can be done based on computer simulations and so called resampling techniques. In this chapter, you will learn how to use a computer experiment to draw samples from a simulated data set. You will see that every random sample is different. This experience should elucidate the concept of randomness or chance and variability. 6.1 Random sampling of data We will use the following libraries. The library infer is a dedicated package for tidy inference. library(tidyverse) library(moderndive) We will simulate our own data set, our own population, and invent the getsmarter university with 12000 students. The code below draws random numbers and simulates the following variables: student_id: 1 to 12000 gender: male or female travel_time: time students travel to the university in minutes residence: type of place of residents, either town = urban or countryside = rural transport: how students travel/come to the university time_lib: time students spend in the university library in minutes We organize all these variables in a tibble that we call getsmarter_pop. Because we draw random numbers using R, every time we rerun the code, new numbers will be generated. To have a reproducible population, we use the function set.seed(). It accepts as a parameter an integer. Which integer we use does not really matter as long as we use the same every time we rerun the code. By setting the seed, we set the random number generate in R to a certain reproducible state. The numbers are still random, but reproducible . set.seed(123) student_id &lt;- 1:12000 travel_time &lt;- c(runif(n = 12000 * 0.8, min = 5, max = 40), runif(n = 12000 * 0.2, min = 60, max = 120)) gender &lt;- sample(c(&#39;m&#39;, &#39;f&#39;), size = 12000, replace = TRUE) residence &lt;- sapply(travel_time, function(x) { if(x &lt; 30) &#39;urban&#39; else &#39;rural&#39; }) transport &lt;- sapply(travel_time, function(x) { if(x &lt;= 10) &#39;foot&#39; else if(x &gt; 10 &amp; x &lt;= 15) sample(c(&#39;foot&#39;, &#39;bike&#39;), size = 1) else if(x &gt; 15 &amp; x &lt;= 45) sample(c(&#39;bus&#39;, &#39;bike&#39;, &#39;car&#39;), size = 1) else sample(c(&#39;bus&#39;, &#39;car&#39;), size = 1) }) time_lib &lt;- 5 * 60 - 0.7 * travel_time + rnorm(length(travel_time), 0, 20) getsmarter_pop &lt;- tibble(student_id, gender, residence, transport, travel_time, time_lib) getsmarter_pop ## # A tibble: 12,000 × 6 ## student_id gender residence transport travel_time time_lib ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 f urban bus 15.1 294. ## 2 2 f rural bike 32.6 254. ## 3 3 f urban bike 19.3 231. ## 4 4 m rural car 35.9 245. ## 5 5 m rural bus 37.9 234. ## 6 6 f urban foot 6.59 303. ## 7 7 f urban bus 23.5 284. ## 8 8 m rural car 36.2 274. ## 9 9 m urban bike 24.3 299. ## 10 10 f urban bus 21.0 282. ## # ℹ 11,990 more rows We conduct a survey among the students of getsmarter and record the values of the variables listed above. We select the students randomly. Therefore, the data set that we generate through our survey will be a random sample. To simulate such a survey, we use the function rep_sample_n(). It samples repeatedly n data points (students in our case) from a given data set (our population getsmarter_pop). Because we sample randomly (and use the random number generator in R to do so), we need to set the seed again for reproducibility. We will simulate a survey of 50 students and define the survey size by survey_size &lt;- 50. The survey will be done once, parameter reps = 1, and students can be interviewed only once, parameter replace = FALSE, in the function rep_sample_n(). set.seed(345) survey_size &lt;- 50 survey &lt;- rep_sample_n(getsmarter_pop, size = survey_size, replace = FALSE, reps = 1) The function rep_sample_n returns a grouped tibble. The variable replicate contained only the number 1 because rep = 1. It is an indicator variable of the replication. survey ## # A tibble: 50 × 7 ## # Groups: replicate [1] ## replicate student_id gender residence transport travel_time time_lib ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1623 m urban foot 7.06 299. ## 2 1 9171 m urban bike 11.3 278. ## 3 1 10207 f rural bus 107. 199. ## 4 1 3506 f urban bus 25.0 326. ## 5 1 8892 f urban bus 28.1 259. ## 6 1 5460 m urban bus 23.6 299. ## 7 1 6120 f urban bus 20.0 268. ## 8 1 865 f urban bike 26.6 290. ## 9 1 11586 m rural bus 114. 207. ## 10 1 8153 f urban foot 8.06 297. ## # ℹ 40 more rows How many students from the survey live in the town and how many in the countryside? survey %&gt;% group_by(residence) %&gt;% count() ## # A tibble: 2 × 2 ## # Groups: residence [2] ## residence n ## &lt;chr&gt; &lt;int&gt; ## 1 rural 21 ## 2 urban 29 Instead of actual numbers, we want to express residence as proportions. survey %&gt;% group_by(residence) %&gt;% summarise(prop = n()/survey_size) ## # A tibble: 2 × 2 ## residence prop ## &lt;chr&gt; &lt;dbl&gt; ## 1 rural 0.42 ## 2 urban 0.58 42% of survey students live in the countryside and 58% in town. But which proportions would we obtain if we repeated the survey? Let’s repeat the survey 33 times and observe the variability in residence. This is an unrealistic scenario for real life, however, for a computer experiment, no problem. We set reps = 33. set.seed(234) survey_reps &lt;- rep_sample_n(getsmarter_pop, size = survey_size, replace = FALSE, reps = 33) Now survey_reps shows 33 replicates and the variable replicate ranges from 1 to 33. The data set has 1650 = 33 \\(\\times\\) survey_size rows. survey_reps ## # A tibble: 1,650 × 7 ## # Groups: replicate [33] ## replicate student_id gender residence transport travel_time time_lib ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2079 m rural car 38.8 262. ## 2 1 1314 m urban bike 13.3 301. ## 3 1 1710 m urban car 26.5 272. ## 4 1 4386 f urban bus 23.9 269. ## 5 1 9490 m rural car 34.2 262. ## 6 1 11757 f rural bus 102. 227. ## 7 1 11649 f rural bus 111. 202. ## 8 1 2244 m rural bus 38.9 256. ## 9 1 3652 f urban bike 10.3 254. ## 10 1 3127 m urban bike 29.6 271. ## # ℹ 1,640 more rows How do the proportions in residence vary from survey to survey? Now we must group by residence and replicate. residence_props &lt;- survey_reps %&gt;% group_by(replicate, residence) %&gt;% summarise(prop = n()/survey_size) residence_props ## # A tibble: 66 × 3 ## # Groups: replicate [33] ## replicate residence prop ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 rural 0.42 ## 2 1 urban 0.58 ## 3 2 rural 0.36 ## 4 2 urban 0.64 ## 5 3 rural 0.4 ## 6 3 urban 0.6 ## 7 4 rural 0.38 ## 8 4 urban 0.62 ## 9 5 rural 0.4 ## 10 5 urban 0.6 ## # ℹ 56 more rows The numbers vary from replicate to replicate because in every survey replicate different students were drawn at random. Let’s look at the distribution of the proportions in a histogram. ggplot(data = residence_props, aes(x = prop)) + geom_histogram(binwidth = 0.05, boundary = 0.4, col = &#39;white&#39;) + facet_wrap(~ residence) + labs(x = &#39;Proportion of residence&#39;, title = &#39;Distribution of residences&#39;, y = &#39;Frequency&#39;) The most frequent proportions for rural are around 40% and for urban around 60%. Because we have chosen binwidth = 0.05, i.e. the width of the columns is 5%, we can be more precise. Among the surveyed students, 35–40% live in the countryside and 55–60% in town. 6.2 Number of replications and variability Our histograms show that proportions of residence vary from survey to survey. 33 is a large number in real life, but a small number in statistics. Which patterns will we observe in our histograms if we repeat the survey 1000 times? We use a variable reps_num to define the number of repetitions. set.seed(345) reps_num &lt;- 1000 survey_reps &lt;- rep_sample_n(getsmarter_pop, size = survey_size, replace = FALSE, reps = reps_num) residence_props &lt;- survey_reps %&gt;% group_by(replicate, residence) %&gt;% summarise(prop = n()/survey_size) ggplot(data = residence_props, aes(x = prop)) + geom_histogram(binwidth = 0.02, boundary = 0.4, col = &#39;white&#39;) + facet_wrap(~ residence, scales = &#39;free_x&#39;) + labs(x = &#39;Proportion of residence&#39;, title = &#39;Distribution of residences&#39;, y = &#39;Frequency&#39;) The histograms show now a nice symmetrical pattern around 40–42% for rural and 58–60% for urban residences. The parameter scales = 'free_x' allows scaling each histogram’s \\(x\\) axis separately. Such a distribution is called a sampling distribution. It shows the distribution of possible values of a statistics (“proportions of residence” in our case) that one obtains by a repeated sampling from a population. The statistics “proportion of residence” is a random variable. Every new survey brings new values. The sampling distribution can tell us which values are frequent, i.e. more probable to occur if we survey our students randomly. To summarize a sampling distribution, we can calculate its mean value and standard deviation. The latter has a special name, standard error. residence_props %&gt;% group_by(residence) %&gt;% summarise(prop_sd = sd(prop)) ## # A tibble: 2 × 2 ## residence prop_sd ## &lt;chr&gt; &lt;dbl&gt; ## 1 rural 0.0701 ## 2 urban 0.0701 The standard errors for rural and urban are identical because they are dependent: rural = 1 - urban. 6.3 Sample size The standard error depends on the sample size. Let’s repeat our survey for 25, 50 and 100 students, respectively and observe how the standard error vary. For repetitive tasks like this, it is better to define a function that does the job instead of copy-pasting the code to vary the sample size. I will define such a function without many comments. We will talk about functions later, if time allows. calculate_props &lt;- function(population = getsmarter_pop, survey_size, reps_num = 1000) { survey &lt;- rep_sample_n(population, size = survey_size, replace = FALSE, reps = reps_num) residence_props &lt;- survey %&gt;% group_by(replicate, residence) %&gt;% summarise(prop = n()/survey_size) residence_props } Let’s survey. set.seed(123) # Sample size 25 residence_props_25 &lt;- calculate_props(population = getsmarter_pop, survey_size = 25, reps_num = 1000) # Sample size 50 residence_props_50 &lt;- calculate_props(population = getsmarter_pop, survey_size = 50, reps_num = 1000) # Sample size 100 residence_props_100 &lt;- calculate_props(population = getsmarter_pop, survey_size = 100, reps_num = 1000) We plot the resulting sampling distributions. ggplot(data = residence_props_25, aes(x = prop)) + geom_histogram(binwidth = 0.04, boundary = 0.4, col = &#39;white&#39;) + facet_wrap(~ residence, scales = &#39;free_x&#39;) + labs(x = &#39;Proportion of residence&#39;, title = &#39;Distribution of residences, sample size = 25&#39;, y = &#39;Frequency&#39;) ggplot(data = residence_props_50, aes(x = prop)) + geom_histogram(binwidth = 0.02, boundary = 0.4, col = &#39;white&#39;) + facet_wrap(~ residence, scales = &#39;free_x&#39;) + labs(x = &#39;Proportion of residence&#39;, title = &#39;Distribution of residences, sample size = 50&#39;, y = &#39;Frequency&#39;) ggplot(data = residence_props_100, aes(x = prop)) + geom_histogram(binwidth = 0.02, boundary = 0.4, col = &#39;white&#39;) + facet_wrap(~ residence, scales = &#39;free_x&#39;) + labs(x = &#39;Proportion of residence&#39;, title = &#39;Distribution of residences, sample size = 100&#39;, y = &#39;Frequency&#39;) We compare the standard errors of the three sampling distributions. residence_props_25 %&gt;% group_by(residence) %&gt;% summarise(prop_sd = sd(prop)) ## # A tibble: 2 × 2 ## residence prop_sd ## &lt;chr&gt; &lt;dbl&gt; ## 1 rural 0.102 ## 2 urban 0.102 residence_props_50 %&gt;% group_by(residence) %&gt;% summarise(prop_sd = sd(prop)) ## # A tibble: 2 × 2 ## residence prop_sd ## &lt;chr&gt; &lt;dbl&gt; ## 1 rural 0.0679 ## 2 urban 0.0679 residence_props_100 %&gt;% group_by(residence) %&gt;% summarise(prop_sd = sd(prop)) ## # A tibble: 2 × 2 ## residence prop_sd ## &lt;chr&gt; &lt;dbl&gt; ## 1 rural 0.0458 ## 2 urban 0.0458 You see that the standard error decreases with increasing sample size. This makes sense because the larger the sample size, the more information it contains about the population, i.e. it is more representative. Accordingly, the variability of the information (measured by the standard error) decreases. This time, I have some pre-formulated take-home messages because sampling and variability are crucial topics . A random sample\\(^*\\) can be used to obtain information about a larger group, a population. A statistics calculated from a random sample is sometimes called a sampling statistics. The distribution of a statistics calculated from random samples is called sampling distribution. A sampling distribution is obtained by repeated random sampling. The more random samples are drawn, the better one can characterize the sampling distribution (i.e. its shape). The standard error is the standard deviation of a sampling statistics. The standard error decreases with increasing sample size. \\(^*\\) Sometimes, random sampling is not appropriate and stratified sampling or more complex sampling designs should be used. This is the case, when subgroups with different properties relevant for the sampling, statistics exist in the population. However, this topic is beyond the scope of this course. 6.4 Practice on your own! How large are the proportions of rural and urban in the population of students from getsmarter. How large are the mean values from the sampling distributions with sample sizes 25, 50 and 100. Compare these figures to the population parameters and comment. The student representatives would like to see more students using a bike or a bus instead of a car. As a first step, they need to know how many students actually use a car. Conduct a repeated survey (1000 repetitions) of 50 students and estimate the proportion of people using a car. How large is the standard error of this proportion? Compare to the proportion in the population. Hint: Before you calculate the mean of all proportions estimated in the 1000 replicates, use ungroup() to delete the grouping by replicates. 6.5 Reading assignment Chapter 7 in Ismay and Kim (2021) 6.6 Turning in your work Save your R Notebook as an *.Rmd file. Upload your R Notebook to ILIAS. You don’t need to upload the .nb.html file. You will find an upload option in today’s session. You should receive a solution file after your upload. Be sure to upload before the deadline! References Ismay, Chester, and Albert Y. Kim. 2021. ModernDive: Statistical Inference via Data Science. https://moderndive.com/. "],["bootstrap.html", "7 Bootstrap and confidence intervals 7.1 The formulas 7.2 Bootstrap – use your computer 7.3 Confidence intervals for the mean travel time 7.4 Bootstrap with the library infer 7.5 Interpreting the confidence intervals 7.6 Practice on your own! 7.7 Reading assignment 7.8 Turning in your work", " 7 Bootstrap and confidence intervals Explain the idea behind the bootstrap Calculate bootstrap confidence intervals for the mean In chapter 6 we studied the variability of repeated random sampling from a population. We did computer experiments and sampled from a simulated population repeatedly without replacement (i.e. the data could appear in the sample only once) and calculated a statistics, the proportion of rural and urban residents. Such a statistics is a random variable, and it is characterized by the sampling distribution. It shows which values you can expect by randomly sampling from a population. We learned that the shape of the sampling distribution and the size of its standard error depended on the sample size. In this chapter, we will learn how to quantify the standard error or to calculate plausible values (confidence intervals) for real-life applications, when you have one sample only. 7.1 The formulas You will never have the opportunity to sample/survey repeatedly to estimate the information (i.e. a statistics or parameter) about the population of interest. In real-life applications, you will have one (hopefully cleverly obtained) random sample. So, how can we have access to the shape of the sampling distribution? For several standard statistics, there are formulas that describe the standard error. You have probably learned that the standard error for the estimated mean equalled \\(\\sigma/\\sqrt(n)\\), \\(\\sigma\\) being the variance of the population and \\(n\\) the sample size. If you don’t know the variance of the population, then you have to estimate it from the sample itself. But where does this formula come from? The crucial Central Limit Theorem tells us that the random variable mean that we can estimate from our random sample is normally distributed with a mean equalled to the true mean of the population and the standard deviation of, you guessed it, \\(\\sigma/\\sqrt(n)\\). Based on the normal distribution, we can calculate plausible values, a so-called confidence interval. Informally, we could define the confidence interval as follows: Is there a plausible range of values that we could obtain if we randomly sample from a population? Plausible means that if we repeated the sampling very often, this range would contain the true mean in let’s say 95% of the time. Such a plausible range is called a 95% confidence interval. Because we know the theoretical distribution of the mean we can calculate the 95% confidence interval as \\(\\hat\\mu \\pm 1.96 \\cdot SE\\), with \\(\\hat\\mu\\) being the estimated mean, \\(SE\\) the standard error \\(\\sigma/\\sqrt(n)\\) and the magic factor of 1.96 comes from the fact that in a normal distribution, 95% of all values fall into the interval \\(\\mu \\pm 1.96 \\cdot sd\\), \\(sd\\) being the standard deviation. 7.2 Bootstrap – use your computer Formulas date from the early time of statistics, where computational power was limited or unavailable and approximations were the only tools one could use to estimate the variability. The Central Limit Theorem is very useful, but theoretical distributions do not always exist. In such cases, we can use our computer to calculate confidence intervals using re-sampling, i.e. repeated sampling from our random sample. This method is called bootstrap (or bootstrapping) and sounds like self-deception à la Baron Munchausen at first glance (Figure 7.1). However, it has a solid mathematical foundation (Efron 1979). Figure 7.1: Münchhausen removes himself from the swamp using his own braids (Theodor Hosemann (1807-1875), Public domain, via Wikimedia Commons) Link to figure Figure 7.2 shows the setup that we practised in chapter 6. We wanted to estimate a parameter (proportion of rural and urban residents) called \\(\\mu\\) in the figure, and we sampled randomly several times from the population. From all those random samples, we calculated our statistics and obtained the sampling distribution. Figure 7.2: Calculation of a sampling distribution by repeated random sampling from a population. Figure by Hesterberg (2015), their Figure 4. The publication is open-access and can be used for non-commercial purposes. Link to licence Figure 7.3 looks nearly the same, except one important difference. We don’t have access to several random samples from the population, but have one random sample only. And now we remember that we invested time and energy in thinking about the best way to obtain this sample, and we believe that it is representative of our population. Thus, we can think about this sample as our mini population. We replace the population by its miniature, i.e. our sample, and sample from this sample as if it were the true population. However, because the sample is limited in size, we have to sample with replacement. The random samples we obtain by sampling with replacement from the original random sample are called bootstrap samples. For each of those bootstrap samples, we calculate the statistics and obtain its bootstrap sampling distribution. The confidence intervals are most often calculated by the so-called percentile method: we take the 2.5% quantile and the 97.5% quantile from the sampling distribution. The range of values in-between equals the 95% bootstrap confidence interval. Figure 7.3: Calculation of a sampling distribution by repeated random sampling with replacement from a sample. Figure by Hesterberg (2015), their Figure 5. The publication is open-access and can be used for non-commercial purposes. Link to licence 7.3 Confidence intervals for the mean travel time Let’s come back to our simulated getsmarter university population and estimate the mean travel time with its bootstrap confidence interval. library(tidyverse) library(infer) set.seed(123) student_id &lt;- 1:12000 travel_time &lt;- c(runif(n = 12000 * 0.8, min = 5, max = 40), runif(n = 12000 * 0.2, min = 60, max = 120)) gender &lt;- sample(c(&#39;m&#39;, &#39;f&#39;), size = 12000, replace = TRUE) residence &lt;- sapply(travel_time, function(x) { if(x &lt; 30) &#39;urban&#39; else &#39;rural&#39; }) transport &lt;- sapply(travel_time, function(x) { if(x &lt;= 10) &#39;foot&#39; else if(x &gt; 10 &amp; x &lt;= 15) sample(c(&#39;foot&#39;, &#39;bike&#39;), size = 1) else if(x &gt; 15 &amp; x &lt;= 45) sample(c(&#39;bus&#39;, &#39;bike&#39;, &#39;car&#39;), size = 1) else sample(c(&#39;bus&#39;, &#39;car&#39;), size = 1) }) time_lib &lt;- 5 * 60 - 0.7 * travel_time + rnorm(length(travel_time), 0, 20) getsmarter_pop &lt;- tibble(student_id, gender, residence, transport, travel_time, time_lib) getsmarter_pop ## # A tibble: 12,000 × 6 ## student_id gender residence transport travel_time time_lib ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 f urban bus 15.1 294. ## 2 2 f rural bike 32.6 254. ## 3 3 f urban bike 19.3 231. ## 4 4 m rural car 35.9 245. ## 5 5 m rural bus 37.9 234. ## 6 6 f urban foot 6.59 303. ## 7 7 f urban bus 23.5 284. ## 8 8 m rural car 36.2 274. ## 9 9 m urban bike 24.3 299. ## 10 10 f urban bus 21.0 282. ## # ℹ 11,990 more rows We survey 200 students using the function rep_sample_n, with one replicate and without replacement. This function adds a new variable called replicate to the dataset. Later we will use the same function for bootstrapping, which will generate another replicate variable. To prevent conflicts from having duplicate variable names, we need to remvoe the existing replicate variable now. We do this using select(!...), which drops a variable from the dataset. set.seed(345) survey_size &lt;- 200 survey &lt;- rep_sample_n(getsmarter_pop, size = survey_size, replace = FALSE, reps = 1) %&gt;% ungroup() %&gt;% select(!replicate) The true mean travel time to the university and the mean from the survey equal mean_pop &lt;- getsmarter_pop %&gt;% summarise(mean = mean(travel_time)) mean_pop ## # A tibble: 1 × 1 ## mean ## &lt;dbl&gt; ## 1 36.0 mean_survey &lt;- survey %&gt;% summarise(mean = mean(travel_time)) mean_survey ## # A tibble: 1 × 1 ## mean ## &lt;dbl&gt; ## 1 34.1 To calculate the bootstrap distribution, we sample from the data set survey with replacement. The size of the bootstrap samples always equal the size of the original sample. We use the same function rep_sample_n as in chapter 6, but change replace = TRUE. set.seed(345) reps_num &lt;- 10000 survey_reps_bootstrap &lt;- rep_sample_n(survey, size = survey_size, replace = TRUE, reps = reps_num) survey_reps_bootstrap ## # A tibble: 2,000,000 × 7 ## # Groups: replicate [10,000] ## replicate student_id gender residence transport travel_time time_lib ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 7038 m urban bus 29.4 243. ## 2 1 493 m urban bus 21.1 303. ## 3 1 3442 m rural car 30.3 311. ## 4 1 4920 f urban bus 21.1 290. ## 5 1 3178 f urban foot 8.07 325. ## 6 1 7694 f rural car 31.3 295. ## 7 1 9479 m rural bus 39.5 302. ## 8 1 3367 m rural car 36.4 272. ## 9 1 5985 f urban bike 15.9 303. ## 10 1 843 m urban bike 26.4 300. ## # ℹ 1,999,990 more rows Because we sample with replacement, students can now appear several times in one and the same bootstrap sample. Let’s check this for the first 50 bootstrap samples. Student 4787 was sampled 8 times in the replicate 43, for example. survey_reps_bootstrap %&gt;% filter(replicate %in% (1:50)) %&gt;% group_by(replicate, student_id) %&gt;% tally() %&gt;% filter(n != 1) %&gt;% arrange(desc(n)) ## # A tibble: 2,657 × 3 ## # Groups: replicate [50] ## replicate student_id n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 43 4787 8 ## 2 20 5985 6 ## 3 34 7749 6 ## 4 38 5641 6 ## 5 41 8456 6 ## 6 3 7083 5 ## 7 3 10943 5 ## 8 5 8994 5 ## 9 6 10118 5 ## 10 8 11425 5 ## # ℹ 2,647 more rows Now we can calculate the mean travel times from the bootstrap samples. res_means_bootstrap &lt;- survey_reps_bootstrap %&gt;% group_by(replicate) %&gt;% summarise(mean_tt = mean(travel_time)) res_means_bootstrap ## # A tibble: 10,000 × 2 ## replicate mean_tt ## &lt;int&gt; &lt;dbl&gt; ## 1 1 32.6 ## 2 2 31.9 ## 3 3 38.9 ## 4 4 33.5 ## 5 5 35.4 ## 6 6 37.2 ## 7 7 34.4 ## 8 8 33.4 ## 9 9 35.5 ## 10 10 31.0 ## # ℹ 9,990 more rows The standard error and the 95% confidence interval based on the bootstrap are calculated as follows: stat_bootstrap &lt;- res_means_bootstrap %&gt;% summarize(mean_bootstrap = mean(mean_tt), se = sd(mean_tt), ci_2.5 = quantile(mean_tt, probs = 0.025), ci_97.5 = quantile(mean_tt, probs = 0.975)) stat_bootstrap ## # A tibble: 1 × 4 ## mean_bootstrap se ci_2.5 ci_97.5 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 34.1 2.06 30.1 38.3 7.4 Bootstrap with the library infer The library infer offers a convenient framework for calculating the bootstrap confidence intervals and to do hypothesis tests. We will talk about the latter in a later session. infer is a dedicated package for tidy inference and is organized around 5 verbs: specify() variables or relationships between them hypothesize() define the null hypothesis (for hypothesis tests only) generate() generate data either for the confidence intervals or under the null hypothesis calculate() the sampling distribution visualize() visualize the sampling distribution How does our example of calculating the bootstrap confidence intervals for the mean travel time look like in the infer workflow? We first calculate the bootstrap distribution. set.seed(345) bootstrap_distribution &lt;- survey %&gt;% specify(response = travel_time) %&gt;% generate(reps = 10000, type = &#39;bootstrap&#39;) %&gt;% calculate(stat = &#39;mean&#39;) Then, we calculate the confidence intervals based on the percentile method. percentile_ci &lt;- bootstrap_distribution %&gt;% get_confidence_interval(level = 0.95, type = &quot;percentile&quot;) percentile_ci ## # A tibble: 1 × 2 ## lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; ## 1 30.1 38.3 And then visualize all results. Because the function visualize is based on ggplot2, we can add custom axis labels. visualize(bootstrap_distribution) + shade_confidence_interval(endpoints = percentile_ci, color = &quot;orange&quot;, fill = &quot;khaki&quot;) + geom_vline(xintercept = mean_pop$mean, linetype = &#39;dashed&#39;) + labs(x = &#39;Mean trave time (min)&#39;, y = &#39;Frequency&#39;) 7.5 Interpreting the confidence intervals The upper and lower limit of the confidence interval depend on the sample, i.e. they are themselves random variables. Definitely, random variables are everywhere . It means that the confidence interval could fail to include the true population parameter. We are now ready for a more formal definition of a confidence interval: If we repeat the sampling very often and recalculate the 95% confidence intervals, we expect them to contain the true population parameters in about 95% of the time. It means that some confidence intervals won’t contain the true population parameter. The confidence interval is our best guess of plausible values for the population parameter. In case of a theoretical distribution of the statistics, the bootstrap and the theoretical confidence intervals coincide. Often, the interpretation is short-handed as: We are 95% confident that the confidence interval captures the true parameter. This is incorrect. It is better to state that in 95% of the time, the confidence interval captures the true population parameter. And now you know what is meant by “95% of the time” . 7.6 Practice on your own! Repeat the analysis we did for the mean travel time now for the proportion of urban residents. Use the workflow in infer. Hint: specify(response = residence, success = 'urban'). How does the width of the confidence interval depend on the sample size? Repeat the analysis for the mean travel time for a survey of 30 students. 7.7 Reading assignment Chapter 8 in Ismay and Kim (2021) 7.8 Turning in your work Save your R Notebook as an *.Rmd file. Upload your R Notebook to ILIAS. You don’t need to upload the .nb.html file. You will find an upload option in today’s session. You should receive a solution file after your upload. Be sure to upload before the deadline! References Efron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26. https://doi.org/10.1214/aos/1176344552. Hesterberg, Tim C. 2015. “What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum.” The American Statistician 69 (4): 371–86. https://doi.org/10.1080/00031305.2015.1089789. Ismay, Chester, and Albert Y. Kim. 2021. ModernDive: Statistical Inference via Data Science. https://moderndive.com/. "],["hypothesis.html", "8 Hypothesis tests versus effect size estimation 8.1 Hypothesis tests with infer 8.2 There is only one test! 8.3 Hypothesis test versus effect size estimation 8.4 Practice on your own! 8.5 Reading assignment 8.6 Additional resources 8.7 Turning in your work", " 8 Hypothesis tests versus effect size estimation Explain the idea behind simulation-based hypothesis tests Calculate hypothesis tests with infer Explain the difference between a test and an effect size estimation In chapter 7 we learned how to calculate confidence intervals, a range of plausible values. Or more formally, an interval that expect to contain the true population parameters in about 95% of the time, if we repeat the sampling very often. In this chapter, we will learn what hypothesis tests are and why it is often better to estimate effect sizes. 8.1 Hypothesis tests with infer library(tidyverse) library(infer) library(knitr) Let’s start with an old study that was done in the 1970. Rosen and Jerdee (1974) analysed whether “sex role stereotypes [had] an influence on personnel decisions”. In our modern language, they wanted to know whether women are discriminated against in promotion decisions. They asked 95 bank supervisors to decide based on a personnel file only whether a person should be promoted or not. All files were identical except the gender and the files were distributed randomly to the participants. Because of this random assignment of files, the procedure is an experiment, and we can draw conclusions about the research question. Here, we look at one part of the data, namely promotion decision for a simple job (c.f. the original publication for more details.) study &lt;- tibble(&#39;gender&#39; = c(rep(&#39;male&#39;, 24), rep(&#39;female&#39;, 24)), &#39;decision&#39; = c(rep(&#39;promoted&#39;, 21), rep(&#39;not_promoted&#39;, 3), rep(&#39;promoted&#39;, 14), rep(&#39;not_promoted&#39;, 10))) study %&gt;% table() %&gt;% addmargins() %&gt;% kable() not_promoted promoted Sum female 10 14 24 male 3 21 24 Sum 13 35 48 Let’s plot the data before we analyse it. ggplot(study, aes(x = gender, fill = decision)) + geom_bar() + labs(x = &#39;Gender on personnel file&#39;, y = &#39;Abs. frequency&#39;, title = &#39;Decisions of bank supervisors from study by Rosen &amp; Jerdee (1974)&#39;) How large is the difference of proportions in promotions between men and women? Is this difference due to chance, or are women discriminated against? A hypothesis test helps to decide whether an observed effect, in our case the difference in proportions of promotions, can be attributed to chance. In a hypothesis test, two different statements are contrasted, a null hypothesis and an alternative hypothesis: Statement 1: There is no real difference, the observed difference is due to chance. H\\(_0\\): Null hypothesis. The variables gender and decision are independent; the observed difference in promotions is random. Statement 2: Women are discriminated against. H\\(_A\\): Alternative hypothesis. The variables gender and decision are dependent. Women are discriminated against in decisions about promotion. We will use a general framework in the package infer for hypothesis tests. It offers a state-of-the-art simulation-based approach and is based on the following steps (Figure 8.1): specify() variables or relationships between them hypothesize() define the null hypothesis generate() generate data under the null hypothesis calculate() the sampling distribution under the null hypothesis visualize() visualize the sampling distribution under the null hypothesis Two additional functions, shade_p_value and get_p_value visualize and calculate the \\(p\\) value (see below for definition), respectively. Figure 8.1: General framework in infer. (Source: https://infer.netlify.app/). Let’s proceed step by step. First, we calculate the observed difference from our data using infer. The parameter order = c(\"male\", \"female\") tells us that we calculate the difference as male - female. prop_hat &lt;- study %&gt;% specify(formula = decision ~ gender, success = &quot;promoted&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;male&quot;, &quot;female&quot;)) prop_hat ## Response: decision (factor) ## Explanatory: gender (factor) ## # A tibble: 1 × 1 ## stat ## &lt;dbl&gt; ## 1 0.292 We observe 29.2% less women promoted than men. Let’s see whether this difference can be attributed to chance. We generate data under the null hypothesis, i.e. a world where gender and decision to promote are completely independent. Then we compare the data we obtain in this world with the observed 29.2% difference between men and women. We generate the data under the null hypotheses by permutation. It means that if gender and decision are unrelated, then we could have observed other combinations of them. Therefore, any permuted combination would be in agreement with the null hypothesis of independence of these variables. We generate 10000 permuted samples. set.seed(123) null_distn &lt;- study %&gt;% specify(formula = decision ~ gender, success = &quot;promoted&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 10000) %&gt;% calculate(stat = &quot;diff in props&quot;, order = c(&quot;male&quot;, &quot;female&quot;)) ## Setting `type = &quot;permute&quot;` in `generate()`. null_distn ## Response: decision (factor) ## Explanatory: gender (factor) ## Null Hypothesis: independence ## # A tibble: 10,000 × 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 0.125 ## 2 2 0.292 ## 3 3 -0.0417 ## 4 4 -0.0417 ## 5 5 -0.125 ## 6 6 -0.0417 ## 7 7 0.0417 ## 8 8 0.0417 ## 9 9 0.0417 ## 10 10 0.0417 ## # ℹ 9,990 more rows The object null_distn contains 10000 differences in proportions of promotion from the permuted data. We can visualize their distribution and highlight the observed difference to compare. The parameter direction = \"greater\" defines the alternative hypothesis that women are discriminated against, i.e. the difference is larger than zero. The function shade_p_value shades values that are larger than the observed statistics. The red bar shows the observed statistic itself. visualize(null_distn, bins = 12) + shade_p_value(obs_stat = prop_hat, direction = &quot;greater&quot;) The \\(p\\) value is the probability to obtain data as extreme or more extreme than what has been observed, if the null hypothesis is true. The \\(p\\) value in our example equals null_distn %&gt;% get_p_value(obs_stat = prop_hat, direction = &#39;greater&#39;) ## # A tibble: 1 × 1 ## p_value ## &lt;dbl&gt; ## 1 0.0268 This \\(p\\) value is really small. It is rare to observe a difference as large as 29.2%, if we assume that women and men are promoted equally often. We reject the null hypothesis of independence and conclude that women are discriminated against in promotion. A cautionary note: In this experiment, we assumed that the participating bank supervisors were representative for their profession in the 1970s. Thus, it is more cautious to conclude that women were discriminated against in the 1970 (provided the white male bank supervisors are representative for the then-population of decision-makers). 8.2 There is only one test! When you read through literature, you will find many tests. It is difficult to keep all those names and remember when to use what. The good news is that it is unnecessarily complicated and can be simplified with a test procedure based on computer simulations as we did in infer. The general logic behind simulation-based tests is shown in Figure 8.2. Figure 8.2: General logic behind any simulation-based hypothesis test. (Source: http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html. Used with permission by the author Prof. Allen Downey). The test procedure is based on the same steps, regardless which test you want to apply. Step 1: calculate the test statistic in your data Summarize your data with a test statistic. It can be a mean or a difference in proportions. In Figure 8.2 this statistic is called \\(\\sigma*\\). Step 2: formulate the null hypothesis Think about your research question. What do you want to know? Derive a model of a world where this effect is absent. This is your null hypothesis, \\(H_0\\). This model can be a permutation of variables or a theoretical model or a complicated model . Step 3: generate the data under the null hypothesis Generate the data, i.e. many data samples, using your model of \\(H_0\\) and calculate the same statistic as you did for your observed data for each of those samples. Step 4: calculate the sampling distribution These statistics under the null give you a sampling distribution that you can visualize. Additionally, visualize the observed statistic \\(\\sigma*\\) and compare. Does such a value often appear in your sampling distribution or is it rare? The \\(p\\) value gives you the frequency of how often the sampling distribution has values that are at least as large as your \\(\\sigma*\\). Step 5: Decide! Is the \\(p\\) values small? Use your domain knowledge to frame the result. No need to refer to statistical significance here! Formulate a conclusion in plain language. Think about how you obtained the data. There is always uncertainty, so be modest and do not overstate your finding! There is so much more to say about the misuse of \\(p\\) values and the term “statistical significance”. A good starting point is the paper by Wasserstein, Schirm, and Lazar (2019). 8.3 Hypothesis test versus effect size estimation Compared to a statistical test, the estimation of a so-called effect size can be much more interesting. Effect size can be the difference in means or proportions or more elaborated (standardized) effects. While a statistical test gives you a “feeling” of whether the observed effect could be attributed to chance, estimation of the effect size and its confidence intervals gives you a range of plausible values. It is much more interesting and relevant to discuss those and what they could mean for real life than just to state that something is (not) due chance. Prefer effect size and confidence intervals where possible to hypothesis tests. 8.4 Practice on your own! Does the travel time and the time spent in the library correlate in our getsmarter population? Do a statistical test based on a survey of 200 students. Formulate the null and alternative hypothesis before doing the test. Hint: specify(travel_time ~ time_lib) and calculate(stat = \"correlation\"). Consult the website of the library infer (c.f. below). Does the travel time and the time spent in the library correlate in our getsmarter population? This time, estimate the correlation and calculate its confidence interval. Compare with the hypothesis test. 8.5 Reading assignment Chapter 9 in Ismay and Kim (2021) 8.6 Additional resources Webpage of infer: https://infer.netlify.app/ Great talk by the author of infer. 8.7 Turning in your work Save your R Notebook as an *.Rmd file. Upload your R Notebook to ILIAS. You don’t need to upload the .nb.html file. You will find an upload option in today’s session. You should receive a solution file after your upload. Be sure to upload before the deadline! References Ismay, Chester, and Albert Y. Kim. 2021. ModernDive: Statistical Inference via Data Science. https://moderndive.com/. Rosen, Benson, and Thomas H. Jerdee. 1974. “Influence of Sex Role Stereotypes on Personnel Decisions.” Journal of Applied Psychology 59 (1): 9–14. https://doi.org/10.1037/h0035834. Wasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019. “Moving to a World Beyond ‘p \\(&lt;\\) 0.05’.” The American Statistician 73 (sup1): 1–19. https://doi.org/10.1080/00031305.2019.1583913. "],["lin-reg.html", "9 Linear regression 9.1 What is a linear regression model? 9.2 Simple linear regression 9.3 Example: Relationship between the time in the library and the travel time 9.4 Practice on your own! 9.5 Reading assignment", " 9 Linear regression Explain the general structure of a linear model. Name the assumption of a linear model. Calculate a simple linear regression in R. With this chapter, we start our journey to statistical modelling. Basically, there are two main reasons why one would like to build a statistical model. Explicative modelling: We suppose that there is a relationship between variables and would like to quantify it. Predictive modelling: We want to build a model that can (as accurately as possible) predict future values. In this course, we will focus on explicative modelling and work with linear regression models. 9.1 What is a linear regression model? The term regression was coined by Francis Galton (1822-1911) who studied the relationship between the height of parents and their children (Fahrmeir, Kneib, and Lang 2009). He showed that taller-than-average parents tended to have children who were smaller than their parents (more towards the average) and smaller-than-average parents tended to have taller children. He called this phenomenon regression towards the mean. A regression model estimates the relationship between a dependent variable, often called \\(y\\) and independent variables (or predictors) often termed \\(X\\). Note the capital letter \\(X\\) to indicate that it can be a collection of independent variables (a matrix). We try a definition: A regression model has the form: \\[y = f(X) + \\varepsilon\\] \\(y\\): dependent variable \\(f\\): type of relationship (function) \\(X\\): independent variables (predictors, explanatory variables) \\(\\varepsilon\\): error term If: \\(f\\) linear (effect of predictors is additive), the model is called linear regression model \\(X\\) is a single predictor, the model is called simple linear regression model, otherwise multiple linear regression model Components of the model: \\(f(X)\\): systematic or deterministic component \\(\\varepsilon\\): stochastic component (error term, residuals) In a regression model, we are not after the exact values of the dependent variable, but after the systematic influence of predictors on the mean value of the dependent variable. Because of the error term (which can contain random fluctuations, measurement errors etc.), the dependent variable \\(y\\) is a random variable. 9.2 Simple linear regression In case we have one predictor only, the model is a simple linear regression model and its geometric form is a line. A line is defined by its intercept (the point where it cuts the \\(y\\) axis) and its slope. This is precisely the form of a simple linear regression model: Given data pairs: \\((y_i,x_i), \\quad i=1,\\dots,n\\) \\(y\\) and \\(x\\) are numeric variables. We call the model \\[y_i=\\beta_0 + \\beta_1x_i + \\varepsilon_i, \\qquad i=1,\\dots,n.\\] a simple linear regression model, if the errors \\(\\varepsilon_1,\\dots, \\varepsilon_n\\) are independent and identically distributed (iid) and \\[\\mathrm{E}(\\varepsilon_i) = 0, \\qquad \\mathrm{Var}(\\varepsilon_i)=\\sigma^2.\\] If additionally \\[\\varepsilon_i \\sim N(0,\\sigma^2)\\] i.e. the residuals are normally distributed, we call the model a normal linear regression model. \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) the slope of the model. The assumptions of the normal linear regression model can be summarized as LINE: Linear relationship between variables Independence of residuals Normal residuals Equality of variance (called homoscedasticity) and a mean of zero in residuals Before you look at the model results, the confidence intervals of the slope or the intercept, you have to ensure that those assumptions are met! 9.3 Example: Relationship between the time in the library and the travel time We come back to our lovely getsmarter university and want to study the relationship between the time a student needs to come to the university and the time she spends in the library. library(tidyverse) library(infer) library(moderndive) library(knitr) Let’s generate the population. set.seed(123) student_id &lt;- 1:12000 travel_time &lt;- c(runif(n = 12000 * 0.8, min = 5, max = 40), runif(n = 12000 * 0.2, min = 60, max = 120)) gender &lt;- sample(c(&#39;m&#39;, &#39;f&#39;), size = 12000, replace = TRUE) residence &lt;- sapply(travel_time, function(x) { if(x &lt; 30) &#39;urban&#39; else &#39;rural&#39; }) transport &lt;- sapply(travel_time, function(x) { if(x &lt;= 10) &#39;foot&#39; else if(x &gt; 10 &amp; x &lt;= 15) sample(c(&#39;foot&#39;, &#39;bike&#39;), size = 1) else if(x &gt; 15 &amp; x &lt;= 45) sample(c(&#39;bus&#39;, &#39;bike&#39;, &#39;car&#39;), size = 1) else sample(c(&#39;bus&#39;, &#39;car&#39;), size = 1) }) time_lib &lt;- 5 * 60 - 0.7 * travel_time + rnorm(length(travel_time), 0, 20) getsmarter_pop &lt;- tibble(student_id, gender, residence, transport, travel_time, time_lib) getsmarter_pop ## # A tibble: 12,000 × 6 ## student_id gender residence transport travel_time time_lib ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 f urban bus 15.1 294. ## 2 2 f rural bike 32.6 254. ## 3 3 f urban bike 19.3 231. ## 4 4 m rural car 35.9 245. ## 5 5 m rural bus 37.9 234. ## 6 6 f urban foot 6.59 303. ## 7 7 f urban bus 23.5 284. ## 8 8 m rural car 36.2 274. ## 9 9 m urban bike 24.3 299. ## 10 10 f urban bus 21.0 282. ## # ℹ 11,990 more rows We survey 200 students. set.seed(345) survey_size &lt;- 200 survey &lt;- rep_sample_n(getsmarter_pop, size = survey_size, replace = FALSE, reps = 1) 9.3.1 Fit a simple normal regression model We first check that the linear model makes sense and the relationship between the two variables is (approximately) linear. ggplot(data = survey, aes(x = travel_time, y = time_lib)) + geom_point() + labs(x = &#39;Travel time (min)&#39;, y = &#39;Time in the library (min)&#39;) This looks reasonably linear, and we can proceed with the modelling. To fit a linear model in R, we use the function lm(). The dependent and the independent variables are joined in a formula by a tilde sign ~. You can also omit the word formula and just type time_lib ~ travel_time. lin_mod &lt;- lm(formula = time_lib ~ travel_time, data = survey) That’s it . Let’s have a look at the estimated intercept and the slope. The function get_regression_table provides a tidy form of the model results, and the function kable() layouts them nicely. get_regression_table(lin_mod) %&gt;% kable() term estimate std_error statistic p_value lower_ci upper_ci intercept 302.094 2.282 132.387 0 297.594 306.594 travel_time -0.766 0.051 -14.990 0 -0.867 -0.665 The estimated intercept equals 302.094 minutes and the slope -0.766 minutes, respectively. We can write our model as: \\[\\widehat{\\text{time_lib}_i} = 302.094 - 0.766 \\cdot \\text{travel_time}_i + \\varepsilon_i\\] The model estimates a time_lib value for every student \\(i\\), with \\(i\\) being an arbitrary student index running from 1 to 200 because we have surveyed 200 students. The “hat” above \\(\\text{time_lib}_i\\) indicates that this is an estimate. The estimated time in the library is the systematic component of the model, namely \\(302.094 - 0.766 \\cdot \\text{travel_time}_i\\). The difference of the actual time in the library and this estimated value is the error term or the residuum \\(\\varepsilon_i\\). To simplify our analysis, we put the original data, estimated values (also called fitted values) and the residuals in one tibble. The respective values can be extracted from the model object lin_mod with the functions fitted() and residuals(). model_res &lt;- survey %&gt;% mutate(fitted = fitted(lin_mod), residuals = residuals(lin_mod)) model_res ## # A tibble: 200 × 9 ## # Groups: replicate [1] ## replicate student_id gender residence transport travel_time time_lib fitted residuals ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1623 m urban foot 7.06 299. 297. 2.55 ## 2 1 9171 m urban bike 11.3 278. 293. -15.3 ## 3 1 10207 f rural bus 107. 199. 220. -21.3 ## 4 1 3506 f urban bus 25.0 326. 283. 42.6 ## 5 1 8892 f urban bus 28.1 259. 281. -21.2 ## 6 1 5460 m urban bus 23.6 299. 284. 14.8 ## 7 1 6120 f urban bus 20.0 268. 287. -18.8 ## 8 1 865 f urban bike 26.6 290. 282. 8.03 ## 9 1 11586 m rural bus 114. 207. 215. -7.44 ## 10 1 8153 f urban foot 8.06 297. 296. 1.19 ## # ℹ 190 more rows Let’s have a look at the residuals and the fitted values. ggplot(model_res, aes(x = travel_time, y = time_lib)) + geom_segment(aes(xend = travel_time, yend = fitted, lty = &#39;Residuals&#39;), alpha = 0.2) + geom_abline(intercept = coef(lin_mod)[1], slope = coef(lin_mod)[2], color = &quot;lightblue&quot;) + geom_point(aes(col = &#39;observed&#39;)) + geom_point(aes(y = fitted, col = &#39;fitted&#39;), shape = 1, size = 2) + labs(x = &#39;Travel time (min)&#39;, y = &#39;Time in the library (min)&#39;) + scale_color_manual(name = &#39;&#39;, values = c(observed = &#39;black&#39;, fitted = &#39;blue&#39;), breaks = c(&#39;observed&#39;, &#39;fitted&#39;), label = c(&#39;Observed values&#39;, &#39;Fitted values&#39;)) + scale_linetype_manual(name = &#39;&#39;, values = (&#39;Residuals&#39; = &#39;solid&#39;)) + theme(legend.position = &#39;bottom&#39;) 9.3.2 Model assumptions Before we interpret the meaning of the model parameters intercept and slope, we have to check the model assumptions. We already saw that the relationship is reasonably linear. Let’s look at the rest of the assumptions. Independent residuals. This is difficult to check and should come with the study design. If you have, like in our example, independently surveyed students, you can reasonably assume that both, the data and the residuals are independent. Conversely, it means that you cannot use a normal linear regression model for time -dependent data (time series) or spatially dependent data. The line you fit will still be correct, but the confidence intervals won’t. Normal residuals. To check for normality of residuals, we use a graphical tool, namely the so-called qq-plot. It compares random data from a normal distribution with our residuals. If the data fall on strait line (at least approximately) then we can conclude that our residuals are normally distributed. ggplot(model_res, aes(sample = residuals)) + stat_qq() + stat_qq_line() + labs(x = &#39;Quantiles of the normal distribution&#39;, y = &#39;Qauntiles of the residuals&#39;) This looks good . You will need some practice to judge the qq-plots. You need to worry if you see large deviations from the line for small and large values. Equal variance and zero mean. For the last assumption, we plot the residuals versus fitted values. It is important that the residuals remain (approximately) equally large for small and large fitted values and fluctuate around zero. ggplot(data = model_res, aes(x = fitted, y = residuals)) + geom_point() + geom_hline(yintercept=0, col = &#39;red&#39;) + labs(x = &#39;Fitted values&#39;, y = &#39;Residuals&#39;) We have slightly larger residuals for larger fitted values. However, we also have more large than small fitted values (i.e., travel times). This distorts the picture a bit. Overall, there is no serious problem there. 9.3.3 Interprete your model We can conclude that our model meets all the assumptions. Therefore, we can interpret the estimated parameters and their confidence intervals now. get_regression_table(lin_mod) %&gt;% kable() term estimate std_error statistic p_value lower_ci upper_ci intercept 302.094 2.282 132.387 0 297.594 306.594 travel_time -0.766 0.051 -14.990 0 -0.867 -0.665 We see that the time a student spend in the library decreases with increasing travel time. More precisely, if the travel time increases by one minute, the time spent in the library changes by -0.8 [-0.9, -0.7]. The confidence interval is narrow. This means that the estimation is precise. Now, we need to frame our result. A decrease by less than a minute per minute travel time sounds rather small. However, if a student needs one hour more to come to the university, she spends roughly 45 minutes less in the library. Basically, the time spent for travelling cannot be spent working in the library. Stated like this, the result is important for students’ time management. 9.4 Practice on your own! Predictors do not need to be numeric. Work through Chapter 5.2 in Ismay and Kim (2021) to see an example of a categorical predictor. 9.5 Reading assignment Chapter 5 in Ismay and Kim (2021) References Fahrmeir, L., T. Kneib, and S. Lang. 2009. Regression. Springer. http://link.springer.com/book/10.1007/978-3-642-01837-4. Ismay, Chester, and Albert Y. Kim. 2021. ModernDive: Statistical Inference via Data Science. https://moderndive.com/. "],["lin-reg-inference.html", "10 Inference in linear regression 10.1 How good is the model? 10.2 Bootstrap with infer: Confidence interval for the slope 10.3 Practice on your own! 10.4 Reading assignment 10.5 Turning in your work", " 10 Inference in linear regression Calculate confidence intervals for model parameters Interpret the summary of a linear regression model Use bootstrap for confidence intervals In the last chapter, we learned how to fit a simple linear model. Remember the assumptions of the model: Linear relationship between variables Independence of residuals Normal residuals Equality of variance (called homoscedasticity) and a mean of zero in residuals In this chapter, we will see how to judge the quality of our model. And we will learn what to do in case the normality and homoscedasticity assumptions are violated. 10.1 How good is the model? library(tidyverse) library(infer) library(knitr) library(moderndive) The data that we model have a certain variability that we quantify by e.g. its variance. To judge how good a model captures the relationship between the dependent variable and the predictors, we could quantify how much variability of the dependent variable can be explained by the model. Thus, we split the variability of the dependent variable (i.e. our observed data) as: \\[ \\begin{align*} \\mathit{SQT} &amp;= \\mathit{SQE} + \\mathit{SQR}\\\\ \\sum^{n}_{i = 1} (y_i-\\bar{y})^2 &amp;= \\sum^{n}_{i=1} (\\hat{y}_i - \\bar{y})^2 + \\sum^{n}_{i=1} (y_i - \\hat{y}_i)^2\\\\ \\end{align*} \\] where \\(y_i\\): observed data, \\(\\bar{y}\\): mean, \\(\\hat{y}_i\\): fitted values \\(\\mathit{SQT}\\) Sum of squares total: variability or variance of the data \\(\\mathit{SQE}\\) Sum of squares explained: variability explained by the model \\(\\mathit{SQR}\\) Sum of squares residual: variability not explained by the model The \\(\\mathit{SQE}\\) quantifies the variability of the fitted values around the mean of the data and \\(\\mathit{SQR}\\) shows how much variability the model fails to capture. The smaller this residual variability \\(\\mathit{SQR}\\) the better the model! The so-called coefficient of determination calculates the proportion of explained variability. The larger it is the better the model : \\[R^2 = \\frac{\\mathit{SQE}}{\\mathit{SQT}} = 1 - \\frac{SQR}{SQT} = 1- \\frac{\\sum^{n}_{i=1} (y_i - \\hat{y}_i)^2}{\\sum^{n}_{i = 1} (y_i - \\bar{y}_i)^2}\\] Let’s go back to our example and look at the coefficient of determination. lin_mod &lt;- lm(formula = time_lib ~ travel_time, data = survey) get_regression_summaries(lin_mod) %&gt;% kable() r_squared adj_r_squared mse rmse sigma statistic p_value df nobs 0.532 0.529 430.4021 20.74614 20.851 224.697 0 1 200 There is a lot of information coming with the summary. In details, we find: r_squared: Coefficient of determination \\(R^2\\) adj_r_squared: \\(R^2_\\text{ajd} = 1 - (1 - R^2) \\frac{n-1}{n - p - 1}\\), \\(n\\): number of data points, \\(p\\): number of predictors (without the intercept); more robust than \\(R^2\\) for multiple linear regression mse: mean standard error mean(residuals(lin_mod)^2) rmse: square root of mse simga: standard error of the error term \\(\\varepsilon\\) statistic: value of the \\(F\\) statistics for the hypothesis test, where \\(H_0\\): all model parameters equal zero p-value: \\(p\\) value of the hypothesis test df: degrees of freedom, here number of predictors nobst: number of data points Thus, we conclude hat our model explained 53% of the variance in the data. 10.2 Bootstrap with infer: Confidence interval for the slope In case, the residuals are non-normally distributed and/or heteroscedastic, the confidence intervals for the model parameters could be wrong. In particular for small data sets, the violation of those assumptions is problematic. To avoid interpreting (possibly) wrong confidence intervals, we can use the bootstrap to construct confidence intervals that do not require the normality and homoscedasticity of residuals. However, they still require independent data (this is always the case for the ordinary bootstrap). In a simple linear regression, the most interesting parameter is the slope. We can use our usual framework in infer to determine its confidence interval. Step 1: Bootstrap the data and calculate the statistic slope bootstrap_distn_slope &lt;- survey %&gt;% specify(formula = time_lib ~ travel_time) %&gt;% generate(reps = 10000, type = &quot;bootstrap&quot;) %&gt;% calculate(stat = &quot;slope&quot;) Step 2: Calculate the confidence interval percentile_ci &lt;- bootstrap_distn_slope %&gt;% get_confidence_interval(type = &quot;percentile&quot;, level = 0.95) percentile_ci ## # A tibble: 1 × 2 ## lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.848 -0.687 Step 3: Visualise the result visualize(bootstrap_distn_slope) + shade_confidence_interval(endpoints = percentile_ci) Compared to the standard confidence interval based on the normality and homoscedasticity assumptions, the bootstrap confidence interval is similar. get_regression_table(lin_mod) %&gt;% filter(term == &#39;travel_time&#39;) %&gt;% kable() term estimate std_error statistic p_value lower_ci upper_ci travel_time -0.766 0.051 -14.99 0 -0.867 -0.665 This is because in our model, the assumptions were valid. In such a case, the confidence intervals are similar and you can safely use the standard confidence interval. Otherwise prefer the bootstrap. 10.3 Practice on your own! You will analyse the relationship between the number of plant species and the number of endemic species on the Galapagos islands. The data set is called gala and is part of the library faraway. Load the data set and read the help pages to understand the meaning of the variables. We want to know how the number of endemic species depends on the number of plant species on the islands. Fit a linear regression model that takes the number of species as predictor and the number of endemic species as dependent variable. Check the model assumptions. Use the workflow in infer to calculate a confidence interval for the slope in the model. Compare the confidence interval based on normality assumption and the bootstrap confidence intervals 10.4 Reading assignment Chapter 10 in Ismay and Kim (2021) 10.5 Turning in your work Save your R Notebook as an *.Rmd file. Upload your R Notebook to ILIAS. You don’t need to upload the .nb.html file. You will find an upload option in today’s session. You should receive a solution file after your upload. Be sure to upload before the deadline! References Ismay, Chester, and Albert Y. Kim. 2021. ModernDive: Statistical Inference via Data Science. https://moderndive.com/. "],["tsa.html", "11 Introduction to time series analysis 11.1 What are time series? 11.2 An example from the German Meteorological Service (DWD) 11.3 Trend analysis of yearly data 11.4 Analysis of seasonality 11.5 Advanced: Simultaneous analysis of trend and seasonality 11.6 Practice on your own! 11.7 Turning in your work", " 11 Introduction to time series analysis 11.1 What are time series? Time series are sequences of values ordered by time. The order of the values is crucial. Often, subsequent values are similar to each other, i.e. correlated. This implies that many statistical techniques which require independent observations do not work. 11.2 An example from the German Meteorological Service (DWD) We will analyse daily temperature and precipitation data. The goal is to see whether (i) we can detect a trend in the data and (ii) how the data fluctuates through the year. A trend is any long-term change in the data which is not due to obvious cyclic processes like the yearly fluctuation. In contrast, seasonality shows exactly this, a (more or less) regular fluctuation which repeats itself in a certain time. I downloaded the data using the library rdwd (https://bookdown.org/brry/rdwd/) on 20 June 2022. The data was measured daily at the station “Koeln-Bonn” starting from 1957 to end of 2021. You can find the complete data description here: Data description We will need the following variables: Variable Description Format/Unit MESS_DATUM the date of measurements yyyymmdd TMK daily mean of temperature °C RSK daily precipitation height mm We load the necessary libraries. library(tidyverse) library(lubridate) library(Kendall) library(ggfortify) rdwddownloads a zipped file. First, we need to unzip it and then to read it in. Note that the NAs are coded as “-999”. unzip(&#39;data/daily_kl_historical_tageswerte_KL_02667_19570701_20211231_hist.zip&#39;, exdir = &quot;data&quot;) clim &lt;- read_delim(&#39;data/produkt_klima_tag_19570701_20211231_02667.txt&#39;, delim = &#39;;&#39;, na = &#39;-999&#39;, trim_ws = TRUE) ## Rows: 23560 Columns: 19 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (1): eor ## dbl (18): STATIONS_ID, MESS_DATUM, QN_3, FX, FM, QN_4, RSK, RSKF, SDK, SHK_TAG, NM, VPM, PM, TMK, UPM, TXK, T... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. clim ## # A tibble: 23,560 × 19 ## STATIONS_ID MESS_DATUM QN_3 FX FM QN_4 RSK RSKF SDK SHK_TAG NM VPM PM TMK UPM TXK ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2667 19570701 5 16 2.9 NA NA NA NA NA NA NA NA NA NA NA ## 2 2667 19570702 5 4.7 1.8 NA NA NA NA NA NA NA NA NA NA NA ## 3 2667 19570703 5 10.3 3.5 NA NA NA NA NA NA NA NA NA NA NA ## 4 2667 19570704 5 9.6 3.4 NA NA NA NA NA NA NA NA NA NA NA ## 5 2667 19570705 5 10.4 2.9 NA NA NA NA NA NA NA NA NA NA NA ## 6 2667 19570706 5 9.2 2.9 NA NA NA NA NA NA NA NA NA NA NA ## 7 2667 19570707 5 10.5 3.5 NA NA NA NA NA NA NA NA NA NA NA ## 8 2667 19570708 5 15.9 2.2 NA NA NA NA NA NA NA NA NA NA NA ## 9 2667 19570709 5 13.2 3.1 NA NA NA NA NA NA NA NA NA NA NA ## 10 2667 19570710 5 18.2 2.6 NA NA NA NA NA NA NA NA NA NA NA ## # ℹ 23,550 more rows ## # ℹ 3 more variables: TNK &lt;dbl&gt;, TGK &lt;dbl&gt;, eor &lt;chr&gt; The date of the measurement is not a proper date format and we must convert it first. clim &lt;- clim %&gt;% mutate(MESS_DATUM = ymd(MESS_DATUM)) clim ## # A tibble: 23,560 × 19 ## STATIONS_ID MESS_DATUM QN_3 FX FM QN_4 RSK RSKF SDK SHK_TAG NM VPM PM TMK UPM TXK ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2667 1957-07-01 5 16 2.9 NA NA NA NA NA NA NA NA NA NA NA ## 2 2667 1957-07-02 5 4.7 1.8 NA NA NA NA NA NA NA NA NA NA NA ## 3 2667 1957-07-03 5 10.3 3.5 NA NA NA NA NA NA NA NA NA NA NA ## 4 2667 1957-07-04 5 9.6 3.4 NA NA NA NA NA NA NA NA NA NA NA ## 5 2667 1957-07-05 5 10.4 2.9 NA NA NA NA NA NA NA NA NA NA NA ## 6 2667 1957-07-06 5 9.2 2.9 NA NA NA NA NA NA NA NA NA NA NA ## 7 2667 1957-07-07 5 10.5 3.5 NA NA NA NA NA NA NA NA NA NA NA ## 8 2667 1957-07-08 5 15.9 2.2 NA NA NA NA NA NA NA NA NA NA NA ## 9 2667 1957-07-09 5 13.2 3.1 NA NA NA NA NA NA NA NA NA NA NA ## 10 2667 1957-07-10 5 18.2 2.6 NA NA NA NA NA NA NA NA NA NA NA ## # ℹ 23,550 more rows ## # ℹ 3 more variables: TNK &lt;dbl&gt;, TGK &lt;dbl&gt;, eor &lt;chr&gt; We plot the temperature data. ggplot(data = clim, aes(x = MESS_DATUM, y = TMK)) + geom_line() + labs(y = &#39;Daily mean temperature (°C)&#39;, x = &#39;Date&#39;) We want to analyse complete years only. Which years should be deleted because of too few data points? clim %&gt;% group_by(year(MESS_DATUM)) %&gt;% tally() ## # A tibble: 65 × 2 ## `year(MESS_DATUM)` n ## &lt;dbl&gt; &lt;int&gt; ## 1 1957 184 ## 2 1958 365 ## 3 1959 365 ## 4 1960 366 ## 5 1961 365 ## 6 1962 365 ## 7 1963 365 ## 8 1964 366 ## 9 1965 365 ## 10 1966 365 ## # ℹ 55 more rows The year 1957 is incomplete and we exclude it. clim &lt;- clim %&gt;% filter(year(MESS_DATUM) != &#39;1957&#39;) 11.3 Trend analysis of yearly data We summarize the temperature to yearly values to exclude the seasonal fluctuations. This helps to analyse the trend. temp_yearly &lt;- clim %&gt;% group_by(year(MESS_DATUM)) %&gt;% summarize(mean_temp = mean(TMK, na.rm = T)) %&gt;% rename(year = `year(MESS_DATUM)`) temp_yearly ## # A tibble: 64 × 2 ## year mean_temp ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1958 9.76 ## 2 1959 10.6 ## 3 1960 10.1 ## 4 1961 10.5 ## 5 1962 8.56 ## 6 1963 8.48 ## 7 1964 9.86 ## 8 1965 9.26 ## 9 1966 10.2 ## 10 1967 10.5 ## # ℹ 54 more rows Plot of yearly mean temperature data. ggplot(temp_yearly, aes(x = year, y = mean_temp)) + geom_line() + geom_smooth(formula = &#39;y ~ x&#39;, method = &#39;loess&#39;, se = FALSE) + ylim(5, 15) + labs(y = &#39;Yearly mean temperature (°C)&#39;, x = &#39;Date&#39;) There seem to be an upward trend, but is it significant or due to chance. To answer this question we use the Mann-Kendall test for trend. The null and alternative hypothesis of the test are: \\(H_0\\): data does not have a trend. \\(H_1\\): data has a trend. The test calculates a statistics called \\(\\tau\\). It is based on a comparison of all possible pairs of variables and their succeeding neighbours. If \\(\\tau &lt; 0\\), then more neighbouring points are smaller, and the data has a negative trend. If \\(\\tau &gt; 0\\) more neighbouring points are larger, and the data has a positive trend. If \\(\\tau \\approx 0\\), then the smaller and larger neighbouring points are in balance, and there is no trend. MannKendall(temp_yearly$mean_temp) ## tau = 0.452, 2-sided pvalue =1.1921e-07 The \\(p\\) value is small. It is plausible to conclude that there is a positive trend because \\(\\tau &gt; 0\\). By how much did the mean temperature increase per year? We calculated differences of yearly temperature of consecutive years. temp_diff &lt;- temp_yearly %&gt;% summarize(temp_diff = diff(mean_temp)) %&gt;% mutate(year = temp_yearly$year[-1]) ## Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in dplyr 1.1.0. ## ℹ Please use `reframe()` instead. ## ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()` always returns an ungrouped data ## frame and adjust accordingly. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. mean_diff &lt;- temp_diff %&gt;% summarise(mean = mean(temp_diff)) Plot the distribution of differences. ggplot(temp_diff, aes(x = temp_diff)) + geom_histogram(bins = 15, col = &#39;white&#39;) We see a mean increase of 0.01 degrees per year. 11.4 Analysis of seasonality Summarize by month temp_monthly &lt;- clim %&gt;% group_by(year(MESS_DATUM), month(MESS_DATUM)) %&gt;% summarize(mean_temp = mean(TMK, na.rm = T)) %&gt;% rename(year = `year(MESS_DATUM)`, month = `month(MESS_DATUM)`) %&gt;% mutate(date = ymd(paste(year, month, &#39;15&#39;, sep = &#39;-&#39;))) %&gt;% relocate(date) ## `summarise()` has grouped output by &#39;year(MESS_DATUM)&#39;. You can override using the `.groups` argument. temp_monthly ## # A tibble: 768 × 4 ## # Groups: year [64] ## date year month mean_temp ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1958-01-15 1958 1 2.08 ## 2 1958-02-15 1958 2 4.46 ## 3 1958-03-15 1958 3 1.89 ## 4 1958-04-15 1958 4 6.74 ## 5 1958-05-15 1958 5 13.8 ## 6 1958-06-15 1958 6 15.5 ## 7 1958-07-15 1958 7 17.7 ## 8 1958-08-15 1958 8 18.0 ## 9 1958-09-15 1958 9 16.1 ## 10 1958-10-15 1958 10 10.4 ## # ℹ 758 more rows Plot monthly values. The seasonality seems rather constant over time. ggplot(temp_monthly, aes(x = date, y = mean_temp)) + geom_line() + labs(x = &#39;Date&#39;, y = &#39;Mean daily tempearture (°C)&#39;) The overall seasonality. ggplot(temp_monthly, aes(x = as_factor(month), y = mean_temp)) + geom_boxplot() + scale_x_discrete(labels = month.abb) + labs(x = &#39;Month&#39;, y = &#39;Variation of the mean yearly temperature (°C)&#39;) 11.5 Advanced: Simultaneous analysis of trend and seasonality We can use a model to decompose the time series into a trend, seasonality and a rest (remainder) component. This model is additive, meaning that when you sum the components, you get back your original time series. The model assumes that the seasonality remains stable over time, i.e. it is inappropriate for climate change studies. However, it is still a good starting point. We need to convert the data to a ts (time series) object first. This type of objects keeps the data along with the time. time_series &lt;- ts(temp_monthly$mean_temp, start = c(1958, 1), end = c(2021, 12), frequency = 12) str(time_series) ## Time-Series [1:768] from 1958 to 2022: 2.08 4.46 1.89 6.74 13.82 ... We use the function decompose() to decompose the time series. components &lt;- decompose(time_series) plot(components) Plot the trend using ggfortify. This library helps to transform the complicated object created by decompose() to a data.frame which can be easily plotted with ggplot(). ggplot(data = fortify(components), aes(x = Index, y = trend)) + geom_line() + annotate(&#39;rect&#39;, xmin = as.Date(&#39;2006-01-01&#39;), xmax = as.Date(&#39;2007-12-31&#39;), ymin = -Inf, ymax = Inf, fill = &#39;red&#39;, alpha = 1/3) + ylim(5, 15) + labs(x = &#39;Date&#39;, y = &#39;Trend component of the temperature (°C)&#39;, title = &#39;Red bar highlights the year 2007.&#39;) The large peak is due to the highest monthly mean in July 2007, don’t over-interpret this as a change in the trend! General advice: If you want to analyse the trend, you need to get rid of the seasonality. The most simple way is to calculate yearly data (if not already provided) and then do the Mann-Kendall test to test for a trend. Calculate the mean change over years or analyse changes in more details. Report about those changes and the \\(p\\) value of the test. If you have seasonality, you can use decompose() to decompose the data and estimate trend and seasonality. However, remember that the model assumes that the seasonality remains stable over time. 11.6 Practice on your own! Select another location and repete the analysis of temperature. Analyse the precipitation data. Is there a trend? Instead of calculating mean values, you must sum the precipitations! Analyse the yearly sums only, no decomposition because seasonality is less important. 11.7 Turning in your work Save your R Notebook as an *.Rmd file. Upload your R Notebook to ILIAS. You don’t need to upload the .nb.html file. You will find an upload option in today’s session. You should receive a solution file after your upload. Be sure to upload before the deadline! "],["stat-background.html", "A Statistics refresher A.1 Descriptive statistics A.2 Measures of association", " A Statistics refresher Refresh basic statistics This chapter is a short refresher on the basic statistics and does not replace a statistics book. I will use the penguins data set to calculate the examples. Figure A.1: Artwork by @allison_horst library(palmerpenguins) library(tidyverse) A.1 Descriptive statistics A.1.1 Mean The mean is one of the common summary statistics you can calculate for your data. To calculate the mean you sum all the values in your data set and divide the sum by the number of values. \\[\\bar{x} = \\sum_{i = 1}^{i = n} \\frac{x_i}{n}\\] where: \\(\\bar{x}\\): mean value of data set \\(x\\) \\(i\\): index running from 1 to \\(n\\), the number of your data (sample size) \\(x_i\\): single data points in the data sets \\(x\\) A simple example first, before we turn to the penguins. Let’s calculate the mean of the data set a containing the values 2, 4.3, 5 and 10 by hand and compare it to the values obtained with the function mean(). a &lt;- c(2, 4.3, 5, 10) hand_mean &lt;- (2 + 4.3 + 5 + 10)/4 r_mean &lt;- mean(a) hand_mean ## [1] 5.325 r_mean ## [1] 5.325 As expected, the values are identical. Let’s now calculate the mean bill length of the penguins. mean(x = penguins$bill_length_mm) ## [1] NA The function mean() returns NA if there are missing values in the data set. Remember to exclude NAs with na.rm = TRUE to get a meaningful result. mean(x = penguins$bill_length_mm, na.rm = TRUE) ## [1] 43.92193 A.1.2 Variance and standard deviation Variance and standard deviation are both measures of variability in your data. They show how far the data deviates from its mean value on the average. This deviation is defined as the squared difference between a data point and the mean of the data. The variance is defined as: \\[v = \\frac{1}{n-1} \\sum_{i = 1}^{i = n} (x_i - \\bar{x})^2\\] The standard deviation is the square root of the variance and is defined as: \\[s = \\sqrt{\\frac{1}{n-1} \\sum_{i = 1}^{i = n} (x_i - \\bar{x})^2}\\] where: \\(\\bar{x}\\): mean value of data set \\(x\\) \\(i\\): index running from 1 to \\(n\\), the number of your data points (sample size) \\(x_i\\): single data points in the data sets \\(x\\) How large is the standard deviation in the data set a? Again, we compare the results of by-hand calculation and the output of the function sd(). sd_hand &lt;- sqrt(((2 - r_mean)^2 + (4.3 - r_mean)^2 + (5 - r_mean)^2 + (10 - r_mean)^2)/(4 - 1)) r_sd &lt;- sd(a) sd_hand ## [1] 3.369842 r_sd ## [1] 3.369842 Again, identical . How large is the standard deviation of penguins’ bill lengths? Don’t forget to exclude the missing values! sd(x = penguins$bill_length_mm, na.rm = TRUE) ## [1] 5.459584 A.2 Measures of association A.2.1 Linear correlation coefficent Aka Pearson correlation coefficient and Pearson product-moment correlation coefficient measures the linear association between two numeric variables: \\[ r = \\frac{\\sum_{i = 1}^{i = n} (x_i - \\bar{x})(y_i - \\bar{y})}{ \\sqrt{\\sum_{i = 1}^{i = n} (x_i - \\bar{x})^2} \\sqrt{\\sum_{i = 1}^{i = n} (y_i - \\bar{y})^2}}\\] where: \\(\\bar{x}\\) and \\(\\bar{y}\\): mean values of data set \\(x\\) and \\(y\\), respectively \\(i\\): index running from 1 to \\(n\\), the number of your data points (sample size) \\(x_i\\) and \\(y_i\\): single data points in the data sets \\(x\\) and \\(y\\), respectively The Pearson correlation coefficients varies between -1 (perfect negative correlation) and 1 (perfect positive correlation). A value around zero shows no linear correlation. However, a different kind of assiciation might exist in the data. Remember to always plot your data! How large is the correlation coefficient between the bill length and the bill depth for the species Adélie? Figure A.2: Artwork by @allison_horst adelie &lt;- penguins %&gt;% filter(species == &#39;Adelie&#39;) cor(adelie$bill_length_mm, adelie$bill_depth_mm, method = &#39;pearson&#39;, use = &#39;pairwise.complete.obs&#39;) ## [1] 0.3914917 We need to distinguish between species because the correlations differ between them. penguins %&gt;% group_by(species) %&gt;% summarise(cor = cor(bill_length_mm, bill_depth_mm, method = &#39;pearson&#39;, use = &#39;pairwise.complete.obs&#39;)) ## # A tibble: 3 × 2 ## species cor ## &lt;fct&gt; &lt;dbl&gt; ## 1 Adelie 0.391 ## 2 Chinstrap 0.654 ## 3 Gentoo 0.643 ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, col = species)) + geom_point() + geom_smooth(se = FALSE, method = lm) "],["daten-und-bericht.html", "B Data sources and final report B.1 Data sources B.2 Research proposal B.3 Structure of the final report B.4 Evaluation critera", " B Data sources and final report Know different data sources Use libraries for direct access to databases B.1 Data sources In this chapter, I will introduce some interesting data sources (databases) that you can use for your final report. I offer technical assistance for the package eurostat. You can discover other packages on your own (this could be a possible challenge for the final report, see below ). B.1.1 Federal Statistical Office The Federal Statistical Office offers data about Germany in different areas through their database GENESIS. You can navigate to the data sets and download them. Pay attention to select the format flat when downloading to get a tidy data set. B.1.2 eurostat eurostat is the statistical office of the European Union. You will find many different data sets about Europe there. However, I strongly recommend using the dedicated R library eurostat to download the data. My experience shows that students underestimate how error-prone manual download from a large database can be! The R library eurostat has an informative web site with many tutorials. Some of the show how to use the eurostat data to visualize maps (possible report challenge). B.1.3 gapminder You already worked with an excerpt of data by gapminder. The complete data set contains much more. You can download more data here. gapminder organises their data in a so-called DDF Format (data description format) as tidy .csv files. A possible report challenge could be to understand this format. A complete data set is availagle via GitHub here. B.1.4 National Oceanic and Atmospheric Administration (NOAA) NOAA offers data on oceans, climate and weather. You can download the data sets via the library rnoaa. B.1.5 More data sources World Bank Open Data World Happiness Report Global Carbon Budget 2020 Publication and data set Global Carbon Project Soil organic carbon contents from sites under perennial cultivation Publication Data set PANGAEA: one of the largest databases for environmental datasets Overview of data and libraries on ROpenScie: Data Libraries B.2 Research proposal Before you write your final report, you need to submit a research proposal on ILIAS. Be sure to upload before the deadline!. You will receive feedback on it. This should avoid any misunderstanding about your final report. Please use the template provided on ILIAS for the research proposal. B.3 Structure of the final report B.3.1 Sturcture of the working directory Create a new R project for your final report. You can find more about using projects here. A project helps you to structure your work properly. Inside your project folder, create a folder for data, figures and (help) scripts (if appropriate). Save your notebooks in the root directory of your project. B.3.2 Download and save data If you want to use libraries to download data, then create a notebook for this task. Don’t download and analyse the data on-the-fly. Data can change and there will be a time delay between your analysis and my assessment of your report. Download and save your data in the data folder. Use the saved data for the analysis. This will ensure that your report is fully reproducible. B.3.3 Structure of the report Please structure your report as follows: Introduction (with research question at the end) Material and Methods: Data description: date of download, reproducibility is crucial), description of variables and their units Description of the method with references, description of the research area if appropriate Results: Explorative data analysis (mandatory!) Further analysis to respond to your research question(s) Discussion: use peer-reviewed literature to discuss your results Conclusions Bibliography You can combine results and discussion to one section. Each report should contain a challenge. This can be an extensive data tidying and wrangling or use of a new and interesting library (e.g. to plot spatial data in eurostat) etc. You can write your report in groups of two, I even recommend to do so. In this case, please add your names to the respective parts that you are responsible to receive individual grades. Every part will be graded separately. Every group member need to do some analysis and some writing. Alternatively, give your consent to receive the same grade if you analysed the data and wrote the report together. I don’t impose any particular length for your report. However, please be concise. Knit the final report to an html-document. Think about whether you need to show all chunks, avoid redundancy. Compress the whole project and submit everything! Your report and your analysis must run on my computer so observe the paths! Submit the compressed project to ILIAS before the deadline. B.4 Evaluation critera I will evaluate your report along the following criteria: Does the notebook run without errors? Is the the notebook structured as prescribed above? Are the research questions stated clearly? Are the methods described and necessary literature cited. Were the data downloaded and saved outside the main report in an extra notebook for full reproducibility? In case the data is downloaded by hand, this must be explained in the main report. Is the research area described? Quality of data preparation and exploratory analysis. Quality of the main analysis, which you do to respond to research questions. Quality of figures. Are the results discussed and additional literature cited? This literature should help evaluating and understanding your results. Are the conclusions sound and supported by the data and results? You will receive a corrected report as feedback. "],["additional-exercises.html", "C Additional exercises C.1 Introduction to R and RStudio C.2 The big practical: importing, wrangling, summerizing and plotting C.3 The big practical: statistical inference", " C Additional exercises C.1 Introduction to R and RStudio C.1.1 Rob’s account book The young master student Rob Stat thinks seriously about his mother’s advice to monitor his expenses. He begins by writing down what he spends during the week in the Mensa: Table C.1: Table C.2: Rob’s account book Day of week Amount spent (€) Monday 2.57 Tuesday 2.90 Wednesday 2.73 Thrusday 3.23 Friday 3.90 Generate a vector of Rob’s expenses and assign it to the variable expenses. Use the function c() and use the numeric expenses only, not the days of the week. How much did Rob spend during this week? Use the function sum(). Rob seems to have spent the smallest amount on Tuesday. How much would he have spent if he paid that much every day of the week? Use the array notation with the square brackets. Unfortunately, Rob misspelled the amount on Tuesday. Actually, he invited his girl friend for lunch and paid 7.95 € instead of 2.90 €. Correct Rob’s typo. How does the result change? C.1.2 Missing values R codes missing values as NA. Rob ate in the Mensa last Monday, but forgot to write down the amount. Table C.3: Table C.4: Rob’s account book, cont. Day of week Amount spent (€) Monday, 9 March 2.57 Tuesday, 10 March 2.90 Wednesday, 11 March 2.73 Thrusday, 12 March 3.23 Friday, 13 March 3.90 Monday, 16 March NA How does NA change the calculated sum? Read what happens when the data contains NAs by calling help on sum, i.e. type ?sum in R console. Correct your call to sum() accordingly. C.1.3 Your very first plot In particular in the beginning of learning R you should not forget why you are doing it. Because R is really beautiful and you want to analyse and learn from real data. Even if you don’t fully understand the following code, just copy and paste it into your .R file and let it run! library(tidyverse) library(gapminder) gapminder2007 &lt;- gapminder %&gt;% filter(year == 2007) ggplot(gapminder2007, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + geom_point() + scale_x_log10() + xlab(&#39;GDP per capita&#39;) + ylab(&#39;Life expectancy&#39;) + labs(title = &#39;Gapminder data for the year 2007&#39;) What is the data set about. Use help like this ?gapminder. What do the colours represent? What does the size of the circles represent? How would you describe the relationship between the GDP per capita and Life expectancy? C.2 The big practical: importing, wrangling, summerizing and plotting C.2.1 Temperature along the Dutch coast The file Temperatur.csv from the book by Zuur, Ieno, and Meesters (2009) contains measurements of temperature, salinity and content of chlorophyll a at 31 locations along the Dutch coast. You can download the data set here. The data is provided by the Dutch institute RIKZ (monitoring program MWTL: Monitoring Waterstaatkundige Toestand des Lands), was measured between 1990 and 2005 between 0 and 4 times per month depending on the season. Read the file Temperatur.csv into R. Convert the column Date to a proper date format. Use the library lubridate. Calculate the number of measurements, mean and standard deviations of temperature per monitoring station. Hint: use n() inside summarize() to get the number of measurements. Calculate the number of measurements, mean and standard deviations of temperature per month. Plot the mean monthly temperature as a line and add the standard deviation as a band around it. Label the axis appropriately. Save you graph as a pdf file. C.2.2 Temperature along the Dutch coast, revisited Calculate the monthly means and standard deviations per monitoring station. Hint group_by(Station, Month). Plot the means with an error band in different plots. Hint: use facet_wrap()). Save you graph as a pdf file. C.2.3 Excel data turns tidy We will import and tidy World Development Indicators data downloaded from the World Bank on 2021-06-09 for 20 countries. This is an extract only and more data is available. This exercise will show you how to load excel data directly without converting it to .csv file. The format of the data is a typical non-tidy one and you will wrangle it to a tidy tibble. The file is called Data_Extract_From_World_Development_Indicators.xlsx. The goal of this exercise is to learn how to read data from an excel file using tidyverse functions. We will use the library readxl and the function read_xlsx() for reading such files. Read the help pages of read_xlsx(), find and set the parameter for reading a particular table sheet correctly. Open the excel sheet and look through the data carefully. How are NAs coded? Which data sheet do you need to read? Read the excel file into R. Call it wdi. This data set is not tidy. In particular, the year is coded as column name. Those column names contain the year twice, once as a number and once as [YR NUMBER]. We rename the columns first. wdi &lt;- wdi %&gt;% rename_with(.fn = function(x) str_sub(x, start = 1, end = 4), .cols = starts_with(&#39;20&#39;)) What does this code mean? Read the help pages for functions rename_with(), str_sub() and starts_with(). Pivot the data set to a tidy format: variables in columns and measurements in rows. Use pivot_longer. wdi &lt;- wdi %&gt;% pivot_longer(names_to = &#39;year&#39;, values_to = &#39;indicator_value&#39;, cols = starts_with(&#39;20&#39;)) %&gt;% mutate(year = as.numeric(year)) wdi What does this code mean? Read the help pages for functions pivot_longer() and as.numeric(). Why is it necessary to convert year with as.numeric()? Filter for the indicator GDP (current US$) and plot the data as time series. Hint: You can also filter for the indicator’s code; look it up in the excel file. Label the axis appropriately. C.3 The big practical: statistical inference C.3.1 Species richness in grasslands You will work with grassland species monitoring data from the Yellowstone National Park provided by Zuur, Ieno, and Meesters (2009) and Sikkink et al. (2007). You can download the data set here. The researchers monitored the changes in the grassland communities over time and related them to environmental factors. Biodiversity is expressed as the number of different species per site (variable R). Approximately 90 species were identified in 8 transects in monitoring campaigns repeated every 4 to 10 years, resulting in 58 observations. The data is saved in the file Vegetation2.xls. Read the data and explore its structure. Describe the type of variables. Does the type correspond to your expectation for the respective variable? Remember to set the name of the table sheet you want to read in read_xls(). Short explorative data analysis: calculate the number of measurements, the mean and the standard deviations of species number R per transect. Plot the species number versus the variable BARESOIL (proportion of bare soil). Colour the dots by transect. Hint: convert the transect as_factor(). Add a smoothing line without a confidence band (geom_smooth(se = FALSE)) through all points independently of the transect. You might want to consult Section 4.2 in the book ggplot2 (Wickham 2020). Hint: set the colour aes in geom_point() instead of in ggplot(). Add labels to your graph and assign it to an object. Plot the species number as a time series by transect. Add both, points and lines. The size of the points should reflect the proportion of bare soil. You might want to consult Section 12.1 in the book ggplot2 (Wickham 2020). Think about where to set the aesthetic size in order to scale the points only. Add labels to your graph and assign it to an object. Put both graphs side-by-side and save them as a pdf (ggsave()). Calculate the linear Pearson correlation coefficient between the species number and the proportion of bare soil. Calculate the 95% confidence interval. Use the framework in infer. If you calculate the 90% confidence interval instead of the 95% confidence interval, does the confidence interval increase or decrease? Why? C.3.2 Soil compaction Heavy agricultural machines could compact the soil. In a randomized field trial, plots (variable plots) on a homogeneous agricultural field were assigned randomly either to the control (control) or to treatment where a heavy agricultural machine was used (compacted). Bulk density in [g/cm³] (mass of dry soil divided by soil volume) was measured on every plot. It is a parameter for soil structure and can help to spot soil compaction. The data is stored inbd_compaction.csv. Read the data and do a short explorative data analysis. Did the bulk density increase due to heavy machinery or could the difference be due to chance? Use the framework in infer. Calculate the effect size (difference in means) and its 95% confidence interval. References Sikkink, P. G., A. F. Zuur, E. N. Ieno, and G. M. Smith. 2007. “Monitoring for Change: Using Generalised Least Squares, Non-Metric Multidimensional Scaling, and the Mantel Test on Western Montana Grasslands.” In Analysing Ecological Data, edited by Alain F. Zuur, Elena N. Ieno, and Graham M. Smith, 463–84. Statistics for Biology and Health. New York, NY: Springer. https://doi.org/10.1007/978-0-387-45972-1_26. Wickham, Hadley. 2020. Ggplot2: Elegant Graphics for Data Analysis. 3rd, in progress. Zuur, A. F., E. Ieno, and E. Meesters. 2009. A Beginner’s Guide to R. Springer. "],["faq.html", "D Frequently and not-so-frequently asked questions", " D Frequently and not-so-frequently asked questions library(tidyverse) library(palmerpenguins) What is the difference between double \"\" and single '' quotes? There is no difference. Both are valid to quote text/strings. Pay attention when you use quotes in quotes like “My solutions to exercise ‘Find the mean’”. Then quotes inside quotes must be different (either “” outside and ’’ inside or the other way round). However, they may be rendered differently between plain R and R Markdown in e.g. the title. print(&#39;I am a text.&#39;) ## [1] &quot;I am a text.&quot; print(&quot;So am I.&quot;) ## [1] &quot;So am I.&quot; print(&quot;I am a &#39;quote&#39; in a text.&quot;) ## [1] &quot;I am a &#39;quote&#39; in a text.&quot; print(&#39;So am &quot;I&quot;, but not so nice looking.&#39;) ## [1] &quot;So am \\&quot;I\\&quot;, but not so nice looking.&quot; When do we use the pipe operator %&gt;% and when the plus sign + to connect lines of code? The pipe operator %&gt;% is used to compose functions. Instead of saving the result of every function and then passing this result explicitly to the next function, we can omit the intermediate saving and “pipe” the results through: penguins %&gt;% filter(species == &#39;Adelie&#39;) %&gt;% summarise(mean = mean(bill_length_mm, na.rm = TRUE)) ## # A tibble: 1 × 1 ## mean ## &lt;dbl&gt; ## 1 38.8 # The same result, but not as tibble! filtered_data &lt;- filter(penguins, species == &#39;Adelie&#39;) mean(filtered_data$bill_length_mm, na.rm = TRUE) ## [1] 38.79139 More details on pipes here. The plus sign + is used as operator in ggplot2 only, to construct a graph. ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, col = species)) + geom_point() + geom_smooth(se = FALSE, method = lm) "],["literature-1.html", "Literature", " Literature Diez, David M, Mine Çetinkaya-Rundel, and Christopher D Barr. 2019. OpenIntro Statistics. Fourth. Efron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” The Annals of Statistics 7 (1): 1–26. https://doi.org/10.1214/aos/1176344552. Fahrmeir, L., T. Kneib, and S. Lang. 2009. Regression. Springer. http://link.springer.com/book/10.1007/978-3-642-01837-4. Hesterberg, Tim C. 2015. “What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum.” The American Statistician 69 (4): 371–86. https://doi.org/10.1080/00031305.2015.1089789. Ihaka, Ross, and Robert Gentleman. 1996. “R: A Language for Data Analysis and Graphics.” Journal of Computational and Graphical Statistics 5 (3): 299–314. https://doi.org/10.1080/10618600.1996.10474713. Ismay, Chester, and Albert Y. Kim. 2021. ModernDive: Statistical Inference via Data Science. https://moderndive.com/. Knuth, D. E. 1984. “Literate Programming.” The Computer Journal 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97. Rosen, Benson, and Thomas H. Jerdee. 1974. “Influence of Sex Role Stereotypes on Personnel Decisions.” Journal of Applied Psychology 59 (1): 9–14. https://doi.org/10.1037/h0035834. Sikkink, P. G., A. F. Zuur, E. N. Ieno, and G. M. Smith. 2007. “Monitoring for Change: Using Generalised Least Squares, Non-Metric Multidimensional Scaling, and the Mantel Test on Western Montana Grasslands.” In Analysing Ecological Data, edited by Alain F. Zuur, Elena N. Ieno, and Graham M. Smith, 463–84. Statistics for Biology and Health. New York, NY: Springer. https://doi.org/10.1007/978-0-387-45972-1_26. Wasserstein, Ronald L., Allen L. Schirm, and Nicole A. Lazar. 2019. “Moving to a World Beyond ‘p \\(&lt;\\) 0.05’.” The American Statistician 73 (sup1): 1–19. https://doi.org/10.1080/00031305.2019.1583913. Wickham, Hadley. 2020. Ggplot2: Elegant Graphics for Data Analysis. 3rd, in progress. Wickham, Hadley, and Garrett Grolemund. 2021. R for Data Science. https://r4ds.had.co.nz/. Zuur, A. F., E. Ieno, and E. Meesters. 2009. A Beginner’s Guide to R. Springer. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
